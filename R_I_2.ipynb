{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVLQwMhbfBjo",
        "outputId": "2042a732-bfe0-490b-fdfd-968223fd3f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-28 22:21:16--  https://huggingface.co/datasets/WINGNUS/ACL-OCL/resolve/main/acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl\n",
            "Resolving huggingface.co (huggingface.co)... 3.171.171.128, 3.171.171.6, 3.171.171.104, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.171.171.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6374015cc3b104ef17a000f2/8f0a273e6b400ed0c74635f29e6713af2f5bea1e823f427d41bde16cfce9a379?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251228T222117Z&X-Amz-Expires=3600&X-Amz-Signature=53ec46f7a78eee8a2e7179e48412feb25e19edad1141ebce6a29143f0e73e125&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl%3B+filename%3D%22acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl%22%3B&x-id=GetObject&Expires=1766964077&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Njk2NDA3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82Mzc0MDE1Y2MzYjEwNGVmMTdhMDAwZjIvOGYwYTI3M2U2YjQwMGVkMGM3NDYzNWYyOWU2NzEzYWYyZjViZWExZTgyM2Y0MjdkNDFiZGUxNmNmY2U5YTM3OSoifV19&Signature=FzADCNnmT6cF0pTE6WfhO9g5NO0Fp6W3sflS5%7ElGmSkGKk4XOuFcE2NUHd7W%7Es7UTnzVpNwREmPxKQB0zMXsLYDI-P8MIaOAhDlTmod6uccavnsI8eD10zbOgsacc3qg8R6RvHw9k%7En79Ok9kCZPICC7TIDbkgRK-q9WFKE8D6SWl9%7EJVLVOvqJF8nVbFaWprc89tWLkgx2wnE5jJ4-OJFOv8f4%7E4Mmhg%7ELMj0wwWbR10ImhIkWjVPcpEgdqulMBFU4%7ERhNozBMmfP0FQ7J-Lz%7EA6qtU8L7igwh6NkO3sVSJaclJc1WLAYeDnC-RhJocoYZQcWBIhH-82aDZvwRHtw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-28 22:21:17--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6374015cc3b104ef17a000f2/8f0a273e6b400ed0c74635f29e6713af2f5bea1e823f427d41bde16cfce9a379?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251228T222117Z&X-Amz-Expires=3600&X-Amz-Signature=53ec46f7a78eee8a2e7179e48412feb25e19edad1141ebce6a29143f0e73e125&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl%3B+filename%3D%22acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl%22%3B&x-id=GetObject&Expires=1766964077&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Njk2NDA3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82Mzc0MDE1Y2MzYjEwNGVmMTdhMDAwZjIvOGYwYTI3M2U2YjQwMGVkMGM3NDYzNWYyOWU2NzEzYWYyZjViZWExZTgyM2Y0MjdkNDFiZGUxNmNmY2U5YTM3OSoifV19&Signature=FzADCNnmT6cF0pTE6WfhO9g5NO0Fp6W3sflS5%7ElGmSkGKk4XOuFcE2NUHd7W%7Es7UTnzVpNwREmPxKQB0zMXsLYDI-P8MIaOAhDlTmod6uccavnsI8eD10zbOgsacc3qg8R6RvHw9k%7En79Ok9kCZPICC7TIDbkgRK-q9WFKE8D6SWl9%7EJVLVOvqJF8nVbFaWprc89tWLkgx2wnE5jJ4-OJFOv8f4%7E4Mmhg%7ELMj0wwWbR10ImhIkWjVPcpEgdqulMBFU4%7ERhNozBMmfP0FQ7J-Lz%7EA6qtU8L7igwh6NkO3sVSJaclJc1WLAYeDnC-RhJocoYZQcWBIhH-82aDZvwRHtw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.32.179.62, 13.32.179.58, 13.32.179.6, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.32.179.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1606761180 (1.5G)\n",
            "Saving to: ‘acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl.2’\n",
            "\n",
            "acl-publication-inf 100%[===================>]   1.50G  30.0MB/s    in 43s     \n",
            "\n",
            "2025-12-28 22:22:00 (35.6 MB/s) - ‘acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl.2’ saved [1606761180/1606761180]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/WINGNUS/ACL-OCL/resolve/main/acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DDyT6KlbfDnf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "x05UKEDufDkB"
      },
      "outputs": [],
      "source": [
        "data=pd.read_pickle(\"acl-publication-info.74k.v3.full-sections-partial-topic-labels.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IGjcTUuDfDhs",
        "outputId": "31b47bae-9d04-4767-bef9-4e44423ab865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         acl_id                                           abstract  \\\n",
              "0      O02-2002  There is a need to measure word similarity whe...   \n",
              "1      L02-1310                                               None   \n",
              "2      R13-1042  Thread disentanglement is the task of separati...   \n",
              "3      W05-0819  In this paper, we describe a word alignment al...   \n",
              "4      L02-1309                                               None   \n",
              "...         ...                                                ...   \n",
              "73280  P99-1002  This paper describes recent progress and the a...   \n",
              "73281  P00-1009  We present an LFG-DOP parser which uses fragme...   \n",
              "73282  P99-1056  The processes through which readers evoke ment...   \n",
              "73283  P99-1051  This paper examines the extent to which verb d...   \n",
              "73284  P00-1013  Spoken dialogue managers have benefited from u...   \n",
              "\n",
              "                                               full_text  corpus_paper_id  \\\n",
              "0      There is a need to measure word similarity whe...         18022704   \n",
              "1                                                   None          8220988   \n",
              "2      Thread disentanglement is the task of separati...         16703040   \n",
              "3      In this paper, we describe a word alignment al...          1215281   \n",
              "4                                                   None         18078432   \n",
              "...                                                  ...              ...   \n",
              "73280  This paper describes recent progress and the a...           715160   \n",
              "73281  We present an LFG-DOP parser which uses fragme...          1356246   \n",
              "73282  The processes through which readers evoke ment...          7277828   \n",
              "73283  This paper examines the extent to which verb d...          1829043   \n",
              "73284  Spoken dialogue managers have benefited from u...         10903652   \n",
              "\n",
              "                                       pdf_hash  numcitedby  \\\n",
              "0      0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
              "1      8d5e31610bc82c2abc86bc20ceba684c97e66024          93   \n",
              "2      3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
              "3      b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
              "4      011e943b64a78dadc3440674419821ee080f0de3          12   \n",
              "...                                         ...         ...   \n",
              "73280  ab17a01f142124744c6ae425f8a23011366ec3ee          11   \n",
              "73281  ad005b3fd0c867667118482227e31d9378229751          12   \n",
              "73282  924cf7a4836ebfc20ee094c30e61b949be049fb6          14   \n",
              "73283  6b1f6f28ee36de69e8afac39461ee1158cd4d49a          92   \n",
              "73284  483c818c09e39d9da47103fbf2da8aaa7acacf01         330   \n",
              "\n",
              "                                                     url  \\\n",
              "0                      https://aclanthology.org/O02-2002   \n",
              "1      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
              "2                      https://aclanthology.org/R13-1042   \n",
              "3                      https://aclanthology.org/W05-0819   \n",
              "4      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
              "...                                                  ...   \n",
              "73280                  https://aclanthology.org/P99-1002   \n",
              "73281                  https://aclanthology.org/P00-1009   \n",
              "73282                  https://aclanthology.org/P99-1056   \n",
              "73283                  https://aclanthology.org/P99-1051   \n",
              "73284                  https://aclanthology.org/P00-1013   \n",
              "\n",
              "                                            publisher  \\\n",
              "0                                                None   \n",
              "1      European Language Resources Association (ELRA)   \n",
              "2                       INCOMA Ltd. Shoumen, BULGARIA   \n",
              "3           Association for Computational Linguistics   \n",
              "4      European Language Resources Association (ELRA)   \n",
              "...                                               ...   \n",
              "73280       Association for Computational Linguistics   \n",
              "73281       Association for Computational Linguistics   \n",
              "73282       Association for Computational Linguistics   \n",
              "73283       Association for Computational Linguistics   \n",
              "73284       Association for Computational Linguistics   \n",
              "\n",
              "                                  address  year  ... number volume journal  \\\n",
              "0                                    None  2002  ...   None   None    None   \n",
              "1      Las Palmas, Canary Islands - Spain  2002  ...   None   None    None   \n",
              "2                        Hissar, Bulgaria  2013  ...   None   None    None   \n",
              "3                     Ann Arbor, Michigan  2005  ...   None   None    None   \n",
              "4      Las Palmas, Canary Islands - Spain  2002  ...   None   None    None   \n",
              "...                                   ...   ...  ...    ...    ...     ...   \n",
              "73280         College Park, Maryland, USA  1999  ...   None   None    None   \n",
              "73281                           Hong Kong  2000  ...   None   None    None   \n",
              "73282         College Park, Maryland, USA  1999  ...   None   None    None   \n",
              "73283         College Park, Maryland, USA  1999  ...   None   None    None   \n",
              "73284                           Hong Kong  2000  ...   None   None    None   \n",
              "\n",
              "      editor  isbn      ENTRYTYPE                                ID language  \\\n",
              "0       None  None  inproceedings               chen-you-2002-study     None   \n",
              "1       None  None  inproceedings       mihalcea-2002-bootstrapping     None   \n",
              "2       None  None  inproceedings  jamison-gurevych-2013-headerless     None   \n",
              "3       None  None  inproceedings   aswani-gaizauskas-2005-aligning     None   \n",
              "4       None  None  inproceedings         suyaga-etal-2002-proposal     None   \n",
              "...      ...   ...            ...                               ...      ...   \n",
              "73280   None  None  inproceedings              furui-1999-automatic     None   \n",
              "73281   None  None  inproceedings                 bod-2000-improved     None   \n",
              "73282   None  None  inproceedings         lange-content-1999-grapho     None   \n",
              "73283   None  None  inproceedings             lapata-1999-acquiring     None   \n",
              "73284   None  None  inproceedings              roy-etal-2000-spoken     None   \n",
              "\n",
              "       note                             Model Predicted Topics  \n",
              "0      None                                                NaN  \n",
              "1      None                                                NaN  \n",
              "2      None      Computational Social Science and Social Media  \n",
              "3      None                                                NaN  \n",
              "4      None                                                NaN  \n",
              "...     ...                                                ...  \n",
              "73280  None                           Speech and Multimodality  \n",
              "73281  None              Syntax: Tagging, Chunking and Parsing  \n",
              "73282  None  Linguistic Theories, Cognitive Modeling and Ps...  \n",
              "73283  None  Linguistic Theories, Cognitive Modeling and Ps...  \n",
              "73284  None                   Dialogue and Interactive Systems  \n",
              "\n",
              "[73285 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eba62a64-96cc-48a2-8aec-f0d3ff17d31f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acl_id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "      <th>corpus_paper_id</th>\n",
              "      <th>pdf_hash</th>\n",
              "      <th>numcitedby</th>\n",
              "      <th>url</th>\n",
              "      <th>publisher</th>\n",
              "      <th>address</th>\n",
              "      <th>year</th>\n",
              "      <th>...</th>\n",
              "      <th>number</th>\n",
              "      <th>volume</th>\n",
              "      <th>journal</th>\n",
              "      <th>editor</th>\n",
              "      <th>isbn</th>\n",
              "      <th>ENTRYTYPE</th>\n",
              "      <th>ID</th>\n",
              "      <th>language</th>\n",
              "      <th>note</th>\n",
              "      <th>Model Predicted Topics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O02-2002</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>18022704</td>\n",
              "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
              "      <td>14</td>\n",
              "      <td>https://aclanthology.org/O02-2002</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2002</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>chen-you-2002-study</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L02-1310</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>8220988</td>\n",
              "      <td>8d5e31610bc82c2abc86bc20ceba684c97e66024</td>\n",
              "      <td>93</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>Las Palmas, Canary Islands - Spain</td>\n",
              "      <td>2002</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>mihalcea-2002-bootstrapping</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R13-1042</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>16703040</td>\n",
              "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
              "      <td>10</td>\n",
              "      <td>https://aclanthology.org/R13-1042</td>\n",
              "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
              "      <td>Hissar, Bulgaria</td>\n",
              "      <td>2013</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>jamison-gurevych-2013-headerless</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Computational Social Science and Social Media</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>W05-0819</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>1215281</td>\n",
              "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
              "      <td>15</td>\n",
              "      <td>https://aclanthology.org/W05-0819</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>Ann Arbor, Michigan</td>\n",
              "      <td>2005</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>aswani-gaizauskas-2005-aligning</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L02-1309</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>18078432</td>\n",
              "      <td>011e943b64a78dadc3440674419821ee080f0de3</td>\n",
              "      <td>12</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>Las Palmas, Canary Islands - Spain</td>\n",
              "      <td>2002</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>suyaga-etal-2002-proposal</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73280</th>\n",
              "      <td>P99-1002</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>715160</td>\n",
              "      <td>ab17a01f142124744c6ae425f8a23011366ec3ee</td>\n",
              "      <td>11</td>\n",
              "      <td>https://aclanthology.org/P99-1002</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>College Park, Maryland, USA</td>\n",
              "      <td>1999</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>furui-1999-automatic</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Speech and Multimodality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73281</th>\n",
              "      <td>P00-1009</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>1356246</td>\n",
              "      <td>ad005b3fd0c867667118482227e31d9378229751</td>\n",
              "      <td>12</td>\n",
              "      <td>https://aclanthology.org/P00-1009</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>2000</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>bod-2000-improved</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Syntax: Tagging, Chunking and Parsing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73282</th>\n",
              "      <td>P99-1056</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>7277828</td>\n",
              "      <td>924cf7a4836ebfc20ee094c30e61b949be049fb6</td>\n",
              "      <td>14</td>\n",
              "      <td>https://aclanthology.org/P99-1056</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>College Park, Maryland, USA</td>\n",
              "      <td>1999</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>lange-content-1999-grapho</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Linguistic Theories, Cognitive Modeling and Ps...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73283</th>\n",
              "      <td>P99-1051</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>1829043</td>\n",
              "      <td>6b1f6f28ee36de69e8afac39461ee1158cd4d49a</td>\n",
              "      <td>92</td>\n",
              "      <td>https://aclanthology.org/P99-1051</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>College Park, Maryland, USA</td>\n",
              "      <td>1999</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>lapata-1999-acquiring</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Linguistic Theories, Cognitive Modeling and Ps...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73284</th>\n",
              "      <td>P00-1013</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>10903652</td>\n",
              "      <td>483c818c09e39d9da47103fbf2da8aaa7acacf01</td>\n",
              "      <td>330</td>\n",
              "      <td>https://aclanthology.org/P00-1013</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>2000</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>inproceedings</td>\n",
              "      <td>roy-etal-2000-spoken</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Dialogue and Interactive Systems</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73285 rows × 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eba62a64-96cc-48a2-8aec-f0d3ff17d31f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eba62a64-96cc-48a2-8aec-f0d3ff17d31f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eba62a64-96cc-48a2-8aec-f0d3ff17d31f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-674e69e3-b1e7-448a-9b5e-d710ac1796a9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-674e69e3-b1e7-448a-9b5e-d710ac1796a9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-674e69e3-b1e7-448a-9b5e-d710ac1796a9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c6daf045-4b0d-488d-a9c4-2ee01a9ec35d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c6daf045-4b0d-488d-a9c4-2ee01a9ec35d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "oA0EmrKWfDff"
      },
      "outputs": [],
      "source": [
        "data_clean=pd.DataFrame(columns=[\"acl_id\",\"title\",\"abstract\",\"full_text\",\"language\",\"publisher\",\"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "KdW2ef55fDdC"
      },
      "outputs": [],
      "source": [
        "data_clean[\"acl_id\"]=data[\"acl_id\"]\n",
        "data_clean[\"title\"]=data[\"title\"]\n",
        "data_clean[\"abstract\"]=data[\"abstract\"]\n",
        "data_clean[\"full_text\"]=data[\"full_text\"]\n",
        "data_clean[\"language\"]=data[\"language\"]\n",
        "data_clean[\"publisher\"]=data[\"publisher\"]\n",
        "data_clean[\"url\"]=data[\"url\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gVP0yZbSfNju",
        "outputId": "ba5bb4ed-ea2f-4532-dd49-27e40826774d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         acl_id                                              title  \\\n",
              "0      O02-2002  A Study on Word Similarity using Context Vecto...   \n",
              "1      L02-1310           Bootstrapping Large Sense Tagged Corpora   \n",
              "2      R13-1042  Headerless, Quoteless, but not Hopeless? Using...   \n",
              "3      W05-0819  Aligning Words in {E}nglish-{H}indi Parallel C...   \n",
              "4      L02-1309  Proposal of a very-large-corpus acquisition me...   \n",
              "...         ...                                                ...   \n",
              "73280  P99-1002  Automatic Speech Recognition and Its Applicati...   \n",
              "73281  P00-1009  An Improved Parser for Data-Oriented Lexical-F...   \n",
              "73282  P99-1056  The grapho-phonological system of written {F}r...   \n",
              "73283  P99-1051  Acquiring Lexical Generalizations from Corpora...   \n",
              "73284  P00-1013  Spoken Dialogue Management Using Probabilistic...   \n",
              "\n",
              "                                                abstract  \\\n",
              "0      There is a need to measure word similarity whe...   \n",
              "1                                                   None   \n",
              "2      Thread disentanglement is the task of separati...   \n",
              "3      In this paper, we describe a word alignment al...   \n",
              "4                                                   None   \n",
              "...                                                  ...   \n",
              "73280  This paper describes recent progress and the a...   \n",
              "73281  We present an LFG-DOP parser which uses fragme...   \n",
              "73282  The processes through which readers evoke ment...   \n",
              "73283  This paper examines the extent to which verb d...   \n",
              "73284  Spoken dialogue managers have benefited from u...   \n",
              "\n",
              "                                               full_text language  \\\n",
              "0      There is a need to measure word similarity whe...     None   \n",
              "1                                                   None     None   \n",
              "2      Thread disentanglement is the task of separati...     None   \n",
              "3      In this paper, we describe a word alignment al...     None   \n",
              "4                                                   None     None   \n",
              "...                                                  ...      ...   \n",
              "73280  This paper describes recent progress and the a...     None   \n",
              "73281  We present an LFG-DOP parser which uses fragme...     None   \n",
              "73282  The processes through which readers evoke ment...     None   \n",
              "73283  This paper examines the extent to which verb d...     None   \n",
              "73284  Spoken dialogue managers have benefited from u...     None   \n",
              "\n",
              "                                            publisher  \\\n",
              "0                                                None   \n",
              "1      European Language Resources Association (ELRA)   \n",
              "2                       INCOMA Ltd. Shoumen, BULGARIA   \n",
              "3           Association for Computational Linguistics   \n",
              "4      European Language Resources Association (ELRA)   \n",
              "...                                               ...   \n",
              "73280       Association for Computational Linguistics   \n",
              "73281       Association for Computational Linguistics   \n",
              "73282       Association for Computational Linguistics   \n",
              "73283       Association for Computational Linguistics   \n",
              "73284       Association for Computational Linguistics   \n",
              "\n",
              "                                                     url  \n",
              "0                      https://aclanthology.org/O02-2002  \n",
              "1      http://www.lrec-conf.org/proceedings/lrec2002/...  \n",
              "2                      https://aclanthology.org/R13-1042  \n",
              "3                      https://aclanthology.org/W05-0819  \n",
              "4      http://www.lrec-conf.org/proceedings/lrec2002/...  \n",
              "...                                                  ...  \n",
              "73280                  https://aclanthology.org/P99-1002  \n",
              "73281                  https://aclanthology.org/P00-1009  \n",
              "73282                  https://aclanthology.org/P99-1056  \n",
              "73283                  https://aclanthology.org/P99-1051  \n",
              "73284                  https://aclanthology.org/P00-1013  \n",
              "\n",
              "[73285 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-205e5a0e-d61c-47d4-b58a-b5f7bcea692b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acl_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "      <th>language</th>\n",
              "      <th>publisher</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O02-2002</td>\n",
              "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://aclanthology.org/O02-2002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L02-1310</td>\n",
              "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R13-1042</td>\n",
              "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>None</td>\n",
              "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
              "      <td>https://aclanthology.org/R13-1042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>W05-0819</td>\n",
              "      <td>Aligning Words in {E}nglish-{H}indi Parallel C...</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/W05-0819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L02-1309</td>\n",
              "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73280</th>\n",
              "      <td>P99-1002</td>\n",
              "      <td>Automatic Speech Recognition and Its Applicati...</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73281</th>\n",
              "      <td>P00-1009</td>\n",
              "      <td>An Improved Parser for Data-Oriented Lexical-F...</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P00-1009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73282</th>\n",
              "      <td>P99-1056</td>\n",
              "      <td>The grapho-phonological system of written {F}r...</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73283</th>\n",
              "      <td>P99-1051</td>\n",
              "      <td>Acquiring Lexical Generalizations from Corpora...</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73284</th>\n",
              "      <td>P00-1013</td>\n",
              "      <td>Spoken Dialogue Management Using Probabilistic...</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P00-1013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73285 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-205e5a0e-d61c-47d4-b58a-b5f7bcea692b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-205e5a0e-d61c-47d4-b58a-b5f7bcea692b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-205e5a0e-d61c-47d4-b58a-b5f7bcea692b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-df3e1e7d-b4e7-4c1a-8e54-d27cb38abc52\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df3e1e7d-b4e7-4c1a-8e54-d27cb38abc52')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-df3e1e7d-b4e7-4c1a-8e54-d27cb38abc52 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6be2e211-95cb-49a2-bfed-f9aaff1da044\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_clean')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6be2e211-95cb-49a2-bfed-f9aaff1da044 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_clean');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_clean",
              "summary": "{\n  \"name\": \"data_clean\",\n  \"rows\": 73285,\n  \"fields\": [\n    {\n      \"column\": \"acl_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 73285,\n        \"samples\": [\n          \"L08-1302\",\n          \"L10-1106\",\n          \"W05-1303\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 72510,\n        \"samples\": [\n          \"Lexicalized {TAG}s, Parsing and Lexicons\",\n          \"{C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies\",\n          \"{NETL}: A System for Representing and Using Real-World Knowledge\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 67465,\n        \"samples\": [\n          \"Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD 1 , a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into stateof-the-art solutions.\",\n          \"We consider the gap between user demands for seamless handling of complex interactions, and recent advances in dialog state tracking technologies. We propose a new statistical approach, Task Lineage-based Dialog State Tracking (TL-DST), aimed at seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. TL-DST consists of three components: (1) task frame parsing, (2) context fetching and (3) task state update (for which TL-DST takes advantage of previous work in dialog state tracking). There is at present very little publicly available multi-task, complex goal dialog data; however, as a proof of concept, we applied TL-DST to the Dialog State Tracking Challenge (DSTC) 2 data, resulting in state-of-the-art performance. TL-DST also outperforms the DSTC baseline tracker on a set of pseudo-real datasets involving multiple tasks with complex goals which were synthesized using DSTC3 data.\",\n          \"Les mod\\u00e8les vectoriels de s\\u00e9mantique distributionnelle (ou word embeddings), notamment ceux produits par les m\\u00e9thodes neuronales, posent des questions de reproductibilit\\u00e9 et donnent des repr\\u00e9sentations diff\\u00e9rentes \\u00e0 chaque utilisation, m\\u00eame sans modifier leurs param\\u00e8tres. Nous pr\\u00e9sentons ici un ensemble d'exp\\u00e9rimentations permettant de mesurer cette instabilit\\u00e9, \\u00e0 la fois globalement et localement. Globalement, nous avons mesur\\u00e9 le taux de variation du voisinage des mots sur trois corpus diff\\u00e9rents, qui est estim\\u00e9 autour de 17% pour les 25 plus proches voisins d'un mot. Localement, nous avons identifi\\u00e9 et caract\\u00e9ris\\u00e9 certaines zones de l'espace s\\u00e9mantique qui montrent une relative stabilit\\u00e9, ainsi que des cas de grande instabilit\\u00e9.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 67378,\n        \"samples\": [\n          \"Statistical Relational Learning (SRL) is an interdisciplinary research area that combines firstorder logic and machine learning methods for probabilistic inference. Although many Natural Language Processing (NLP) tasks (including text classification, semantic parsing, information extraction, coreference resolution, and sentiment analysis) can be formulated as inference in a firstorder logic, most probabilistic firstorder logics are not efficient enough to be used for largescale versions of these tasks. In this tutorial, we provide a gentle introduction to the theoretical foundation of probabilistic logics, as well as their applications in NLP. We describe recent advances in designing scalable probabilistic logics, with a special focus on ProPPR. Finally, we provide a handson demo about scalable probabilistic logic programming for solving practical NLP problems. Outline: \\u2022 Part 1: Foundations and Applications of Probabilistic FirstOrder Logic \\u2022 We will provide a brief review of some firstorder learning systems that have been developed in the past: Markov Logic Networks (Richardson and Domingos, 2006) , Stochastic Logic Programs (Muggleton, 1996) . In this part, we introduce the semantics of the above languages with their inference (and learning) approaches. We analyze and discuss the core ideas behind of such language. We show various applications of probabilistic logics in NLP. \\u2022 Part 2: Scalable Probabilistic Logics: A Case Study of ProPPR. \\u2022 We will focus on the efficiency issue, and introduce recent advances of scalable probabilistic logics, including lifted inference techniques (Van den Broeck and Suciu, 2014) and probabilistic soft logic (Bach et al., 2015) . In particular, we will take CMU's ProPPR (Wang et al., 2013) as a case study. We describe the main contributions of ProPPR: including its approximate personalized PageRank inference scheme, parallel stochastic gradient descent learning method, and its flexibility in theory engineering. We then introduce the structure learning methods in ProPPR (Wang et al., CIKM 2014) , including a structured regularization method as an alternative to predicate invention (Wang et al., IJCAI 2015) . We will also cover our latest attempt of learning firstorder logic formula embeddings, and discuss its relationship to (and possible connections between) even newer approaches to modeling knowledge bases, relationships, and inference using deep learning methods. To conclude this part, we show an interesting application of ProPPR (Wang et al., ACLIJCNLP 2015) : a joint information extraction and knowledge reasoning engine. \\u2022 Part 3: Demos and Practical Applications. \\u2022 We switch from the theoretical presentations to an interactive demonstration session: we aim at providing a handson lab session to transfer the theories of scalable probabilistic logics into practices. More specifically, we will provide a demo of several applications on synthetic and realworld datasets. Participants are encouraged to check out our repository on Github ( https://github.com/TeamCohen/ProPPR ) and bring laptops to the tutorial. The list of demo examples to be considered are text categorization, entity resolution, knowledge base completion (Wang et al., MLJ 2015) , dependency parsing (Wang et al., EMNLP 2014) extraction, text categorization and learning from large datasets. He has a longstanding interest in statistical relational learning and learning models, or learning from data, that display nontrivial structure. He holds seven patents related to learning, discovery, information retrieval, and data integration, and is the author of more than 200 publications. He was a past president of International Machine Learning Society. He is a AAAI fellow, and was a winner of SIGMOD Test of Time Award and SIGIR Test of Time Award.\",\n          \"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F 1 , macro-F 1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model. Introduction Relation classification (RC) models are typically compared with either micro-F 1 or macro-F 1 , often without discussing the measure's properties (see e.g. Zhang et al., 2017; Yao et al., 2019) . Each measure highlights different aspects of model performance (Sun et al., 2009) . However, using an inappropriate measure can lead to the preference of an unsuitable model (Branco et al., 2016) , e.g., tasks with an imbalanced or long-tailed class distribution. We argue that model evaluation should better reflect this, particularly as rare phenomena become more important in NLP (Rogers, 2021) . For instance, popular datasets for RC, such as TACRED (Zhang et al., 2017) , NYT (Riedel et al., 2010) , ChemProt (Kringelum et al., 2016) , Do-cRED (Yao et al., 2019) , and SemEval-2010 Task 8 (Hendrickx et al., 2010) , often exhibit a highly imbalanced label distribution (see Table 1 and, e.g., the TACRED class distribution 1 ). The main reasons are the natural data imbalance, i.e. the occurrence frequency of relation mentions in text, as well as the incompleteness of knowledge graphs like Freebase (Bollacker et al., 2008) used in distantly supervised RC. For example, 58% of the relations in the NYT dataset (Riedel et al., 2010) have fewer than 100 training instances (Han et al., 2018) , and the most frequent relation location/contains is assigned to 48.3% of the positive test instances. However, for applying RC to real-world problems, it is especially important to discover instances of relations that are not yet covered well in a given knowledge base. Table 1 lists statistics of the aforementioned RC datasets, including their perplexity and common evaluation measures. TACRED and the original version of NYT contain predominantly negative samples 2 . All datasets, except for undirectional SemEval, exhibit a large ratio between most frequent and least frequent positive class in the test set. The perplexity of test set distributions is also much lower than the relation count for all datasets except SemEval. Reporting only a single measure therefore cannot exhaustively capture model performance on these datasets, especially for the long tail of relation types. For example, Alt et al. (2019) show that on the NYT dataset, AUC scores and P-R-Curves of several state-of-the-art models are heavily skewed towards the two most frequent relation types location/contains and person/nationality. TACRED, ChemProt, DocRED and SemEval results are usually only reported in micro-F 1 , which does not consider class membership. In this paper, we introduce a framework for weighting schemes of measures to address these evaluation deficits. We present and motivate two new weighting schemes that are in between the extremes of micro-and macro-weighting. We demonstrate these, micro-, class-weighted-and macro-F 1 on TACRED and SemEval with two popular models each. We show that more information about models can be inferred from our results and point out what further steps should be taken to improve evaluation in relation classification. Methods We first give background on the F 1 -score and existing F 1 weighting schemes. We present our framework of weighting schemes. We introduce two new weighting schemes. Finally, we outline statistical tests. Background The F \\u03b2 -score (Rijsbergen, 1979; Lewis and Gale, 1994) calculates a score in the interval [0, 1] through the formula F \\u03b2 = (1 + \\u03b2 2 ) \\u2022 T P (1 + \\u03b2 2 ) \\u2022 T P + \\u03b2 2 \\u2022 F N + F P (1) with the true positives (TP), false negatives (FN) and false positives (FP) of a confusion matrix. This definition is identical to the weighted harmonic mean of precision and recall. The positive coefficient \\u03b2 is used as a trade-off between the error types FN and FP. If there is no preference known or pre-determined, this coefficient is usually set to 1. In multi-class classification the confusion matrix can either be calculated once for the whole dataset, or separately for each class. The former method yields micro-F 1 . Micro weighting does not consider class membership for any test sample. If the predictions and labels of all classes are considered, micro-F 1 is equal to accuracy, as the denominator in Eq. 1 is twice the dataset. In RC, the TP of the negative class are usually not considered, in which case micro-F 1 is not equal to accuracy. For the F -score, micro is the only weighting where the impact of a sample on the score is not conditioned on the model performance on the rest of the class (Forman and Scholz, 2010). If the test set is considered to have a representative data distribution, the microweighted score is a frequentist evaluation of model performance. There exist two other ways to calculate and combine F 1 -scores for a multi-class problem. First, multi-class F 1 -scores can be calculated for each class and then a weighted average class score is taken. Second, precision and recall scores for each class can be calculated and weighted, then the harmonic mean of weighted precision and weighted recall is taken. Opitz and Burst (2019) show that the first method is more robust and less favorable to biased classifiers. We use this method in our proposed framework. (Class-)weighted-F 1 is similar to micro-F 1 . F 1scores are calculated for each class individually and then weighted by the class count. Thus, both schemes approximately weigh all samples equally. Macro weighting gives an equal weight for each class with positive sample count regardless of the specific sample count. This gives information about model performance if class imbalance is not considered. In general, there is a correspondence between training loss and evaluation measure (Li et al., 2020) . One disadvantage of multiple weighting schemes is that each weighting scheme can be optimized for. To achieve a better score for a specific weighting, class weights could be set proportional to the weighting of the class during training. How-  ever, we argue that model results should always be presented with multiple weightings for one dataset. Especially, when comparing different models all weightings should be reported for each model. This can clarify whether a model is good for all weightings or just micro or macro. Furthermore, with datasets that are currently evaluated with different weightings, it is easier to identify whether a model is specifically good for a dataset or for a weighting. Framework for Weighting Schemes We discuss a framework that summarizes the rules we give to class-weighting schemes. Then we introduce two new class weighting schemes. All discussed weighting schemes can be found in Table 2. They are independent of the measure that is used to calculate a score for each class. (Class-)weighted and macro weighting are the extremes of \\\"degressive proportionality\\\" 3 or \\\"allocation functions\\\" (S\\u0142omczy\\u0144ski and \\u017byczkowski, 2012) . These are, e.g., used by the European Parliament to allocate seats to member nations depending on the population of the nation. They state that allocation should be monotonic increasing (see D1) and proportionally decreasing (see D2). To adopt this to a weighting scheme for multi-class evaluation, we add a normalizing desideratum that determines the sum of weights over all classes to be 1 (see D0). Let n i > 0 be the count of samples of class i and w i \\u2265 0 the weight assigned to the score of class i. 3 https://eur-lex.europa.eu/ legal-content/EN/TXT/HTML/?uri=CELEX: 32013D0312&from=EN#d1e114-57-1 We have the following desiderata: i w i = 1 (D0) n i \\u2265 n j \\u21d2 w i \\u2265 w j (D1) n i \\u2265 n j \\u21d2 w i n i \\u2264 w j n j (D2) Note that these desiderata do not restrict the scoring function that assigns scores s i to class i. The weighted evaluation score is then given by i w i s i . Weighting Schemes Macro: Macro weighting is one extreme by setting equality on the weights of desideratum D1. It implies that we do not consider the instance counts per class, but treat all classes equally. (Class-)weighted: Class-weighted is the other extreme by setting equality on the fraction of weights and counts in desideratum D2. It implies that we do not consider class constituency but weight all samples equally. Dodrans: Cao et al. (2019) demonstrate that their balanced generalization error bound for binary classifiers in the separable case can be optimized by setting margins proportional to n i \\u22121/4 . They use this derivation from a limited theoretical scenario to improve the performance of several classifiers on imbalanced multi-class datasets. A term proportional to n i \\u22121/4 is added in the loss function. While this added term is not directly transferable, we propose adapting this as a multiplicative factor in weighting classes for multi-class evaluation: w i \\u221d n i \\u22121/4 n i = n 3/4 i . We coin this weighting dodrans (\\\"three-quarter\\\"). Entropy: We also want to provide a weighting scheme that takes into consideration how hard a class is to predict. To this end, we propose weighting classes proportional to their term in the Shannon entropy formula H(X) = \\u2212 i P (x i ) log(P (x i )) (2) w i \\u221d P (x i ) log(P (x i )). (3) We interpret P (x i ) for class i to be the probability of it appearing in the dataset, s.t. P (x i ) = n i / j n j . Thus, without normalization the model score is now the sum over all classes of the model performance on a class times the difficulty and frequency of the class. Note, that this weighting scheme does not fulfil desideratum D1, since it is decreasing for classes i with P (x i ) > e \\u22121 . This is related to the fact that classes that are too large become easier to predict for a model, the model can just default to predicting this class. It can also be desirable that a class does not gain relative importance once it contains more than half of the dataset. For RC, this often has little consequence. If we include NA in the normalization, it is usually the largest class and other classes are below an e-th of the dataset. Table 2 shows an overview of the mentioned schemes. Figure 1 displays the weights that these schemes assign to the classes of the TACRED test set. The weighted scheme is proportional to class counts and produces the most imbalanced weights. Dodrans and entropy produce slightly more balanced weights and differ from weighted for the most frequent classes. Macro considers all classes equally, regardless of class count. Statistical Testing Currently, most RC works report a single score for each dataset. This can be the result from a single run or the median score from multiple runs. However, this does not allow to measure how large the difference between models is. Recently, analysis papers in NLP have recorded mean and standard deviation over multiple runs (Madhyastha and Jain, 2019; Zhou et al., 2020) , as this allows for statistical tests. We first test for significance and report p-values. We employ Welch's t-test to test the hypothesis that the models have equal mean. Following Zhu et al. (2020) , we also report Cohen's d effect size to determine how large the difference between models is for a specific measure. For two models with the Figure 1 : TACRED relations and their respective weights under different weighting schemes. The lower x-axis denotes the normalized weight given to a relation for a scheme. The upper x-axis corresponds to the counts of the relations in the test set for the classweighted scheme. The y-axis denotes all positive relations. The negative NA class is not listed and has 12184 samples. The entropy and dodrans weighting scheme produce similar weights and are between weighted and macro weighting. same number n > 1 of runs, Cohen's d is given by d = \\u221a 2 \\u00b5 1 \\u2212 \\u00b5 2 \\u03c3 2 1 + \\u03c3 2 2 (4) with \\u00b5 i and \\u03c3 2 i being mean and variance of model i's scores. We do this, as two different models never perform exactly the same, i.e. significance just depends on the number of runs and we also want to score the difference between the models. Experiments We evaluate and compare three RC methods with our proposed measures on two datasets. We choose these methods, as RECENT (Lyu and Chen, 2021) and BERT EM (Baldini Soares et al., 2019) are based on vanilla fine-tuning of a pre-trained language model, with a classification head on top. PTR (Han et al., 2021) this way we can compare performance of the two paradigms for other weightings. RECENT proposes a model-agnostic paradigm that exploits entity types to narrow down the candidate relations. Given an entity-type combination, a separate classifier is trained on the restricted classes. Baldini Soares et al. ( 2019 ) compare various strategies that extract relation representation from Transformers and claim ENTITY START (i.e. insert entity markers at the start of two entity mentions) yields the best performance. PTR also takes entity types into consideration and constructs prompts composed of three subprompts, two corresponding to the fill-in of the entity types and one predicting the relation. In our experiments we use RECENT GCN for RE-CENT, BERT EM with ENTITY START, and unreversed prompts for PTR. We use the official repositories for RECENT and PTR, we reimplement BERT EM 4 . We use the hyperparameters proposed in the original papers and conduct five runs for each model. Additional implementation and training details can be found in Appendices A and B. The main focus is unearthing performance information about these methods that was previously obscured by single score measures. The number of weighting schemes does not influence the computational cost, as each score is determined through the predictions in a run and does not require specific tuning. 5 We acknowledge that each weighting scheme could be optimized for during training which gives additional importance to reporting multiple measures for each model. Results Table 3 shows results for TACRED. PTR significantly outperforms RECENT across all weighting schemes. The difference between the models is smallest for micro-F 1 and increases for all schemes that weigh classes more equally. For macro-F 1 the difference is starkest with effect size 24.2. Table 4 displays results for SemEval. BERT EM significantly outperforms PTR in the micro-F 1 measure and all other weightings except for macro-F 1 . All effect sizes are either large or huge, by far the largest effect size is between PTR and BERT EM regarding macro-F 1 though. The Sem-Eval test set contains a single sample of the Entity-Destination(e2,e1) class which is quite impactful for the macro-F 1 of the models but has negligible impact on all other weighting schemes. The scores from dodrans and entropy indicate that only if all classes are considered equally important the PTR model should be preferred. This indicates that either the PTR model learns almost regardless of class frequency or BERT EM has a class preference that is only discoverable with macro-F 1 . We demonstrate that evaluation on micro-F 1 does not give adequate information about model performance on long-tail classes. In Tables 3 and  4 we see that the model which performs better under micro-F 1 can either be significantly better or worse for classes with few samples. The weighted-F 1 produces similar results to micro-F 1 except for RECENT. Macro-F 1 on the other hand is very sensitive to model performance on single samples, e.g. the Entity-Destination(e2,e1) class in SemEval. The scores of our proposed schemes are in between the existing measures and might be the best indicators for robust generalization performance. For all experiments, they produce similar results to each other. This could just be a coincidence of the datasets, and is also indicated by Figure 1 . Overall, it might be fair to say that one of the former and latter measures is enough. It would mean one measure that does weigh proportional to sample count (micro-or weighted-F 1 ), an intermediary measure (dodrans-F 1 or entropy-F 1 ) and macro-F 1 . PTR performs better for macro-F 1 on both datasets. Its scores decrease less when classes are weighted more equally. This suggests that it is a better model for classes with low sample counts. Le Scao and Rush (2021) show that prompts can be worth hundreds of data points which would explain why the macro-and micro-F 1 scores are much closer together than for RECENT and BERT EM . Related Work Chauhan et al. ( 2019 ) do a thorough evaluation of their model and notice the significantly different performance measured by micro and macro statistics due to the class imbalance, suggesting that the choice of evaluation measure is crucial. Huang and Wong (2020) further use the closeness between micro-and macro-F 1 scores to claim the stable performance of their model. Mille et al. (2021) point out that evaluating with a single score favors overfitting. They show different evaluation suites that can be created for a dataset. Bragg et al. (2021) address the disjoint evaluation settings across recent research threads in (few-shot) NLP and propose a unified evaluation benchmark which regulates dataset, sample size etc., but fail to take the evaluation measure into consideration, reporting only mean accuracy instead. Post (2018) criticises the inconsistency and under-specification in reporting scores. This problem is also prevalent in RC where the F 1 weighting scheme is often not specified. Zhang et al. (2020) show that bias from corpora persists for fine-tuned pre-trained language models. These models struggle with rare phenomena. For better performance debiasing with weighting is performed. S\\u00f8gaard et al. (2021) argue against using random splits. They show that evaluating models with random splits is not a realistic setting but makes tasks easier by fixing the test data distribution to the train data distribution. Long-tail evaluation is becoming more prominent in NLP research. Models in deep learning tend to show a gap in performance between frequent and infrequent phenomena (Rogers, 2021) . Models in NLP have been shown to perform badly on specific subsets of data (Zhang et al., 2020) . Sokolova and Lapalme (2009) analyze measures for multi-class classification and present invariances regarding the confusion matrix. G\\u00f6sgens et al. (2021) also determine which class measures (including F 1 ) fulfil specific assumptions. Further evaluation can be based on this. Our weighting schemes for F 1 can be transferred to other measures that calculate a score for each class. Outlook We suggest creating and using a bidimensional leaderboard like Kasai et al. (2021) where measures and models can be contributed. To this end, benchmarking of RC models could be done on a centralized site where a model or test set predictions are submitted and measures are calculated automatically through a script. For measures that modify weighting of classes and intra-class scoring, this does not require additional training computation. Due to the reproducibility crisis (Baker, 2016) , not all state-of-the-art scores can be replicated. Possible future work includes a comprehensive evaluation study of papers on leaderboards of RC tasks. This would enable an in-depth discussion of strength and weaknesses (including reproducibil-ity) of these models. The analysis we present can also be extended to other NLP tasks with imbalanced datasets, such as named entity recognition (Tjong Kim Sang and De Meulder, 2003) , part-of-speech tagging (Pradhan et al., 2013) and coreference resolution (Pradhan et al., 2012) . Conclusion We criticise the current practice of reporting a single score when evaluating imbalanced RC datasets. We propose a new framework to weight scores for multi-class evaluation of imbalanced datasets. We provide two new weighting schemes, dodrans and entropy, which are positioned between classweighted and macro. In our experiments, we show that model performance on both TACRED and SemEval, especially on the long-tail relations, is not adequately captured by a single score. Thus, we advocate the use of multiple weighing schemes when reporting model performance on imbalanced datasets. Acknowledgments We would like to thank Nils Feldhus, Sebastian M\\u00f6ller, Lisa Raithel, Robert Schwarzenberg and the anonymous reviewers for their feedback on the paper. This work was partially supported by the German Federal Ministry of Education and Research as part of the project CORA4NLP (01IW20010) and by the German Federal Ministry for Economic Affairs and Climate Action as part of the project PLASS (01MD19003E). Christoph Alt is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2002/1 \\\"Science of Intelligence\\\" -project number 390523135. A Implementation Details To evaluate RECENT and PTR, we use the official code at https://github.com//Saintfe/ RECENT (last updated on 01.10.2021) and https: //github.com/thunlp/PTR (last updated on 20.11.2021). Since the official code of BERT EM is not available, we implement this method using the HuggingFace Transformers library (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) , and make our code base available at https://github. com/dfki-nlp/mtb-bert-em. To make our results reproducible, we randomly generated seeds {9, 148, 378, 459, 687} and employed these for all models in their 5 runs. B Training Details B.1 RECENT We consider GCN as the base model. Following the paper and the official code, we set the batch size to be 50, the optimizer to be SGD with learning rate 0.3, and the number of epochs to be 100. It takes a single RTX-A6000 GPU approximately 10 hours to complete all 5 runs on TACRED. B.2 BERT EM We use the pre-trained language model (PLM) bert-large-uncased from the HuggingFace model hub and directly fine-tune the model for the RC task, without matching-the-blank pre-training. As the paper suggests, we set the batch size to be 64, the optimizer to be Adam with learning rate 3 \\u2022 10 \\u22125 , and the number of epochs to be 5. Additionally, we use the max sequence length of 512. It takes a single RTX-A6000 GPU 30 minutes to complete all 5 runs on SemEval. B.3 PTR According to the paper and the official code base, we apply the same settings to evaluate both TACRED and SemEval: We use the PLM roberta-large and set the max sequence length to be 512, the batch size to be 64, the optimizer to be Adam with learning rate 3 \\u2022 10 \\u22125 , the weight decay to be 10 \\u22122 , and the number of epochs to be 5. It takes 4 Quadro-P5000 GPUs 84 hours to complete 5 runs on TACRED, and it takes 8 Titan-V GPUs 9 hours on SemEval.\",\n          \"We propose a simple, yet effective, Word Sense Disambiguation method that uses a combination of a lexical knowledge-base and embeddings. Similar to the classic Lesk algorithm, it exploits the idea that overlap between the context of a word and the definition of its senses provides information on its meaning. Instead of counting the number of words that overlap, we use embeddings to compute the similarity between the gloss of a sense and the context. Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains. Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing. Although the best performing WSD systems are those based on supervised learning methods (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010) , a large amount of manually annotated data is required for training. Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000) . As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009) . They only require two types of information: a set of dictionary entries with definitions for each possible word meaning, and the context in which the word occurs. An example of such a system is the Lesk algorithm (Lesk, 1986 ) that exploits the idea that the overlap between the definition of a word and the definitions of the words in its context can provide information about its meaning. In this paper, we propose a knowledge-based WSD method that is loosely based on the Lesk algorithm exploiting both the context of the words and the definitions (hereafter referred to as glosses) of the senses. Instead of counting the number of words that overlap, we use word-and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. The strong point of our method is that it only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data. Also, it is readily applicable to other languages if a sense inventory is available. Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006) , and capturing lexical relationships (Mikolov et al., 2013) . A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012) , a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although previously mentioned sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable to NLP applications and research experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense-specific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Sch\\u00fctze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015) . Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016) . The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss-and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requires a sense-tagged corpus while our system does not rely on any tagged data. Method Our WSD algorithm takes sentences as input and outputs a preferred sense for each polysemous word. Given a sentence w 1 . . . w i of i words, we retrieve a set of word senses from the sense inventory for each word w. Then, for each sense s of each word w, we consider the similarity of its lexeme (the combination of a word and one of its senses (Rothe and Sch\\u00fctze, 2015) with the context and the similarity of the gloss with the context. For each potential sense s of word w, the cosine similarity is computed between its gloss vector G s and its context vector C w and between the context vector C w and the lexeme vector L s,w . The score of a given word w and sense s is thus defined as follows: Score(s, w) = cos(G s , C w ) + cos(L s,w , C w ) (1) The sense with the highest score is chosen. When no gloss is found for a given sense, only the second part of the equation is used. Prior to disambiguation itself, we sort the words by the number of senses is has, in order that the word with the fewest senses will be considered first. The idea behind this is that words that have fewer senses are easier to disambiguate (Chen et al., 2014) . As the algorithm relies on the words in the context which may themselves be ambiguous, if words in the context have been disambiguated already, this information can be used for the ambiguous words that follow. We, therefore, use the resulting sense of each word for the disambiguation of the following words starting with the \\\"easiest\\\" words. Our method requires lexeme embeddings L s,w for each sense s. For this, we use AutoExtend (Rothe and Sch\\u00fctze, 2015) to create additional embeddings for senses from WordNet on the basis of word embeddings. AutoExtend is an auto-encoder that relies on the relations present in WordNet to learn embeddings for senses and lexemes. To create these embeddings, a neural network containing lexemes and sense layers is built, while the WordNet relations are used to create links between each layer. The advantage of their method is that it is flexible: it can take any set of word embeddings and any lexical database as input and produces embeddings of senses and lexemes, without requiring any extra training data. Ultimately, for each word w we need a vector for the context C w , and for each sense s of word w we need a gloss vector G s . The context vector C w is defined as the mean of all the content word representations in the sentence: if a word in the context has already been disambiguated, we use the corresponding sense embedding; otherwise, we use the word embedding. For each sense s, we take its gloss as provided in WordNet. In line with Banerjee and Pedersen (2002) , we expand this gloss with the glosses of related meanings, excluding antonyms. Similar to the creation of the context vectors, the gloss vector G s is created by averaging the word embeddings of all the content words in the gloss. Experiments The performance of our algorithm was tested on both Dutch and English sentences in an all-words setup. Our sense inventory for Dutch is Cornetto (Vossen et al., 2012) while, for English, we use WordNet 1.7.1 (Fellbaum, 1998) . In Cornetto, 51.0% of the senses have glosses associated with them and in the Princeton WordNet, almost all of them do. The DutchSemCor corpus (Vossen et al., 2013) is used for Dutch evaluation and, for English, we use SemCor (Fellbaum, 1998) . For both languages, a random subset of 5000 manually annotated sentences from each corpus was created. Additionally, we test on the Senseval-2 (SE-2) and Senseval-3 (SE-3) all-words datasets (Snyder and Palmer, 2004; Palmer et al., 2001) 1 . We build 300-dimensional word embeddings on the Dutch Sonar corpus (Oostdijk et al., 2013 ) using word2vec CBOW (Mikolov et al., 2013) , and create sense-and lexeme embeddings with AutoExtend. For English, we use the embeddings from Rothe and Sch\\u00fctze (2015) 2 . They lie within the same vector space as the pre-trained word embeddings by Mikolov et al. (2013) 3 , trained on a part of the Google News dataset, which contains about 100 billion words. This model (similar to the Dutch model) contains 300-dimensional vectors for 3 million words and phrases. We evaluate our method by comparing it with a random baseline and Simplified Lesk with expanded glosses (SE-Lesk) (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2002) . Additionally, we compare our system to a state-of-the-art knowledge-based WSD system, UKB (Agirre and Soroa, 2009) , that, similar to our method, does not require any manually tagged data. UKB can be used for graph-based WSD using a pre-existing knowledge base. It applies random walks, e.g. Personalized PageRank, on the Knowledge Base graph to rank the vertices according to the context. We use UKBs Personalized PageRank method word-by-word with WordNet 1.7 and eXtended WordNet for English, as this setup yielded the best results in Agirre and Soroa (2009) . For Dutch, we use the Cornetto database as input graph. We do not compare our system to the initial results of AutoExtend (Rothe and Sch\\u00fctze, 2015) as they tested it in a supervised setup using sense embeddings as features. However, as is customary in WSD evaluation, we do compare our system to the most frequent WordNet sense baseline, which is notoriously difficult to beat due to the highly skewed distribution of word senses (Agirre and Edmonds, 2007) . As this baseline relies on manually annotated data, which our system aims to avoid, we consider this baseline to be semi-supervised and therefore an upper bound. For Dutch, the manually annotated part of DutchSemCor is balanced per sense which means that an equal number of examples for each sense is annotated. It is therefore not a reliable source for computing the most frequent sense. Alternatively, similar to Vossen et al. (2013) , we derive sense frequencies by using the automatically annotated counts in DutchSemCor 4 , assuming that the automatic annotation sufficiently reflects the true distribution for this purpose. The most frequent sense baseline for Dutch is, therefore, lower as compared to the English one, where the most frequent sense of a word is fully based on manual annotation. Results The results of the evaluation of our method (Lesk++) for both Dutch and English can be found in Table 1. Accuracy is calculated by dividing the number of words that were disambiguated correctly, as compared to the sense tagged corpus, by the total amount of polysemous words. Results are in bold when statistically significant over the baselines at p < 0.05. For both Dutch and English, our method performs significantly better than SE-Lesk and the random baseline for all tasks. Also, our system performs better than UKB on both SemCor and DutchSemCor. On DutchSemCor, it outperforms the most frequent baseline. Dutch Effects of sorting, lexemes, and glosses The main idea behind our method is a simple combination of two cosine similarity scores. In a second experiment, we evaluate the effects of both of these scores by using them separately. Additionally, we examine the use of sorting the words by its number of senses before disambiguation. We compare our final results with a system where similarity is only computed between the context and gloss vector and with a system that only computes the cosine distance between the context and the lexeme (only the first and the second part of Equation 1 respectively). Both systems are tested without and with (+S) sorting. The results of this third experiment on the sense tagged corpora for Dutch (DSC) and English (SC) can be found in Table 2 The second and the fourth column show results of a system that only uses the lexeme (Lex) or gloss vectors (Gloss) respectively. In the third and last column sorting (+S) is added. For Dutch, the results indicate that sorting the words by its number of senses by itself is not very effective compared to the system that does not use this module. The use of glosses, on the other hand, seems to be very effective while the combination of both measures yields the best results. The effect of the gloss vectors is even stronger for English, which can be explained by the fact that the English WordNet has a higher gloss coverage. Also, for English, although both sorting and glosses are effective, the combination performs better. Comparison of Different Domains To examine the robustness of our system in different domains, we evaluate it on different parts of Dutch-SemCor. We randomly took 5000 manually annotated sentences from each of the four largest subsets of the corpus. The results of this experiment for the all-words task can be found in Table 3 tion of the DSC dataset, our method outperforms SE-Lesk, the random baseline and UKB. Furthermore, our method outperforms the most frequent baseline on three of them. The newspapers subsection forms an exception, probably because it belongs to a more general domain (Agirre et al., 2014) . Discussion The difference in results for Dutch and English can possibly be explained by the coverage of the datasets. The Cornetto coverage is about 60%, compared to Princeton Wordnet, with an average polysemy of 1.07 for nouns, 1.56 for verbs and 1.05 for adjectives while, for English it is 1.24 for nouns, 2.17 for verbs and 1.40 for adjectives. Also, not all Dutch senses have corresponding glosses while most of the English ones do. As our method relies greatly on gloss vectors, this could affect its performance. Our WSD approach combines a lexical knowledge base with word-and sense embeddings. The results of our experiments show that the use of embeddings can help improve other Lesk methods (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2002) . An obvious next step would be to see whether other extensions that do not require manually tagged data are compatible as well. For example, Vasilescu et al. (2004) shows improvements by pre-selecting context words using the WordNet hierarchy. Also, the method of Miller et al. (2012) could be used to first expand the glosses and/or the context before using our adaptation of the Lesk system. Conclusion In this paper we propose an extension to the Lesk algorithm which uses sense, gloss and context embeddings to compute the similarity of word senses to the context in which the words occur. Although our approach is a straightforward extension to the Lesk algorithm, it achieves better performance compared to Lesk and a random baseline and outperforms, or yields similar performance to, a state-of-the-art knowledge-based system. For Dutch, it outperforms all other systems including the most frequent sense on three out of four subsets. A second experiment confirms the effects of gloss vectors while the results of a final experiment indicate that our method works well in different domains. The main advantage of our method is its simplicity which makes it fast and easy to apply to other languages. It furthermore only requires unlabeled text and the definitions of senses, and does not rely on any manually annotated data, which makes our system an attractive alternative for supervised WSD.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"French\",\n          \"Portuguese (Brazil)\",\n          \"Chinese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publisher\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 111,\n        \"samples\": [\n          \"{\\\\'U}FAL MFF UK\",\n          \"International Committee on Computational Linguistics\",\n          \"The COLING 2016 Organizing Committee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 73280,\n        \"samples\": [\n          \"https://aclanthology.org/P17-2088\",\n          \"https://aclanthology.org/2020.winlp-1.26\",\n          \"https://aclanthology.org/D09-1013\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "data_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZVjCpUmfObd",
        "outputId": "3b05ece0-b2d0-4b0a-8582-c4b0198ecae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.12/dist-packages (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pylatexenc\n",
        "\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "data_clean[\"clean_title\"] = data_clean[\"title\"].apply(lambda x: LatexNodes2Text().latex_to_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8nhEhhHCf6z"
      },
      "source": [
        "Pour séléctionner uniquement les articles en français, nous avons utilisé detect du module langdetect qui permet d'identifier une langue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7ENx8E3fOTm",
        "outputId": "67612d70-6b9d-4a82-c52b-5f976bbaee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "OEX3dAeSfORd"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect,LangDetectException"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "v0u6hixZfdnf"
      },
      "outputs": [],
      "source": [
        "def det_lang(text):\n",
        "  try:\n",
        "    lang=detect(text)\n",
        "    if lang==\"fr\":\n",
        "      return text\n",
        "  except LangDetectException as e:\n",
        "    lang=\"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "kp7ZICMNfdj8"
      },
      "outputs": [],
      "source": [
        "data_clean[\"title_fr\"]=data_clean[\"clean_title\"].apply(det_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LtuP_A9ufdhA",
        "outputId": "49375dff-9902-4a09-a3cc-fbcbef2f8975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         acl_id                                              title  \\\n",
              "0      O02-2002  A Study on Word Similarity using Context Vecto...   \n",
              "1      L02-1310           Bootstrapping Large Sense Tagged Corpora   \n",
              "2      R13-1042  Headerless, Quoteless, but not Hopeless? Using...   \n",
              "3      W05-0819  Aligning Words in {E}nglish-{H}indi Parallel C...   \n",
              "4      L02-1309  Proposal of a very-large-corpus acquisition me...   \n",
              "...         ...                                                ...   \n",
              "73280  P99-1002  Automatic Speech Recognition and Its Applicati...   \n",
              "73281  P00-1009  An Improved Parser for Data-Oriented Lexical-F...   \n",
              "73282  P99-1056  The grapho-phonological system of written {F}r...   \n",
              "73283  P99-1051  Acquiring Lexical Generalizations from Corpora...   \n",
              "73284  P00-1013  Spoken Dialogue Management Using Probabilistic...   \n",
              "\n",
              "                                                abstract  \\\n",
              "0      There is a need to measure word similarity whe...   \n",
              "1                                                   None   \n",
              "2      Thread disentanglement is the task of separati...   \n",
              "3      In this paper, we describe a word alignment al...   \n",
              "4                                                   None   \n",
              "...                                                  ...   \n",
              "73280  This paper describes recent progress and the a...   \n",
              "73281  We present an LFG-DOP parser which uses fragme...   \n",
              "73282  The processes through which readers evoke ment...   \n",
              "73283  This paper examines the extent to which verb d...   \n",
              "73284  Spoken dialogue managers have benefited from u...   \n",
              "\n",
              "                                               full_text language  \\\n",
              "0      There is a need to measure word similarity whe...     None   \n",
              "1                                                   None     None   \n",
              "2      Thread disentanglement is the task of separati...     None   \n",
              "3      In this paper, we describe a word alignment al...     None   \n",
              "4                                                   None     None   \n",
              "...                                                  ...      ...   \n",
              "73280  This paper describes recent progress and the a...     None   \n",
              "73281  We present an LFG-DOP parser which uses fragme...     None   \n",
              "73282  The processes through which readers evoke ment...     None   \n",
              "73283  This paper examines the extent to which verb d...     None   \n",
              "73284  Spoken dialogue managers have benefited from u...     None   \n",
              "\n",
              "                                            publisher  \\\n",
              "0                                                None   \n",
              "1      European Language Resources Association (ELRA)   \n",
              "2                       INCOMA Ltd. Shoumen, BULGARIA   \n",
              "3           Association for Computational Linguistics   \n",
              "4      European Language Resources Association (ELRA)   \n",
              "...                                               ...   \n",
              "73280       Association for Computational Linguistics   \n",
              "73281       Association for Computational Linguistics   \n",
              "73282       Association for Computational Linguistics   \n",
              "73283       Association for Computational Linguistics   \n",
              "73284       Association for Computational Linguistics   \n",
              "\n",
              "                                                     url  \\\n",
              "0                      https://aclanthology.org/O02-2002   \n",
              "1      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
              "2                      https://aclanthology.org/R13-1042   \n",
              "3                      https://aclanthology.org/W05-0819   \n",
              "4      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
              "...                                                  ...   \n",
              "73280                  https://aclanthology.org/P99-1002   \n",
              "73281                  https://aclanthology.org/P00-1009   \n",
              "73282                  https://aclanthology.org/P99-1056   \n",
              "73283                  https://aclanthology.org/P99-1051   \n",
              "73284                  https://aclanthology.org/P00-1013   \n",
              "\n",
              "                                             clean_title title_fr  \n",
              "0      A Study on Word Similarity using Context Vecto...     None  \n",
              "1               Bootstrapping Large Sense Tagged Corpora     None  \n",
              "2      Headerless, Quoteless, but not Hopeless? Using...     None  \n",
              "3       Aligning Words in English-Hindi Parallel Corpora     None  \n",
              "4      Proposal of a very-large-corpus acquisition me...     None  \n",
              "...                                                  ...      ...  \n",
              "73280  Automatic Speech Recognition and Its Applicati...     None  \n",
              "73281  An Improved Parser for Data-Oriented Lexical-F...     None  \n",
              "73282  The grapho-phonological system of written Fren...     None  \n",
              "73283  Acquiring Lexical Generalizations from Corpora...     None  \n",
              "73284  Spoken Dialogue Management Using Probabilistic...     None  \n",
              "\n",
              "[73285 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62d24689-c371-49d3-8411-c9efbed7b111\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acl_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "      <th>language</th>\n",
              "      <th>publisher</th>\n",
              "      <th>url</th>\n",
              "      <th>clean_title</th>\n",
              "      <th>title_fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O02-2002</td>\n",
              "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>There is a need to measure word similarity whe...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://aclanthology.org/O02-2002</td>\n",
              "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L02-1310</td>\n",
              "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R13-1042</td>\n",
              "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>Thread disentanglement is the task of separati...</td>\n",
              "      <td>None</td>\n",
              "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
              "      <td>https://aclanthology.org/R13-1042</td>\n",
              "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>W05-0819</td>\n",
              "      <td>Aligning Words in {E}nglish-{H}indi Parallel C...</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>In this paper, we describe a word alignment al...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/W05-0819</td>\n",
              "      <td>Aligning Words in English-Hindi Parallel Corpora</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L02-1309</td>\n",
              "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>European Language Resources Association (ELRA)</td>\n",
              "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
              "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73280</th>\n",
              "      <td>P99-1002</td>\n",
              "      <td>Automatic Speech Recognition and Its Applicati...</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>This paper describes recent progress and the a...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1002</td>\n",
              "      <td>Automatic Speech Recognition and Its Applicati...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73281</th>\n",
              "      <td>P00-1009</td>\n",
              "      <td>An Improved Parser for Data-Oriented Lexical-F...</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P00-1009</td>\n",
              "      <td>An Improved Parser for Data-Oriented Lexical-F...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73282</th>\n",
              "      <td>P99-1056</td>\n",
              "      <td>The grapho-phonological system of written {F}r...</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>The processes through which readers evoke ment...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1056</td>\n",
              "      <td>The grapho-phonological system of written Fren...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73283</th>\n",
              "      <td>P99-1051</td>\n",
              "      <td>Acquiring Lexical Generalizations from Corpora...</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>This paper examines the extent to which verb d...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P99-1051</td>\n",
              "      <td>Acquiring Lexical Generalizations from Corpora...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73284</th>\n",
              "      <td>P00-1013</td>\n",
              "      <td>Spoken Dialogue Management Using Probabilistic...</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>Spoken dialogue managers have benefited from u...</td>\n",
              "      <td>None</td>\n",
              "      <td>Association for Computational Linguistics</td>\n",
              "      <td>https://aclanthology.org/P00-1013</td>\n",
              "      <td>Spoken Dialogue Management Using Probabilistic...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73285 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62d24689-c371-49d3-8411-c9efbed7b111')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62d24689-c371-49d3-8411-c9efbed7b111 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62d24689-c371-49d3-8411-c9efbed7b111');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-38198cc9-e501-4a95-abab-01fa90092b78\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38198cc9-e501-4a95-abab-01fa90092b78')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-38198cc9-e501-4a95-abab-01fa90092b78 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9262ca9a-9092-40a5-8f85-5924a9e23f90\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_clean')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9262ca9a-9092-40a5-8f85-5924a9e23f90 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_clean');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_clean",
              "summary": "{\n  \"name\": \"data_clean\",\n  \"rows\": 73285,\n  \"fields\": [\n    {\n      \"column\": \"acl_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 73285,\n        \"samples\": [\n          \"L08-1302\",\n          \"L10-1106\",\n          \"W05-1303\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 72510,\n        \"samples\": [\n          \"Lexicalized {TAG}s, Parsing and Lexicons\",\n          \"{C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies\",\n          \"{NETL}: A System for Representing and Using Real-World Knowledge\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 67465,\n        \"samples\": [\n          \"Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD 1 , a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into stateof-the-art solutions.\",\n          \"We consider the gap between user demands for seamless handling of complex interactions, and recent advances in dialog state tracking technologies. We propose a new statistical approach, Task Lineage-based Dialog State Tracking (TL-DST), aimed at seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. TL-DST consists of three components: (1) task frame parsing, (2) context fetching and (3) task state update (for which TL-DST takes advantage of previous work in dialog state tracking). There is at present very little publicly available multi-task, complex goal dialog data; however, as a proof of concept, we applied TL-DST to the Dialog State Tracking Challenge (DSTC) 2 data, resulting in state-of-the-art performance. TL-DST also outperforms the DSTC baseline tracker on a set of pseudo-real datasets involving multiple tasks with complex goals which were synthesized using DSTC3 data.\",\n          \"Les mod\\u00e8les vectoriels de s\\u00e9mantique distributionnelle (ou word embeddings), notamment ceux produits par les m\\u00e9thodes neuronales, posent des questions de reproductibilit\\u00e9 et donnent des repr\\u00e9sentations diff\\u00e9rentes \\u00e0 chaque utilisation, m\\u00eame sans modifier leurs param\\u00e8tres. Nous pr\\u00e9sentons ici un ensemble d'exp\\u00e9rimentations permettant de mesurer cette instabilit\\u00e9, \\u00e0 la fois globalement et localement. Globalement, nous avons mesur\\u00e9 le taux de variation du voisinage des mots sur trois corpus diff\\u00e9rents, qui est estim\\u00e9 autour de 17% pour les 25 plus proches voisins d'un mot. Localement, nous avons identifi\\u00e9 et caract\\u00e9ris\\u00e9 certaines zones de l'espace s\\u00e9mantique qui montrent une relative stabilit\\u00e9, ainsi que des cas de grande instabilit\\u00e9.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 67378,\n        \"samples\": [\n          \"Statistical Relational Learning (SRL) is an interdisciplinary research area that combines firstorder logic and machine learning methods for probabilistic inference. Although many Natural Language Processing (NLP) tasks (including text classification, semantic parsing, information extraction, coreference resolution, and sentiment analysis) can be formulated as inference in a firstorder logic, most probabilistic firstorder logics are not efficient enough to be used for largescale versions of these tasks. In this tutorial, we provide a gentle introduction to the theoretical foundation of probabilistic logics, as well as their applications in NLP. We describe recent advances in designing scalable probabilistic logics, with a special focus on ProPPR. Finally, we provide a handson demo about scalable probabilistic logic programming for solving practical NLP problems. Outline: \\u2022 Part 1: Foundations and Applications of Probabilistic FirstOrder Logic \\u2022 We will provide a brief review of some firstorder learning systems that have been developed in the past: Markov Logic Networks (Richardson and Domingos, 2006) , Stochastic Logic Programs (Muggleton, 1996) . In this part, we introduce the semantics of the above languages with their inference (and learning) approaches. We analyze and discuss the core ideas behind of such language. We show various applications of probabilistic logics in NLP. \\u2022 Part 2: Scalable Probabilistic Logics: A Case Study of ProPPR. \\u2022 We will focus on the efficiency issue, and introduce recent advances of scalable probabilistic logics, including lifted inference techniques (Van den Broeck and Suciu, 2014) and probabilistic soft logic (Bach et al., 2015) . In particular, we will take CMU's ProPPR (Wang et al., 2013) as a case study. We describe the main contributions of ProPPR: including its approximate personalized PageRank inference scheme, parallel stochastic gradient descent learning method, and its flexibility in theory engineering. We then introduce the structure learning methods in ProPPR (Wang et al., CIKM 2014) , including a structured regularization method as an alternative to predicate invention (Wang et al., IJCAI 2015) . We will also cover our latest attempt of learning firstorder logic formula embeddings, and discuss its relationship to (and possible connections between) even newer approaches to modeling knowledge bases, relationships, and inference using deep learning methods. To conclude this part, we show an interesting application of ProPPR (Wang et al., ACLIJCNLP 2015) : a joint information extraction and knowledge reasoning engine. \\u2022 Part 3: Demos and Practical Applications. \\u2022 We switch from the theoretical presentations to an interactive demonstration session: we aim at providing a handson lab session to transfer the theories of scalable probabilistic logics into practices. More specifically, we will provide a demo of several applications on synthetic and realworld datasets. Participants are encouraged to check out our repository on Github ( https://github.com/TeamCohen/ProPPR ) and bring laptops to the tutorial. The list of demo examples to be considered are text categorization, entity resolution, knowledge base completion (Wang et al., MLJ 2015) , dependency parsing (Wang et al., EMNLP 2014) extraction, text categorization and learning from large datasets. He has a longstanding interest in statistical relational learning and learning models, or learning from data, that display nontrivial structure. He holds seven patents related to learning, discovery, information retrieval, and data integration, and is the author of more than 200 publications. He was a past president of International Machine Learning Society. He is a AAAI fellow, and was a winner of SIGMOD Test of Time Award and SIGIR Test of Time Award.\",\n          \"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F 1 , macro-F 1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model. Introduction Relation classification (RC) models are typically compared with either micro-F 1 or macro-F 1 , often without discussing the measure's properties (see e.g. Zhang et al., 2017; Yao et al., 2019) . Each measure highlights different aspects of model performance (Sun et al., 2009) . However, using an inappropriate measure can lead to the preference of an unsuitable model (Branco et al., 2016) , e.g., tasks with an imbalanced or long-tailed class distribution. We argue that model evaluation should better reflect this, particularly as rare phenomena become more important in NLP (Rogers, 2021) . For instance, popular datasets for RC, such as TACRED (Zhang et al., 2017) , NYT (Riedel et al., 2010) , ChemProt (Kringelum et al., 2016) , Do-cRED (Yao et al., 2019) , and SemEval-2010 Task 8 (Hendrickx et al., 2010) , often exhibit a highly imbalanced label distribution (see Table 1 and, e.g., the TACRED class distribution 1 ). The main reasons are the natural data imbalance, i.e. the occurrence frequency of relation mentions in text, as well as the incompleteness of knowledge graphs like Freebase (Bollacker et al., 2008) used in distantly supervised RC. For example, 58% of the relations in the NYT dataset (Riedel et al., 2010) have fewer than 100 training instances (Han et al., 2018) , and the most frequent relation location/contains is assigned to 48.3% of the positive test instances. However, for applying RC to real-world problems, it is especially important to discover instances of relations that are not yet covered well in a given knowledge base. Table 1 lists statistics of the aforementioned RC datasets, including their perplexity and common evaluation measures. TACRED and the original version of NYT contain predominantly negative samples 2 . All datasets, except for undirectional SemEval, exhibit a large ratio between most frequent and least frequent positive class in the test set. The perplexity of test set distributions is also much lower than the relation count for all datasets except SemEval. Reporting only a single measure therefore cannot exhaustively capture model performance on these datasets, especially for the long tail of relation types. For example, Alt et al. (2019) show that on the NYT dataset, AUC scores and P-R-Curves of several state-of-the-art models are heavily skewed towards the two most frequent relation types location/contains and person/nationality. TACRED, ChemProt, DocRED and SemEval results are usually only reported in micro-F 1 , which does not consider class membership. In this paper, we introduce a framework for weighting schemes of measures to address these evaluation deficits. We present and motivate two new weighting schemes that are in between the extremes of micro-and macro-weighting. We demonstrate these, micro-, class-weighted-and macro-F 1 on TACRED and SemEval with two popular models each. We show that more information about models can be inferred from our results and point out what further steps should be taken to improve evaluation in relation classification. Methods We first give background on the F 1 -score and existing F 1 weighting schemes. We present our framework of weighting schemes. We introduce two new weighting schemes. Finally, we outline statistical tests. Background The F \\u03b2 -score (Rijsbergen, 1979; Lewis and Gale, 1994) calculates a score in the interval [0, 1] through the formula F \\u03b2 = (1 + \\u03b2 2 ) \\u2022 T P (1 + \\u03b2 2 ) \\u2022 T P + \\u03b2 2 \\u2022 F N + F P (1) with the true positives (TP), false negatives (FN) and false positives (FP) of a confusion matrix. This definition is identical to the weighted harmonic mean of precision and recall. The positive coefficient \\u03b2 is used as a trade-off between the error types FN and FP. If there is no preference known or pre-determined, this coefficient is usually set to 1. In multi-class classification the confusion matrix can either be calculated once for the whole dataset, or separately for each class. The former method yields micro-F 1 . Micro weighting does not consider class membership for any test sample. If the predictions and labels of all classes are considered, micro-F 1 is equal to accuracy, as the denominator in Eq. 1 is twice the dataset. In RC, the TP of the negative class are usually not considered, in which case micro-F 1 is not equal to accuracy. For the F -score, micro is the only weighting where the impact of a sample on the score is not conditioned on the model performance on the rest of the class (Forman and Scholz, 2010). If the test set is considered to have a representative data distribution, the microweighted score is a frequentist evaluation of model performance. There exist two other ways to calculate and combine F 1 -scores for a multi-class problem. First, multi-class F 1 -scores can be calculated for each class and then a weighted average class score is taken. Second, precision and recall scores for each class can be calculated and weighted, then the harmonic mean of weighted precision and weighted recall is taken. Opitz and Burst (2019) show that the first method is more robust and less favorable to biased classifiers. We use this method in our proposed framework. (Class-)weighted-F 1 is similar to micro-F 1 . F 1scores are calculated for each class individually and then weighted by the class count. Thus, both schemes approximately weigh all samples equally. Macro weighting gives an equal weight for each class with positive sample count regardless of the specific sample count. This gives information about model performance if class imbalance is not considered. In general, there is a correspondence between training loss and evaluation measure (Li et al., 2020) . One disadvantage of multiple weighting schemes is that each weighting scheme can be optimized for. To achieve a better score for a specific weighting, class weights could be set proportional to the weighting of the class during training. How-  ever, we argue that model results should always be presented with multiple weightings for one dataset. Especially, when comparing different models all weightings should be reported for each model. This can clarify whether a model is good for all weightings or just micro or macro. Furthermore, with datasets that are currently evaluated with different weightings, it is easier to identify whether a model is specifically good for a dataset or for a weighting. Framework for Weighting Schemes We discuss a framework that summarizes the rules we give to class-weighting schemes. Then we introduce two new class weighting schemes. All discussed weighting schemes can be found in Table 2. They are independent of the measure that is used to calculate a score for each class. (Class-)weighted and macro weighting are the extremes of \\\"degressive proportionality\\\" 3 or \\\"allocation functions\\\" (S\\u0142omczy\\u0144ski and \\u017byczkowski, 2012) . These are, e.g., used by the European Parliament to allocate seats to member nations depending on the population of the nation. They state that allocation should be monotonic increasing (see D1) and proportionally decreasing (see D2). To adopt this to a weighting scheme for multi-class evaluation, we add a normalizing desideratum that determines the sum of weights over all classes to be 1 (see D0). Let n i > 0 be the count of samples of class i and w i \\u2265 0 the weight assigned to the score of class i. 3 https://eur-lex.europa.eu/ legal-content/EN/TXT/HTML/?uri=CELEX: 32013D0312&from=EN#d1e114-57-1 We have the following desiderata: i w i = 1 (D0) n i \\u2265 n j \\u21d2 w i \\u2265 w j (D1) n i \\u2265 n j \\u21d2 w i n i \\u2264 w j n j (D2) Note that these desiderata do not restrict the scoring function that assigns scores s i to class i. The weighted evaluation score is then given by i w i s i . Weighting Schemes Macro: Macro weighting is one extreme by setting equality on the weights of desideratum D1. It implies that we do not consider the instance counts per class, but treat all classes equally. (Class-)weighted: Class-weighted is the other extreme by setting equality on the fraction of weights and counts in desideratum D2. It implies that we do not consider class constituency but weight all samples equally. Dodrans: Cao et al. (2019) demonstrate that their balanced generalization error bound for binary classifiers in the separable case can be optimized by setting margins proportional to n i \\u22121/4 . They use this derivation from a limited theoretical scenario to improve the performance of several classifiers on imbalanced multi-class datasets. A term proportional to n i \\u22121/4 is added in the loss function. While this added term is not directly transferable, we propose adapting this as a multiplicative factor in weighting classes for multi-class evaluation: w i \\u221d n i \\u22121/4 n i = n 3/4 i . We coin this weighting dodrans (\\\"three-quarter\\\"). Entropy: We also want to provide a weighting scheme that takes into consideration how hard a class is to predict. To this end, we propose weighting classes proportional to their term in the Shannon entropy formula H(X) = \\u2212 i P (x i ) log(P (x i )) (2) w i \\u221d P (x i ) log(P (x i )). (3) We interpret P (x i ) for class i to be the probability of it appearing in the dataset, s.t. P (x i ) = n i / j n j . Thus, without normalization the model score is now the sum over all classes of the model performance on a class times the difficulty and frequency of the class. Note, that this weighting scheme does not fulfil desideratum D1, since it is decreasing for classes i with P (x i ) > e \\u22121 . This is related to the fact that classes that are too large become easier to predict for a model, the model can just default to predicting this class. It can also be desirable that a class does not gain relative importance once it contains more than half of the dataset. For RC, this often has little consequence. If we include NA in the normalization, it is usually the largest class and other classes are below an e-th of the dataset. Table 2 shows an overview of the mentioned schemes. Figure 1 displays the weights that these schemes assign to the classes of the TACRED test set. The weighted scheme is proportional to class counts and produces the most imbalanced weights. Dodrans and entropy produce slightly more balanced weights and differ from weighted for the most frequent classes. Macro considers all classes equally, regardless of class count. Statistical Testing Currently, most RC works report a single score for each dataset. This can be the result from a single run or the median score from multiple runs. However, this does not allow to measure how large the difference between models is. Recently, analysis papers in NLP have recorded mean and standard deviation over multiple runs (Madhyastha and Jain, 2019; Zhou et al., 2020) , as this allows for statistical tests. We first test for significance and report p-values. We employ Welch's t-test to test the hypothesis that the models have equal mean. Following Zhu et al. (2020) , we also report Cohen's d effect size to determine how large the difference between models is for a specific measure. For two models with the Figure 1 : TACRED relations and their respective weights under different weighting schemes. The lower x-axis denotes the normalized weight given to a relation for a scheme. The upper x-axis corresponds to the counts of the relations in the test set for the classweighted scheme. The y-axis denotes all positive relations. The negative NA class is not listed and has 12184 samples. The entropy and dodrans weighting scheme produce similar weights and are between weighted and macro weighting. same number n > 1 of runs, Cohen's d is given by d = \\u221a 2 \\u00b5 1 \\u2212 \\u00b5 2 \\u03c3 2 1 + \\u03c3 2 2 (4) with \\u00b5 i and \\u03c3 2 i being mean and variance of model i's scores. We do this, as two different models never perform exactly the same, i.e. significance just depends on the number of runs and we also want to score the difference between the models. Experiments We evaluate and compare three RC methods with our proposed measures on two datasets. We choose these methods, as RECENT (Lyu and Chen, 2021) and BERT EM (Baldini Soares et al., 2019) are based on vanilla fine-tuning of a pre-trained language model, with a classification head on top. PTR (Han et al., 2021) this way we can compare performance of the two paradigms for other weightings. RECENT proposes a model-agnostic paradigm that exploits entity types to narrow down the candidate relations. Given an entity-type combination, a separate classifier is trained on the restricted classes. Baldini Soares et al. ( 2019 ) compare various strategies that extract relation representation from Transformers and claim ENTITY START (i.e. insert entity markers at the start of two entity mentions) yields the best performance. PTR also takes entity types into consideration and constructs prompts composed of three subprompts, two corresponding to the fill-in of the entity types and one predicting the relation. In our experiments we use RECENT GCN for RE-CENT, BERT EM with ENTITY START, and unreversed prompts for PTR. We use the official repositories for RECENT and PTR, we reimplement BERT EM 4 . We use the hyperparameters proposed in the original papers and conduct five runs for each model. Additional implementation and training details can be found in Appendices A and B. The main focus is unearthing performance information about these methods that was previously obscured by single score measures. The number of weighting schemes does not influence the computational cost, as each score is determined through the predictions in a run and does not require specific tuning. 5 We acknowledge that each weighting scheme could be optimized for during training which gives additional importance to reporting multiple measures for each model. Results Table 3 shows results for TACRED. PTR significantly outperforms RECENT across all weighting schemes. The difference between the models is smallest for micro-F 1 and increases for all schemes that weigh classes more equally. For macro-F 1 the difference is starkest with effect size 24.2. Table 4 displays results for SemEval. BERT EM significantly outperforms PTR in the micro-F 1 measure and all other weightings except for macro-F 1 . All effect sizes are either large or huge, by far the largest effect size is between PTR and BERT EM regarding macro-F 1 though. The Sem-Eval test set contains a single sample of the Entity-Destination(e2,e1) class which is quite impactful for the macro-F 1 of the models but has negligible impact on all other weighting schemes. The scores from dodrans and entropy indicate that only if all classes are considered equally important the PTR model should be preferred. This indicates that either the PTR model learns almost regardless of class frequency or BERT EM has a class preference that is only discoverable with macro-F 1 . We demonstrate that evaluation on micro-F 1 does not give adequate information about model performance on long-tail classes. In Tables 3 and  4 we see that the model which performs better under micro-F 1 can either be significantly better or worse for classes with few samples. The weighted-F 1 produces similar results to micro-F 1 except for RECENT. Macro-F 1 on the other hand is very sensitive to model performance on single samples, e.g. the Entity-Destination(e2,e1) class in SemEval. The scores of our proposed schemes are in between the existing measures and might be the best indicators for robust generalization performance. For all experiments, they produce similar results to each other. This could just be a coincidence of the datasets, and is also indicated by Figure 1 . Overall, it might be fair to say that one of the former and latter measures is enough. It would mean one measure that does weigh proportional to sample count (micro-or weighted-F 1 ), an intermediary measure (dodrans-F 1 or entropy-F 1 ) and macro-F 1 . PTR performs better for macro-F 1 on both datasets. Its scores decrease less when classes are weighted more equally. This suggests that it is a better model for classes with low sample counts. Le Scao and Rush (2021) show that prompts can be worth hundreds of data points which would explain why the macro-and micro-F 1 scores are much closer together than for RECENT and BERT EM . Related Work Chauhan et al. ( 2019 ) do a thorough evaluation of their model and notice the significantly different performance measured by micro and macro statistics due to the class imbalance, suggesting that the choice of evaluation measure is crucial. Huang and Wong (2020) further use the closeness between micro-and macro-F 1 scores to claim the stable performance of their model. Mille et al. (2021) point out that evaluating with a single score favors overfitting. They show different evaluation suites that can be created for a dataset. Bragg et al. (2021) address the disjoint evaluation settings across recent research threads in (few-shot) NLP and propose a unified evaluation benchmark which regulates dataset, sample size etc., but fail to take the evaluation measure into consideration, reporting only mean accuracy instead. Post (2018) criticises the inconsistency and under-specification in reporting scores. This problem is also prevalent in RC where the F 1 weighting scheme is often not specified. Zhang et al. (2020) show that bias from corpora persists for fine-tuned pre-trained language models. These models struggle with rare phenomena. For better performance debiasing with weighting is performed. S\\u00f8gaard et al. (2021) argue against using random splits. They show that evaluating models with random splits is not a realistic setting but makes tasks easier by fixing the test data distribution to the train data distribution. Long-tail evaluation is becoming more prominent in NLP research. Models in deep learning tend to show a gap in performance between frequent and infrequent phenomena (Rogers, 2021) . Models in NLP have been shown to perform badly on specific subsets of data (Zhang et al., 2020) . Sokolova and Lapalme (2009) analyze measures for multi-class classification and present invariances regarding the confusion matrix. G\\u00f6sgens et al. (2021) also determine which class measures (including F 1 ) fulfil specific assumptions. Further evaluation can be based on this. Our weighting schemes for F 1 can be transferred to other measures that calculate a score for each class. Outlook We suggest creating and using a bidimensional leaderboard like Kasai et al. (2021) where measures and models can be contributed. To this end, benchmarking of RC models could be done on a centralized site where a model or test set predictions are submitted and measures are calculated automatically through a script. For measures that modify weighting of classes and intra-class scoring, this does not require additional training computation. Due to the reproducibility crisis (Baker, 2016) , not all state-of-the-art scores can be replicated. Possible future work includes a comprehensive evaluation study of papers on leaderboards of RC tasks. This would enable an in-depth discussion of strength and weaknesses (including reproducibil-ity) of these models. The analysis we present can also be extended to other NLP tasks with imbalanced datasets, such as named entity recognition (Tjong Kim Sang and De Meulder, 2003) , part-of-speech tagging (Pradhan et al., 2013) and coreference resolution (Pradhan et al., 2012) . Conclusion We criticise the current practice of reporting a single score when evaluating imbalanced RC datasets. We propose a new framework to weight scores for multi-class evaluation of imbalanced datasets. We provide two new weighting schemes, dodrans and entropy, which are positioned between classweighted and macro. In our experiments, we show that model performance on both TACRED and SemEval, especially on the long-tail relations, is not adequately captured by a single score. Thus, we advocate the use of multiple weighing schemes when reporting model performance on imbalanced datasets. Acknowledgments We would like to thank Nils Feldhus, Sebastian M\\u00f6ller, Lisa Raithel, Robert Schwarzenberg and the anonymous reviewers for their feedback on the paper. This work was partially supported by the German Federal Ministry of Education and Research as part of the project CORA4NLP (01IW20010) and by the German Federal Ministry for Economic Affairs and Climate Action as part of the project PLASS (01MD19003E). Christoph Alt is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2002/1 \\\"Science of Intelligence\\\" -project number 390523135. A Implementation Details To evaluate RECENT and PTR, we use the official code at https://github.com//Saintfe/ RECENT (last updated on 01.10.2021) and https: //github.com/thunlp/PTR (last updated on 20.11.2021). Since the official code of BERT EM is not available, we implement this method using the HuggingFace Transformers library (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) , and make our code base available at https://github. com/dfki-nlp/mtb-bert-em. To make our results reproducible, we randomly generated seeds {9, 148, 378, 459, 687} and employed these for all models in their 5 runs. B Training Details B.1 RECENT We consider GCN as the base model. Following the paper and the official code, we set the batch size to be 50, the optimizer to be SGD with learning rate 0.3, and the number of epochs to be 100. It takes a single RTX-A6000 GPU approximately 10 hours to complete all 5 runs on TACRED. B.2 BERT EM We use the pre-trained language model (PLM) bert-large-uncased from the HuggingFace model hub and directly fine-tune the model for the RC task, without matching-the-blank pre-training. As the paper suggests, we set the batch size to be 64, the optimizer to be Adam with learning rate 3 \\u2022 10 \\u22125 , and the number of epochs to be 5. Additionally, we use the max sequence length of 512. It takes a single RTX-A6000 GPU 30 minutes to complete all 5 runs on SemEval. B.3 PTR According to the paper and the official code base, we apply the same settings to evaluate both TACRED and SemEval: We use the PLM roberta-large and set the max sequence length to be 512, the batch size to be 64, the optimizer to be Adam with learning rate 3 \\u2022 10 \\u22125 , the weight decay to be 10 \\u22122 , and the number of epochs to be 5. It takes 4 Quadro-P5000 GPUs 84 hours to complete 5 runs on TACRED, and it takes 8 Titan-V GPUs 9 hours on SemEval.\",\n          \"We propose a simple, yet effective, Word Sense Disambiguation method that uses a combination of a lexical knowledge-base and embeddings. Similar to the classic Lesk algorithm, it exploits the idea that overlap between the context of a word and the definition of its senses provides information on its meaning. Instead of counting the number of words that overlap, we use embeddings to compute the similarity between the gloss of a sense and the context. Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains. Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing. Although the best performing WSD systems are those based on supervised learning methods (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010) , a large amount of manually annotated data is required for training. Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000) . As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009) . They only require two types of information: a set of dictionary entries with definitions for each possible word meaning, and the context in which the word occurs. An example of such a system is the Lesk algorithm (Lesk, 1986 ) that exploits the idea that the overlap between the definition of a word and the definitions of the words in its context can provide information about its meaning. In this paper, we propose a knowledge-based WSD method that is loosely based on the Lesk algorithm exploiting both the context of the words and the definitions (hereafter referred to as glosses) of the senses. Instead of counting the number of words that overlap, we use word-and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. The strong point of our method is that it only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data. Also, it is readily applicable to other languages if a sense inventory is available. Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006) , and capturing lexical relationships (Mikolov et al., 2013) . A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012) , a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although previously mentioned sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable to NLP applications and research experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense-specific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Sch\\u00fctze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015) . Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016) . The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss-and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requires a sense-tagged corpus while our system does not rely on any tagged data. Method Our WSD algorithm takes sentences as input and outputs a preferred sense for each polysemous word. Given a sentence w 1 . . . w i of i words, we retrieve a set of word senses from the sense inventory for each word w. Then, for each sense s of each word w, we consider the similarity of its lexeme (the combination of a word and one of its senses (Rothe and Sch\\u00fctze, 2015) with the context and the similarity of the gloss with the context. For each potential sense s of word w, the cosine similarity is computed between its gloss vector G s and its context vector C w and between the context vector C w and the lexeme vector L s,w . The score of a given word w and sense s is thus defined as follows: Score(s, w) = cos(G s , C w ) + cos(L s,w , C w ) (1) The sense with the highest score is chosen. When no gloss is found for a given sense, only the second part of the equation is used. Prior to disambiguation itself, we sort the words by the number of senses is has, in order that the word with the fewest senses will be considered first. The idea behind this is that words that have fewer senses are easier to disambiguate (Chen et al., 2014) . As the algorithm relies on the words in the context which may themselves be ambiguous, if words in the context have been disambiguated already, this information can be used for the ambiguous words that follow. We, therefore, use the resulting sense of each word for the disambiguation of the following words starting with the \\\"easiest\\\" words. Our method requires lexeme embeddings L s,w for each sense s. For this, we use AutoExtend (Rothe and Sch\\u00fctze, 2015) to create additional embeddings for senses from WordNet on the basis of word embeddings. AutoExtend is an auto-encoder that relies on the relations present in WordNet to learn embeddings for senses and lexemes. To create these embeddings, a neural network containing lexemes and sense layers is built, while the WordNet relations are used to create links between each layer. The advantage of their method is that it is flexible: it can take any set of word embeddings and any lexical database as input and produces embeddings of senses and lexemes, without requiring any extra training data. Ultimately, for each word w we need a vector for the context C w , and for each sense s of word w we need a gloss vector G s . The context vector C w is defined as the mean of all the content word representations in the sentence: if a word in the context has already been disambiguated, we use the corresponding sense embedding; otherwise, we use the word embedding. For each sense s, we take its gloss as provided in WordNet. In line with Banerjee and Pedersen (2002) , we expand this gloss with the glosses of related meanings, excluding antonyms. Similar to the creation of the context vectors, the gloss vector G s is created by averaging the word embeddings of all the content words in the gloss. Experiments The performance of our algorithm was tested on both Dutch and English sentences in an all-words setup. Our sense inventory for Dutch is Cornetto (Vossen et al., 2012) while, for English, we use WordNet 1.7.1 (Fellbaum, 1998) . In Cornetto, 51.0% of the senses have glosses associated with them and in the Princeton WordNet, almost all of them do. The DutchSemCor corpus (Vossen et al., 2013) is used for Dutch evaluation and, for English, we use SemCor (Fellbaum, 1998) . For both languages, a random subset of 5000 manually annotated sentences from each corpus was created. Additionally, we test on the Senseval-2 (SE-2) and Senseval-3 (SE-3) all-words datasets (Snyder and Palmer, 2004; Palmer et al., 2001) 1 . We build 300-dimensional word embeddings on the Dutch Sonar corpus (Oostdijk et al., 2013 ) using word2vec CBOW (Mikolov et al., 2013) , and create sense-and lexeme embeddings with AutoExtend. For English, we use the embeddings from Rothe and Sch\\u00fctze (2015) 2 . They lie within the same vector space as the pre-trained word embeddings by Mikolov et al. (2013) 3 , trained on a part of the Google News dataset, which contains about 100 billion words. This model (similar to the Dutch model) contains 300-dimensional vectors for 3 million words and phrases. We evaluate our method by comparing it with a random baseline and Simplified Lesk with expanded glosses (SE-Lesk) (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2002) . Additionally, we compare our system to a state-of-the-art knowledge-based WSD system, UKB (Agirre and Soroa, 2009) , that, similar to our method, does not require any manually tagged data. UKB can be used for graph-based WSD using a pre-existing knowledge base. It applies random walks, e.g. Personalized PageRank, on the Knowledge Base graph to rank the vertices according to the context. We use UKBs Personalized PageRank method word-by-word with WordNet 1.7 and eXtended WordNet for English, as this setup yielded the best results in Agirre and Soroa (2009) . For Dutch, we use the Cornetto database as input graph. We do not compare our system to the initial results of AutoExtend (Rothe and Sch\\u00fctze, 2015) as they tested it in a supervised setup using sense embeddings as features. However, as is customary in WSD evaluation, we do compare our system to the most frequent WordNet sense baseline, which is notoriously difficult to beat due to the highly skewed distribution of word senses (Agirre and Edmonds, 2007) . As this baseline relies on manually annotated data, which our system aims to avoid, we consider this baseline to be semi-supervised and therefore an upper bound. For Dutch, the manually annotated part of DutchSemCor is balanced per sense which means that an equal number of examples for each sense is annotated. It is therefore not a reliable source for computing the most frequent sense. Alternatively, similar to Vossen et al. (2013) , we derive sense frequencies by using the automatically annotated counts in DutchSemCor 4 , assuming that the automatic annotation sufficiently reflects the true distribution for this purpose. The most frequent sense baseline for Dutch is, therefore, lower as compared to the English one, where the most frequent sense of a word is fully based on manual annotation. Results The results of the evaluation of our method (Lesk++) for both Dutch and English can be found in Table 1. Accuracy is calculated by dividing the number of words that were disambiguated correctly, as compared to the sense tagged corpus, by the total amount of polysemous words. Results are in bold when statistically significant over the baselines at p < 0.05. For both Dutch and English, our method performs significantly better than SE-Lesk and the random baseline for all tasks. Also, our system performs better than UKB on both SemCor and DutchSemCor. On DutchSemCor, it outperforms the most frequent baseline. Dutch Effects of sorting, lexemes, and glosses The main idea behind our method is a simple combination of two cosine similarity scores. In a second experiment, we evaluate the effects of both of these scores by using them separately. Additionally, we examine the use of sorting the words by its number of senses before disambiguation. We compare our final results with a system where similarity is only computed between the context and gloss vector and with a system that only computes the cosine distance between the context and the lexeme (only the first and the second part of Equation 1 respectively). Both systems are tested without and with (+S) sorting. The results of this third experiment on the sense tagged corpora for Dutch (DSC) and English (SC) can be found in Table 2 The second and the fourth column show results of a system that only uses the lexeme (Lex) or gloss vectors (Gloss) respectively. In the third and last column sorting (+S) is added. For Dutch, the results indicate that sorting the words by its number of senses by itself is not very effective compared to the system that does not use this module. The use of glosses, on the other hand, seems to be very effective while the combination of both measures yields the best results. The effect of the gloss vectors is even stronger for English, which can be explained by the fact that the English WordNet has a higher gloss coverage. Also, for English, although both sorting and glosses are effective, the combination performs better. Comparison of Different Domains To examine the robustness of our system in different domains, we evaluate it on different parts of Dutch-SemCor. We randomly took 5000 manually annotated sentences from each of the four largest subsets of the corpus. The results of this experiment for the all-words task can be found in Table 3 tion of the DSC dataset, our method outperforms SE-Lesk, the random baseline and UKB. Furthermore, our method outperforms the most frequent baseline on three of them. The newspapers subsection forms an exception, probably because it belongs to a more general domain (Agirre et al., 2014) . Discussion The difference in results for Dutch and English can possibly be explained by the coverage of the datasets. The Cornetto coverage is about 60%, compared to Princeton Wordnet, with an average polysemy of 1.07 for nouns, 1.56 for verbs and 1.05 for adjectives while, for English it is 1.24 for nouns, 2.17 for verbs and 1.40 for adjectives. Also, not all Dutch senses have corresponding glosses while most of the English ones do. As our method relies greatly on gloss vectors, this could affect its performance. Our WSD approach combines a lexical knowledge base with word-and sense embeddings. The results of our experiments show that the use of embeddings can help improve other Lesk methods (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2002) . An obvious next step would be to see whether other extensions that do not require manually tagged data are compatible as well. For example, Vasilescu et al. (2004) shows improvements by pre-selecting context words using the WordNet hierarchy. Also, the method of Miller et al. (2012) could be used to first expand the glosses and/or the context before using our adaptation of the Lesk system. Conclusion In this paper we propose an extension to the Lesk algorithm which uses sense, gloss and context embeddings to compute the similarity of word senses to the context in which the words occur. Although our approach is a straightforward extension to the Lesk algorithm, it achieves better performance compared to Lesk and a random baseline and outperforms, or yields similar performance to, a state-of-the-art knowledge-based system. For Dutch, it outperforms all other systems including the most frequent sense on three out of four subsets. A second experiment confirms the effects of gloss vectors while the results of a final experiment indicate that our method works well in different domains. The main advantage of our method is its simplicity which makes it fast and easy to apply to other languages. It furthermore only requires unlabeled text and the definitions of senses, and does not rely on any manually annotated data, which makes our system an attractive alternative for supervised WSD.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"French\",\n          \"Portuguese (Brazil)\",\n          \"Chinese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publisher\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 111,\n        \"samples\": [\n          \"{\\\\'U}FAL MFF UK\",\n          \"International Committee on Computational Linguistics\",\n          \"The COLING 2016 Organizing Committee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 73280,\n        \"samples\": [\n          \"https://aclanthology.org/P17-2088\",\n          \"https://aclanthology.org/2020.winlp-1.26\",\n          \"https://aclanthology.org/D09-1013\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 72502,\n        \"samples\": [\n          \"A General-Purpose Rule Extractor for SCFG-Based Machine Translation\",\n          \"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging\",\n          \"CantoMap: a Hong Kong Cantonese MapTask Corpus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_fr\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1906,\n        \"samples\": [\n          \"Notation automatique de r\\u00e9ponses courtes d'\\u00e9tudiants : pr\\u00e9sentation de la campagne DEFT 2022 (Automatic grading of students' short answers : presentation of the DEFT 2022 challenge)\",\n          \"Towards a More Efficient Development of Statistical Machine Translation Systems (Vers un d\\u00e9veloppement plus efficace des syst\\u00e8mes de traduction statistique : un peu de vert dans un monde de BLEU) [in French]\",\n          \"Entre \\u00e9crit et oral ? Analyse compar\\u00e9e de conversations de type tchat et de conversations t\\u00e9l\\u00e9phoniques dans un centre de contact client\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "data_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n857WGV3fsw0"
      },
      "source": [
        "Création d'un nouveau dataframe filtré qui contiendra uniquement les informations concernant le français, afin de ne pas avoir un dataframe avec trop de colonnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "oFJuirVFfndP"
      },
      "outputs": [],
      "source": [
        "data_netoy=pd.DataFrame(columns=[\"acl_id\",\"title\",\"abstract\",\"full_text\",\"language\",\"publisher\",\"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "OzjAznRUfoN_"
      },
      "outputs": [],
      "source": [
        "pub = [\n",
        "    \"ATALA\",\n",
        "    \"ATALA/AFCP\",\n",
        "    \"AFCP-ATALA\",\n",
        "    \"Association pour le Traitement Automatique des Langues\"\n",
        "]\n",
        "\n",
        "data_netoy = data_clean.loc[\n",
        "    data_clean[\"title_fr\"].notna() &\n",
        "    data_clean[\"publisher\"].isin(pub),\n",
        "    [\"acl_id\", \"title_fr\", \"abstract\", \"full_text\", \"publisher\",\"url\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "qmCsy-5PfoKp",
        "outputId": "a12dfaa2-5cb1-4e9b-d029-9db6db1e8dc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              acl_id  \\\n",
              "100      2004.jeptalnrecital-long.32   \n",
              "572      2018.jeptalnrecital-long.10   \n",
              "576     2018.jeptalnrecital-court.17   \n",
              "581      2017.jeptalnrecital-long.13   \n",
              "604       2015.jeptalnrecital-long.9   \n",
              "...                              ...   \n",
              "69517   2009.jeptalnrecital-court.21   \n",
              "69519  2015.jeptalnrecital-recital.5   \n",
              "69523  2015.jeptalnrecital-recital.6   \n",
              "69524   2009.jeptalnrecital-court.11   \n",
              "69528   2008.jeptalnrecital-court.16   \n",
              "\n",
              "                                                title_fr  \\\n",
              "100    Annoter en constituants pour évaluer des analy...   \n",
              "572    Intégration de contexte global par amorçage po...   \n",
              "576    Utilisation de Représentations Distribuées de ...   \n",
              "581    Apprendre des représentations jointes de mots ...   \n",
              "604    Méthode faiblement supervisée pour l'extractio...   \n",
              "...                                                  ...   \n",
              "69517  La complémentarité des approches manuelle et a...   \n",
              "69519  Alignement multimodal de ressources éducatives...   \n",
              "69523  État de l'art : analyse des conversations écri...   \n",
              "69524  Chaîne de traitement linguistique : du repérag...   \n",
              "69528  Vers l'identification et le traitement des act...   \n",
              "\n",
              "                                                abstract  \\\n",
              "100    Cet article présente l'annotation en constitua...   \n",
              "572    Les approches neuronales obtiennent depuis plu...   \n",
              "576    L'identification des entités nommées dans un t...   \n",
              "581    1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...   \n",
              "604    La détection d'opinion ciblée a pour but d'att...   \n",
              "...                                                  ...   \n",
              "69517  Les ressources lexicales sont essentielles pou...   \n",
              "69519  Cet article présente certaines questions de re...   \n",
              "69523  Le développement du Web 2.0 et le processus de...   \n",
              "69524  Cet article présente la chaîne de traitement l...   \n",
              "69528  Il peut être difficile d'attribuer une seule v...   \n",
              "\n",
              "                                               full_text publisher  \\\n",
              "100    Cet article présente l'annotation en constitua...     ATALA   \n",
              "572    Les approches neuronales obtiennent depuis plu...     ATALA   \n",
              "576    L'identification des entités nommées dans un t...     ATALA   \n",
              "581    1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...     ATALA   \n",
              "604    La détection d'opinion ciblée a pour but d'att...     ATALA   \n",
              "...                                                  ...       ...   \n",
              "69517  Les ressources lexicales sont essentielles pou...     ATALA   \n",
              "69519  Cet article présente certaines questions de re...     ATALA   \n",
              "69523  Le développement du Web 2.0 et le processus de...     ATALA   \n",
              "69524  Cet article présente la chaîne de traitement l...     ATALA   \n",
              "69528  Il peut être difficile d'attribuer une seule v...     ATALA   \n",
              "\n",
              "                                                     url  \n",
              "100    https://aclanthology.org/2004.jeptalnrecital-l...  \n",
              "572    https://aclanthology.org/2018.jeptalnrecital-l...  \n",
              "576    https://aclanthology.org/2018.jeptalnrecital-c...  \n",
              "581    https://aclanthology.org/2017.jeptalnrecital-l...  \n",
              "604    https://aclanthology.org/2015.jeptalnrecital-l...  \n",
              "...                                                  ...  \n",
              "69517  https://aclanthology.org/2009.jeptalnrecital-c...  \n",
              "69519  https://aclanthology.org/2015.jeptalnrecital-r...  \n",
              "69523  https://aclanthology.org/2015.jeptalnrecital-r...  \n",
              "69524  https://aclanthology.org/2009.jeptalnrecital-c...  \n",
              "69528  https://aclanthology.org/2008.jeptalnrecital-c...  \n",
              "\n",
              "[1501 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-710770c0-5387-44b6-9cbd-5328c0db9044\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acl_id</th>\n",
              "      <th>title_fr</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "      <th>publisher</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>2004.jeptalnrecital-long.32</td>\n",
              "      <td>Annoter en constituants pour évaluer des analy...</td>\n",
              "      <td>Cet article présente l'annotation en constitua...</td>\n",
              "      <td>Cet article présente l'annotation en constitua...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2004.jeptalnrecital-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>2018.jeptalnrecital-long.10</td>\n",
              "      <td>Intégration de contexte global par amorçage po...</td>\n",
              "      <td>Les approches neuronales obtiennent depuis plu...</td>\n",
              "      <td>Les approches neuronales obtiennent depuis plu...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2018.jeptalnrecital-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>2018.jeptalnrecital-court.17</td>\n",
              "      <td>Utilisation de Représentations Distribuées de ...</td>\n",
              "      <td>L'identification des entités nommées dans un t...</td>\n",
              "      <td>L'identification des entités nommées dans un t...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2018.jeptalnrecital-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>2017.jeptalnrecital-long.13</td>\n",
              "      <td>Apprendre des représentations jointes de mots ...</td>\n",
              "      <td>1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...</td>\n",
              "      <td>1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2017.jeptalnrecital-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>2015.jeptalnrecital-long.9</td>\n",
              "      <td>Méthode faiblement supervisée pour l'extractio...</td>\n",
              "      <td>La détection d'opinion ciblée a pour but d'att...</td>\n",
              "      <td>La détection d'opinion ciblée a pour but d'att...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69517</th>\n",
              "      <td>2009.jeptalnrecital-court.21</td>\n",
              "      <td>La complémentarité des approches manuelle et a...</td>\n",
              "      <td>Les ressources lexicales sont essentielles pou...</td>\n",
              "      <td>Les ressources lexicales sont essentielles pou...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2009.jeptalnrecital-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69519</th>\n",
              "      <td>2015.jeptalnrecital-recital.5</td>\n",
              "      <td>Alignement multimodal de ressources éducatives...</td>\n",
              "      <td>Cet article présente certaines questions de re...</td>\n",
              "      <td>Cet article présente certaines questions de re...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69523</th>\n",
              "      <td>2015.jeptalnrecital-recital.6</td>\n",
              "      <td>État de l'art : analyse des conversations écri...</td>\n",
              "      <td>Le développement du Web 2.0 et le processus de...</td>\n",
              "      <td>Le développement du Web 2.0 et le processus de...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69524</th>\n",
              "      <td>2009.jeptalnrecital-court.11</td>\n",
              "      <td>Chaîne de traitement linguistique : du repérag...</td>\n",
              "      <td>Cet article présente la chaîne de traitement l...</td>\n",
              "      <td>Cet article présente la chaîne de traitement l...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2009.jeptalnrecital-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69528</th>\n",
              "      <td>2008.jeptalnrecital-court.16</td>\n",
              "      <td>Vers l'identification et le traitement des act...</td>\n",
              "      <td>Il peut être difficile d'attribuer une seule v...</td>\n",
              "      <td>Il peut être difficile d'attribuer une seule v...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2008.jeptalnrecital-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1501 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-710770c0-5387-44b6-9cbd-5328c0db9044')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-710770c0-5387-44b6-9cbd-5328c0db9044 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-710770c0-5387-44b6-9cbd-5328c0db9044');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9b73cf87-1ef7-4b25-abe3-193f4263a5cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b73cf87-1ef7-4b25-abe3-193f4263a5cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9b73cf87-1ef7-4b25-abe3-193f4263a5cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_97cd4e62-51f0-4360-adfa-7cb01f1ce3b7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_netoy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_97cd4e62-51f0-4360-adfa-7cb01f1ce3b7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_netoy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_netoy",
              "summary": "{\n  \"name\": \"data_netoy\",\n  \"rows\": 1501,\n  \"fields\": [\n    {\n      \"column\": \"acl_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"2006.jeptalnrecital-poster.10\",\n          \"2006.jeptalnrecital-recitalposter.2\",\n          \"F14-2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_fr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"Reconnaissance de la m\\u00e9trique des po\\u00e8mes arabes par les r\\u00e9seaux de neurones artificiels\",\n          \"Une approche g\\u00e9ometrique pour la mod\\u00e9lisation des lexiques en langues sign\\u00e9es\",\n          \"Multilingual Summarization Experiments on English, Arabic and French (R\\u00e9sum\\u00e9 Automatique Multilingue Exp\\u00e9rimentations sur l'Anglais, l'Arabe et le Fran\\u00e7ais) [in French]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1458,\n        \"samples\": [\n          \"La portabilit\\u00e9 entre les langues des syst\\u00e8mes de reconnaissance d'entit\\u00e9s nomm\\u00e9es est co\\u00fbteuse en termes de temps et de connaissances linguistiques requises. L'adaptation des syst\\u00e8mes symboliques souffrent du co\\u00fbt de d\\u00e9veloppement de nouveaux lexiques et de la mise \\u00e0 jour des r\\u00e8gles contextuelles. D'un autre c\\u00f4t\\u00e9, l'adaptation des syst\\u00e8mes statistiques se heurtent au probl\\u00e8me du co\\u00fbt de pr\\u00e9paration d'un nouveau corpus d'apprentissage. Cet article \\u00e9tudie l'int\\u00e9r\\u00eat et le co\\u00fbt associ\\u00e9 pour porter un syst\\u00e8me existant de reconnaissance d'entit\\u00e9s nomm\\u00e9es pour du texte bien form\\u00e9 vers une autre langue. Nous pr\\u00e9sentons une m\\u00e9thode peu co\\u00fbteuse pour porter un syst\\u00e8me symbolique d\\u00e9di\\u00e9 au fran\\u00e7ais vers l'anglais. Pour ce faire, nous avons d'une part traduit automatiquement l'ensemble des lexiques de mots d\\u00e9clencheurs au moyen d'un dictionnaire bilingue. D'autre part, nous avons manuellement modifi\\u00e9 quelques r\\u00e8gles de mani\\u00e8re \\u00e0 respecter la syntaxe de la langue anglaise. Les r\\u00e9sultats exp\\u00e9rimentaux sont compar\\u00e9s \\u00e0 ceux obtenus avec un syst\\u00e8me de r\\u00e9f\\u00e9rence d\\u00e9velopp\\u00e9 pour l'anglais.\",\n          \"Ces derni\\u00e8res d\\u00e9cennies, notre regard sur les donn\\u00e9es lexicales informatis\\u00e9es a beaucoup \\u00e9volu\\u00e9. D'abord annexe lexicale d'une grammaire ou d'une application, les dictionnaires d'application sont devenues bases lexicales dans lesquelles s'agr\\u00e9geaient les donn\\u00e9es de diff\\u00e9rents modules. L'effort suivant s'est concentr\\u00e9 dans la normalisation du format, avec notamment un mouvement massif vers le tout XML. Le travail de normalisation des structures des lexiques a suivi ensuite. Mais, alors que les normes restent structurellement proches des dictionnaires originaux (vus comme une collection d'entr\\u00e9es organis\\u00e9es de mani\\u00e8re arborescentes), ont \\u00e9merg\\u00e9 des mod\\u00e8les de lexiques pens\\u00e9s comme des graphes. Parall\\u00e8lement, les travaux dans le domaine du Web S\\u00e9mantique nous ont donn\\u00e9 les moyens de repr\\u00e9senter, manipuler et surtout partager nos ressources lexicales. En adoptant une repr\\u00e9sentation en RDF (Resource Description Framework), ainsi que l'approche des donn\\u00e9es li\\u00e9es ouverte (Linked Open Data), nous avons enfin les moyens de lier, fusionner, parcourir l'ensemble des ressources lexicales comme s'il ne s'agissait que d'une seule ressource. Dans cette pr\\u00e9sentation, en m'appuyant sur les travaux r\\u00e9alis\\u00e9s dans le cadre des projets Papillon, LexALP et DBnary, j'essaierai de montrer en quoi, au del\\u00e0 de l'effet de mode actuel, l'utilisation du format des donn\\u00e9s li\\u00e9es ouvertes, est l'\\u00e9tape suivante naturelle dans notre \\u00e9tude du lexique.\",\n          \"Le contr\\u00f4le des hypoth\\u00e8ses concurrentes g\\u00e9n\\u00e9r\\u00e9es par les diff\\u00e9rents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgr\\u00e9 de nombreuses avanc\\u00e9es en terme de robustesse. Nous pr\\u00e9sentons dans cet article une m\\u00e9thodologie g\\u00e9n\\u00e9rique de contr\\u00f4le exploitant des techniques issues de l'aide multicrit\\u00e8re \\u00e0 la d\\u00e9cision. \\u00c0 partir de l'ensemble des crit\\u00e8res de comparaison disponibles et la formalisation des pr\\u00e9f\\u00e9rences d'un expert, l'approche propos\\u00e9e \\u00e9value la pertinence relative des diff\\u00e9rents objets linguistiques g\\u00e9n\\u00e9r\\u00e9s et conduit \\u00e0 la mise en place d'une action de contr\\u00f4le appropri\\u00e9e telle que le filtrage, le classement, le tri ou la propagation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1495,\n        \"samples\": [\n          \"Dans le cadre de notre projet de recherche, qui a pour but l'impl\\u00e9mentation d'un outil de simplification des emplois sp\\u00e9cialis\\u00e9s de verbes dans des corpus m\\u00e9dicaux \\u00e0 partir de l'analyse syntaxico-s\\u00e9mantique de ces verbes en contexte, nous proposons une analyse de quelques approches et travaux qui ont pour objet principal la description du verbe dans les trois domaines de recherche \\u00e0 l'interface desquels se situe notre projet : linguistique, TAL et terminologie. Nous d\\u00e9crivons plus particuli\\u00e8rement les travaux qui peuvent avoir une incidence sur notre \\u00e9tude. Cet \\u00e9tat de l'art nous permet de mieux conna\\u00eetre le cadre th\\u00e9orique dans lequel s'int\\u00e8gre notre projet de recherche et d'avoir les rep\\u00e8res et r\\u00e9f\\u00e9rences susceptibles de contribuer \\u00e0 sa r\\u00e9alisation. Introduction Contexte g\\u00e9n\\u00e9ral L'int\\u00e9r\\u00eat port\\u00e9 au verbe change selon que l'on se situe dans le domaine de la linguistique, de la terminologie ou celui du Traitement Automatique des Langues (TAL). En effet, selon leurs objectifs respectifs, chacune de ces disciplines octroie au verbe une place diff\\u00e9rente reconnaissable \\u00e0 travers l'importance qui lui est donn\\u00e9e dans les diverses \\u00e9tudes et les diff\\u00e9rents cadres th\\u00e9oriques propres au domaine concern\\u00e9. Quelles sont les \\u00e9clairages propos\\u00e9s par les diff\\u00e9rentes approches (linguistique, terminologie et TAL) qui prennent le verbe comme objet d'\\u00e9tude ? Comment-est ce que le verbe est abord\\u00e9 dans ces travaux ? Est-il trait\\u00e9 au m\\u00eame titre que les autres cat\\u00e9gories grammaticales en l'occurrence le nom ? En quoi est-ce que le verbe et sa structure argumentale peuvent-ils \\u00eatre utiles en vue de la simplification des textes sp\\u00e9cialis\\u00e9s ? Telles sont les questions auxquelles nous allons essayer de r\\u00e9pondre dans ce travail qui a pour objectif de dresser un \\u00e9tat de l'art de diff\\u00e9rents mod\\u00e8les de descriptions du verbe dans les trois disciplines concern\\u00e9es. Travail envisag\\u00e9 Le projet que nous entreprenons a pour objectif de proposer une m\\u00e9thode de simplification de textes m\\u00e9dicaux, \\u00e0 partir d'une analyse syntaxico-s\\u00e9mantique des verbes en contexte. Au terme de ce travail, nous souhaitons impl\\u00e9menter un outil de simplification des textes \\u00e9crits en fran\\u00e7ais (et \\u00e9ventuellement en anglais), sp\\u00e9cialis\\u00e9s en cardiologie (ou en d'autres domaines m\\u00e9dicaux). L'outil devra rep\\u00e9rer les emplois verbaux peu communs au discours des patients et devra ensuite proposer des emplois s\\u00e9mantiquement similaires, mais plus adapt\\u00e9s au niveau de sp\\u00e9cialisation de ces utilisateurs. La m\\u00e9thode propos\\u00e9e est bas\\u00e9e sur diff\\u00e9rentes hypoth\\u00e8ses. En effet, nous pensons que le pr\\u00e9dicat verbal peut \\u00eatre un excellent ORNELLA WANDJI point de d\\u00e9part pour cerner la s\\u00e9mantique des textes sp\\u00e9cialis\\u00e9s puisqu'il sert \\u00e0 exprimer l'expertise port\\u00e9e par les mots qui l'entourent dans la phrase (L'Homme & Bodson, 1997) . Par cons\\u00e9quent, nous consid\\u00e9rons la structure argumentale du verbe comme une importante source d'informations sur les propri\\u00e9t\\u00e9s s\\u00e9mantique et syntaxique du verbe. Ce projet de recherche s'inscrit dans le cadre de la simplification des textes sp\\u00e9cialis\\u00e9s. Il s'agit d'une t\\u00e2che du TAL qui consiste \\u00e0 cibler et \\u00e0 simplifier automatiquement les \\u00e9l\\u00e9ments, qui emp\\u00eachent la compr\\u00e9hension ais\\u00e9e d'un texte, afin de faciliter l'acc\\u00e8s au contenu de ce texte. Les travaux existants se focalisent sur la simplification syntaxique (Brouwers et al., 2014) , la simplification lexicale (Elhadad, 2006; Leroy et al., 2012) , la combinaison des fonctions lexicales, grammaticales, syntaxiques et discursives (Heilman et al., 2007 (Heilman et al., , 2008;; Pitler & Nenkova, 2008) , les caract\\u00e9ristiques de surface des textes (nombre de caract\\u00e8res et syllabes par mot), la capitalisation, la ponctuation et les ellipses (Tapas & Orr, 2009) , ou la mod\\u00e9lisation statistique de la langue (Thompson & Callan, 2004) . L'approche que nous proposons se situe \\u00e0 mi-chemin entre la simplification lexicale et la simplification syntaxique et vise \\u00e0 r\\u00e9duire les difficult\\u00e9s de compr\\u00e9hension des textes m\\u00e9dicaux fortement sp\\u00e9cialis\\u00e9s \\u00e0 travers la simplification des constructions verbales. \\u00c0 notre connaissance, il n'existe pas de travaux en simplification de textes autant orient\\u00e9s sur l'analyse des verbes et de leurs argumentales. Une \\u00e9tude comparative du fonctionnement des verbes dans des textes de corpus m\\u00e9dicaux r\\u00e9dig\\u00e9s par des experts et des non-experts en m\\u00e9d\\u00e9cine a permis d'observer que les verbes ont tendance \\u00e0 s'entourer d'arguments fortement sp\\u00e9cialis\\u00e9s dans les \\u00e9crits des experts, rendant parfois leur compr\\u00e9hension difficile pour les non-experts (Wandji Tchami et al., 2013) . Notre travail de recherche vient donner une suite \\u00e0 cette observation. L'objectif principal \\u00e9tant d'am\\u00e9liorer certains aspects de la m\\u00e9thode (l'annotation automatique des arguments, l'analyse des verbes) et de la d\\u00e9velopper davantage, en y int\\u00e9grant un travail de simplification. Le travail pr\\u00e9sent\\u00e9 ici a pour objectif de nous aider \\u00e0 mieux cerner le cadre th\\u00e9orique dans lequel s'inscrit le projet de recherche que nous envisageons de r\\u00e9aliser, en nous donnant une id\\u00e9e pr\\u00e9cise des travaux et outils existants, centr\\u00e9s sur le verbe et susceptibles de nous aider pour r\\u00e9alisation du travail envisag\\u00e9. Il est organis\\u00e9 autour de 4 grandes parties dont les trois premi\\u00e8res sont consacr\\u00e9es \\u00e0 l'exploration des travaux portant sur le pr\\u00e9dicat verbal, respectivement en linguistique (section 2), terminologie (section 3) et TAL (section 4). Dans la derni\\u00e8re partie (section 5), nous faisons une discussion de l'impact que les travaux de l'\\u00e9tat de l'art peuvent avoir sur la r\\u00e9alisation de notre projet de recherche et nous abordons les perspectives de travail que nous envisageons d'explorer. Les approches linguistiques d\\u00e9di\\u00e9es au verbe En linguistique, de nombreux cadres th\\u00e9oriques placent le verbe au coeur de leurs travaux. Nous nous attardons plus particuli\\u00e8rement sur les cadres th\\u00e9oriques qui s'int\\u00e9ressent au verbe en tant qu'\\u00e9l\\u00e9ment r\\u00e9gisseur (c'est-\\u00e0-dire un \\u00e9l\\u00e9ment dont la r\\u00e9alisation syntaxique et s\\u00e9mantique d\\u00e9pend grandement de la pr\\u00e9sence d'autres constituants qui lui sont subordonn\\u00e9s) et d\\u00e9crivent son rapport avec les autres constituants de la phrase. Il existe diverses approches de description du verbe, mais nous ne sommes pas en mesure de fournir une pr\\u00e9sentation exhaustive de toutes les approches th\\u00e9oriques existantes. Nous nous limitons \\u00e0 celles qui servent de bases \\u00e0 la r\\u00e9alisation de diff\\u00e9rentes t\\u00e2ches du TAL (sections 4.2 et 4.1), et \\u00e0 la conception des ressources (section 4.3). Le noeud verbal au coeur de la syntaxe structurale La syntaxe structurale (Tesni\\u00e8re, 1959) est la premi\\u00e8re th\\u00e9orie \\u00e0 avoir mis le verbe au centre de la phrase. En syntaxe structurale, l'ensemble des mots d'une phrase constitue une v\\u00e9ritable hi\\u00e9rarchie au sein de laquelle les constituants sont li\\u00e9s les uns aux autres par des liens de d\\u00e9pendance. La phrase, encore appel\\u00e9e stemma, est d\\u00e9crite comme \\u00e9tant un sch\\u00e9ma arborescent, ou un ensemble de noeuds. Le noeud quant \\u00e0 lui d\\u00e9signe un ensemble constitu\\u00e9 d'un r\\u00e9gissant et de tous ses subordonn\\u00e9s. Dans cette configuration, le noeud central correspond en g\\u00e9n\\u00e9ral au noeud verbal. Le verbe, \\u00e9tant au centre du noeud verbal, est par cons\\u00e9quent au coeur de la phrase. Il est pour ainsi dire le r\\u00e9gissant de toute la phrase. La notion de noeud verbal est d\\u00e9finie \\u00e0 travers une m\\u00e9taphore du drame : \\u00ab le noeuds verbale ... exprime un tout petit drame. Comme un drame, ... il comporte obligatoirement un proc\\u00e8s et plus souvent des acteurs et des circonstants \\u00bb. C'est dans cette optique que cette approche postule l'existence des actants ou participants au proc\\u00e8s verbal (Tesni\\u00e8re, 1959) La th\\u00e9orie des cadres s\\u00e9mantiques Encore appel\\u00e9e Frame semantics, la s\\u00e9mantique des cadres est une approche qui remonte aux ann\\u00e9es 1980. Elle est une extension de la grammaire des Cas (Fillmore, 1968) , qui \\u00e9voquait d\\u00e9j\\u00e0 l'existence des r\\u00f4les s\\u00e9mantiques (agent, lieu, etc.) dans la structure syntaxique profonde du verbe. La s\\u00e9mantique des cadres (Fillmore, 1982) vise \\u00e0 l'origine \\u00e0 faciliter la compr\\u00e9hension des textes. Son principal objectif est de d\\u00e9crire la syntaxe et la s\\u00e9mantique des unit\\u00e9s lexicales (noms, adjectifs, verbes). L'id\\u00e9e principale de Fillmore est que le sens d'un mot ne peut \\u00eatre interpr\\u00e9t\\u00e9 que si l'on a acc\\u00e8s aux informations (linguistiques, extralinguistiques ou encyclop\\u00e9diques) essentielles faisant r\\u00e9f\\u00e9rence \\u00e0 ce mot. Ces informations peuvent \\u00eatre accessibles gr\\u00e2ce \\u00e0 un frame ou cadre au sein duquel les unit\\u00e9s lexicales sont organis\\u00e9es. Le cadre est d\\u00e9fini comme un sc\\u00e9nario, un sch\\u00e9ma ou une structure conceptuelle qui sous-tend l'utilisation d'un item lexical ainsi que son interpr\\u00e9tation (Fontenelle, 2009) . Il d\\u00e9crit une situation particuli\\u00e8re ainsi que les participants Frame elements (FE) qui peuvent \\u00eatre obligatoires (core elements) ou facultatifs (non core elements). Un cadre est \\u00e9voqu\\u00e9 par une unit\\u00e9 lexicale (LU). Par exemple, le frame de la transaction commerciale (Fillmore, 1976) a plusieurs unit\\u00e9s \\u00e9vocatrices : acheter, vendre, payer, recup\\u00e9rer et plusieurs participants : obligatoires (VENDEUR, ARGENT, BIEN, ACHETEUR) et facultatifs (MOYEN), etc. Lorsque l'unit\\u00e9 \\u00e9vocatrice du cadre est un verbe, l'analyse est focalis\\u00e9e sur les arguments de ce dernier qui repr\\u00e9sentent les \\u00e9l\\u00e9ments du cadre. Les classifications de verbes La classification des verbes (anglais) selon Levin Beth Levin (Levin, 1993) propose une classification lexico-s\\u00e9mantique de verbes anglais \\u00e0 partir d'une analyse de leur fonctionnement (syntaxe, classe s\\u00e9mantique des arguments s\\u00e9lectionn\\u00e9s, etc.). Les verbes qui affichent un ensemble d'alternances (de diath\\u00e8ses ou frames) identiques ou similaires dans la r\\u00e9alisation de leurs structures argumentales sont suppos\\u00e9s partager certains \\u00e9l\\u00e9ments de sens et, de ce fait, sont regroup\\u00e9s dans une classe s\\u00e9mantiquement homog\\u00e8ne. L'alternance de diath\\u00e8ses (la relation entre deux r\\u00e9alisations de surface d'un m\\u00eame pr\\u00e9dicat), qui est le principal crit\\u00e8re d'identification des classes verbales dans cette approche, est appuy\\u00e9e par des propri\\u00e9t\\u00e9s suppl\\u00e9mentaires li\\u00e9es \\u00e0 la souscat\\u00e9gorisation, \\u00e0 la morphologie et aux verbes ayant un s\\u00e9mantisme complexe. \\u00c0 partir de ces crit\\u00e8res, la classification couvre 3 024 verbes, 4 186 sens, 240 classes de verbes construites autour de 79 alternances. Par exemple, la classe des pr\\u00e9dicats d\\u00e9notant une configuration spatiale contient les verbes suivants : balance, bend, bow, crouch, dangle, flop, fly, hang, hover, jut, kneel, lean, lie, loll, loom, lounge, nestle, open, perch, plop, project, protude, recline, rest, rise, roost, sag, sit, slope, slouch, slump, sprawl, squat, stand, stoop, straddle, swing, tilt, tower (Levin, 1993) . Une extension substantielle de cette classification int\\u00e8gre 57 nouvelles classes pour les verbes qui n'ont pas \\u00e9t\\u00e9 couverts initiallement (Korhonen & Briscoe, 2004) . Parmi les nouvelles classes, FORCE class regroupe les verbes tels que manipulate, pressure, force. Les classes d'objets de Gaston Gross Une classe d'objets est un \\u00ab ensemble de substantifs, s\\u00e9mantiquement homog\\u00e8nes, qui d\\u00e9termine une rupture d'interpr\\u00e9tation d'un pr\\u00e9dicat donn\\u00e9, en d\\u00e9limitant un emploi sp\\u00e9cifique \\u00bb (Gross, 2008). En d'autres termes, les classes d'objets d\\u00e9terminent l'interpr\\u00e9tation donn\\u00e9e d'un pr\\u00e9dicat parmi d'autres possibles. Elles sont induites par les pr\\u00e9dicats (verbes et adjectifs) et permettent d'identifier en contexte les mots avec lesquels ils entretiennent une relation conceptuelle telle que la synonymie, l'antonymie, etc. Ces entit\\u00e9s sont construites sur des bases syntaxiques et concernent particuli\\u00e8rement les compl\\u00e9ments qui apportent beaucoup plus d'informations que le sujet dans l'interpr\\u00e9tation d'un pr\\u00e9dicat (Gross, 2012) . Par exemple, la phrase vous suivez n'est pas assez pr\\u00e9cise, ce qui rend son interpr\\u00e9tation difficile. Par contre, si l'on y ajoute un compl\\u00e9ment, l'interpr\\u00e9tation sera plus ais\\u00e9e et la signification du verbe sera plus transparente. Ainsi, dans la phrase vous suivez ce chemin, l'objet chemin peut \\u00eatre remplac\\u00e9 par un autre substantif comme route, rue, voie, sentier et le verbe garde le m\\u00eame sens. Ces substantifs peuvent donc \\u00eatre consid\\u00e9r\\u00e9s comme appartenant \\u00e0 une m\\u00eame classe d'objets, celle de <voies>. Par contre, si on remplace chemin par le mot cours, on est face \\u00e0 un autre emploi du verbe car cours appartient \\u00e0 une autre classe d'objets, appel\\u00e9e <enseignements>. Elle contient les mots comme s\\u00e9minaire, stage, formation, cycle \\u00e9tudes, etc. Le principal int\\u00e9r\\u00eat des classes d'objets est de rendre compte des diff\\u00e9rents emplois des pr\\u00e9dicats, en d\\u00e9terminant leurs sch\\u00e9mas d'arguments et en rattachant \\u00e0 ceux-ci un ensemble de propri\\u00e9t\\u00e9s qui les caract\\u00e9risent (Gross, 2008) . ORNELLA WANDJI Lexique-Grammaire des verbes fran\\u00e7ais Le lexique-grammaire des verbes du fran\\u00e7ais (Gross, 1975) est un dictionnaire syntaxique \\u00e9lectronique t\\u00e9l\\u00e9chargeable 1 . Il est organis\\u00e9 en plusieurs tables, chacune regroupant les verbes du lexique qui ont un fonctionnement comparable : constructions types, distribution des actants, s\\u00e9mantique, etc. Chaque table comprend un ensemble de propri\\u00e9t\\u00e9s, et un codage qui pr\\u00e9cise si l'\\u00e9l\\u00e9ment a ou non cette propri\\u00e9t\\u00e9. Chaque entr\\u00e9e d'une table contient les informations suivantes : l'\\u00e9l\\u00e9ment vedette, une construction type dans laquelle il peut appara\\u00eetre, et des constructions associ\\u00e9es \\u00e0 cette construction type. Les diff\\u00e9rents emplois des verbes, \\u00e9num\\u00e9r\\u00e9s dans les tables, sont d\\u00e9crits gr\\u00e2ces \\u00e0 des propri\\u00e9t\\u00e9s structurelles, distributionnelles et s\\u00e9mantiques. Par exemple, les tables de constructions sans compl\\u00e9ments pr\\u00e9positionnels contiennent des constructions types, parmi lesquelles N 0 V et N 0 V N 1 . La construction N 0 V accueille les verbes tels que pleuvoir, b\\u00eatifier, bouillir, pisser et selon le verbe, elle peut accepter un N 0 humain (Luc b\\u00eatifie), non humain (l'eau bout), impersonnel (il pleut), et le verbe peut \\u00eatre modifi\\u00e9 par un adverbe (\\u00e7a ne pisse pas loin) (Leclere, 1990) . Le verbe dans les travaux en terminologie Les entit\\u00e9s nominales ont longtemps occup\\u00e9 la place centrale dans les travaux sur les langues de sp\\u00e9cialit\\u00e9 au d\\u00e9triment des autres parties du discours, plus particuli\\u00e8rement des verbes, mis \\u00e0 l'\\u00e9cart pour diverses raisons. En effet, les travaux en terminologie se focalisent la plupart du temps sur la description des concepts, ou des entit\\u00e9s nominales (particuli\\u00e8rement les noms) et la mise au jour des relations qu'elles partagent (genre-esp\\u00e8ce, partie-tout, etc.). L'un des motifs principaux \\u00e9nonc\\u00e9s justifiant l'exclusion du verbe est la place accord\\u00e9e aux objets et \\u00e0 leurs d\\u00e9nominations dans l'approche de W\\u00fcster (W\\u00fcster, 1985) . Cette situation trouve \\u00e9galement une explication dans le fait que les entit\\u00e9s nominales sont g\\u00e9n\\u00e9ralement utilis\\u00e9es pour le d\\u00e9veloppement des terminologies, ontologies, th\\u00e9saurus, glossaires, ou des vocabulaires. Ce constat s'explique \\u00e9galement par les besoins croissants des applications : l'indexation et l'extraction d'informations sont des t\\u00e2ches typiquement bas\\u00e9es sur les entit\\u00e9s nominales. Pour ces raisons, la plupart des approches th\\u00e9oriques et m\\u00e9thodologiques sont adapt\\u00e9es aux entit\\u00e9s nominales. N\\u00e9anmoins, quelques travaux s'inspirant de la s\\u00e9mantique lexicale s'int\\u00e9ressent aux verbes et \\u00e0 leur mode de fonctionnement dans les domaines sp\\u00e9cialis\\u00e9s. Ces travaux montrent que l'\\u00e9tude du verbe est quasi indispensable dans le cadre des activit\\u00e9s comme l'extraction d'informations (Tateisi et al., 2004) , la conception des dictionnaires terminographiques (Tellier, 2008) ou encore la traduction sp\\u00e9cialis\\u00e9e (Pimentel, 2011) . Les structures argumentales des verbes peuvent \\u00e9galement servir pour la d\\u00e9tection automatique des relations s\\u00e9mantiques (Massimiliano et al., 2008) . Nous parlerons de deux approches d'analyse du verbe terminologique : l'approche conceptuelle (section 3.1) et l'approche lexico-s\\u00e9mantique (section 3.2). L'approche conceptuelle Le principe de l'approche conceptuelle stipule que l'on ne s'int\\u00e9resse au verbe que s'il a l'aptitude de d\\u00e9signer un \\u00ab concept d'activit\\u00e9 \\u00bb, c'est-\\u00e0-dire une activit\\u00e9 (L'Homme, 2012). Telle est la condition qui d\\u00e9termine l'int\\u00e9gration des verbes dans des ressources terminologiques. Autrement dit, le verbe ne peut \\u00eatre consid\\u00e9r\\u00e9 comme terme que s'il est fortement assimilable \\u00e0 un nom sur le plan conceptuel. Rey d\\u00e9finit clairement le statut du verbe selon la perspective conceptuelle en ces termes : \\u00ab la terminologie ne s'int\\u00e9resse aux signes (mots et unit\\u00e9s plus grandes que le mot) qu'en tant qu'ils fonctionnent comme des noms d\\u00e9notant des objets et comme des \\u00ab indicateurs de notions \\u00bb (de concepts) et dans cette optique, les verbes sont des noms de processus, d'actions \\u00bb (Rey, 1979) . Cette conception justifie en partie la discrimination observ\\u00e9e entre les parties du discours trait\\u00e9es dans un dictionnaire sp\\u00e9cialis\\u00e9. En g\\u00e9n\\u00e9ral, on y compte tr\\u00e8s peu de verbes et d'adjectifs, mais beaucoup d'entit\\u00e9s nominales. Les r\\u00e9sultats d'une \\u00e9tude portant sur la pr\\u00e9sence des verbes dans les dictionnaires de sp\\u00e9cialit\\u00e9 \\u00e9valuent \\u00e0 2,44% (entre 0 et 4 verbes par dictionnaire) la moyenne d'apparition des verbes dans quatre dictionnaires terminologiques (L'Homme, 2003) . L'approche conceptuelle a d\\u00e9bouch\\u00e9 de nos jours sur une d\\u00e9marche conceptuelle, incarn\\u00e9e par les ontologies, qui permet de distinguer les concepts d'activit\\u00e9, exprim\\u00e9s par les noms ou par les verbes, dans les domaines de sp\\u00e9cialit\\u00e9. Ainsi, dans le domaine m\\u00e9dical par exemple, les verbes tels que traiter, observer et activer peuvent devenir terminologiques puisqu'ils permettent de rendre compte des notions comme traitement de la maladie, observation du patient et activation des cellules (L'Homme, 2012). L'approche lexico-s\\u00e9mantique La s\\u00e9mantique lexicale est le cadre th\\u00e9orique qui a montr\\u00e9 l'importance de la structure argumentale du verbe et du r\\u00e9seau lexical auquel le verbe appartient. Dans ce cadre, la caract\\u00e9risation de la nature sp\\u00e9cialis\\u00e9e du verbe est bas\\u00e9e sur la description de sa structure argumentale ou son appartenance \\u00e0 un ou plusieurs r\\u00e9seaux lexicaux, (morpho-)s\\u00e9mantiques ou paradigmatiques. Ces t\\u00e2ches reposent sur l'observation et l'analyse des diff\\u00e9rentes occurrences du verbe en corpus. La structure argumentale L'analyse de la structure argumentale du verbe peut avoir pour but de d\\u00e9montrer sa nature terminologique. En effet, la nature pr\\u00e9dicative du verbe fait qu'il a besoin des \\u00e9l\\u00e9ments qu'il r\\u00e9git pour la r\\u00e9alisation de son sens. Une \\u00e9tude propose de prendre en consid\\u00e9ration la nature des arguments du verbe qui d\\u00e9termine son degr\\u00e9 de sp\\u00e9cialisation (L'Homme, 1998). Ce raisonnement illustre l'hypoth\\u00e8se selon laquelle le verbe n'est pas sp\\u00e9cialis\\u00e9 par lui m\\u00eame, mais gr\\u00e2ce \\u00e0 la prise en compte de sa structure argumentale (L'Homme, 2012). C'est ce crit\\u00e8re qui permet d'admettre installer comme verbe sp\\u00e9cialis\\u00e9 dans l'exemple suivant : L'utilisateur installe la nouvelle version du traitement de texte sur son PC. Dans cette phrase, les termes (utilisateur, version, PC) qui repr\\u00e9sentent les t\\u00eates des arguments du verbe appartiennent au domaine de l'informatique. Par cons\\u00e9quent, installer peut \\u00eatre consid\\u00e9r\\u00e9 comme verbe terminologique dans ce domaine. L'analyse des arguments des verbes constitue \\u00e9galement un crit\\u00e8re de poids chez (Tellier, 2008) qui y trouve un moyen de s\\u00e9lection des verbes, \\u00e0 partir d'un corpus sp\\u00e9cialis\\u00e9 relevant du domaine de l'infectiologie, repr\\u00e9sentant de bons candidats termes \\u00e0 ajouter dans un dictionnaire sp\\u00e9cialis\\u00e9. Ce crit\\u00e8re est \\u00e9galement utilis\\u00e9 dans d'autres travaux (Lerat, 2002; Pimentel, 2011) . Cependant, la caract\\u00e9risation des arguments du pr\\u00e9dicat verbal n'a pas pour unique but l'identification des verbes terminologiques. D'autres objectifs peuvent \\u00eatre poursuivis : l'extraction d'informations dans les corpus sp\\u00e9cialis\\u00e9s du domaine de la biologie mol\\u00e9culaire (Tateisi et al., 2004) , l'\\u00e9laboration d'un dictionnaire juridique portugaisanglais (Pimentel, 2011) , l'analyse contrastive des corpus m\\u00e9dicaux de niveaux de sp\\u00e9cialisation diff\\u00e9rents (expert vs profane) (Wandji Tchami et al., 2013) . Le r\\u00e9seau lexical Outre la nature des actants, d'autres param\\u00e8tres peuvent \\u00eatre pris en compte par les chercheurs lors du rep\\u00e9rage des verbes terminologiques. L'un de ces param\\u00e8tres, qui revient tr\\u00e8s souvent, est le lien qu'un verbe peut avoir avec un nom. Ainsi, si le nom est terminologique, et si le verbe est s\\u00e9mantiquement et le plus souvent morphologiquement apparent\\u00e9 \\u00e0 celui-ci, alors, il est fort possible que le verbe soit sp\\u00e9cialis\\u00e9 lui aussi (L'Homme, 2012). Ce crit\\u00e8re s'observe avec les couples tels que d\\u00e9veloppementd\\u00e9velopper, t\\u00e9l\\u00e9chargementt\\u00e9l\\u00e9charger, rechauffementrechauffer, le verbe et le nom correspondant d\\u00e9signent tous les deux une activit\\u00e9. Cependant, il existe des cas o\\u00f9 le sens du verbe et celui du nom sont distincts malgr\\u00e9 le lien morphologique qui existe entre eux. C'est le cas du couple programmeprogrammer, o\\u00f9 le nom programme d\\u00e9signe le r\\u00e9sultat de l'activit\\u00e9 que d\\u00e9note le verbe programmer. Comme nous pouvons le constater, dans l'approche lexico-s\\u00e9mantique, les noms peuvent servir de point de d\\u00e9part \\u00e0 partir duquel les verbes sp\\u00e9cialis\\u00e9s sont identifi\\u00e9s en fonction des liens qu'ils partagent avec eux. C'est d'ailleurs cette m\\u00e9thode qui permet de retenir les verbes \\u00e9voluer, excr\\u00e9ter, infecter et s\\u00e9cr\\u00e9ter comme termes du domaine de l'infectiologie, de part leur parent\\u00e9 aux noms \\u00e9volution, excr\\u00e9tion, infection et s\\u00e9cr\\u00e9tion (Tellier, 2008) . Comme ces noms sont fortement sp\\u00e9cialis\\u00e9s dans ce domaine, les verbes correspondant h\\u00e9ritent de cette caract\\u00e9ristique. Toutefois, il est possible de d\\u00e9placer le point de d\\u00e9part de l'analyse vers le verbe. Cette technique peut permettre de d\\u00e9couvrir d'autres unit\\u00e9s reli\\u00e9es au verbe et d'\\u00e9largir ainsi le r\\u00e9seau lexical construit autour de ce dernier (L'Homme, 2012). Cette d\\u00e9marche a \\u00e9t\\u00e9 appliqu\\u00e9e lors de la conception du DicoInfo (Dictionnaire fondamental de l'informatique et de l'Internet), une base de donn\\u00e9es lexicales contenant des termes (les verbes y compris) fondamentaux du domaine de l'informatique et de l'internet (L'Homme, 2009). L'approche utilis\\u00e9e s'inspire grandement des principes th\\u00e9oriques et m\\u00e9thodologiques de la Lexicologie explicative et combinatoire (Mel'cuk et al., 1995) et permet de fournir pour chaque entr\\u00e9e diff\\u00e9rents types d'informations : la r\\u00e9alisation linguistique des actants, les liens lexicaux, les synonymes, les contextes d'apparition du terme, etc. Pour le verbe programmer par exemple, DicoInfo propose divers types d'unit\\u00e9s lexicales appartenant au r\\u00e9seau lexical notamment, programmation (action de programmer), programme (r\\u00e9sultat de l'action de programmer), informaticien (agent de l'action de programmer), langage (instrument utilis\\u00e9 pour programmer), logiciel (r\\u00e9sultat l'action de programmer), \\u00e9crire (synonyme de programmer), d\\u00e9velopper (synonyme de programmer), etc. Cet exemple permet d'observer que les mots rep\\u00e9r\\u00e9s sont li\\u00e9s au verbe par diff\\u00e9rentes relations exprim\\u00e9es de fa\\u00e7on implicite \\u00e0 travers de courtes gloses explicatives. Le verbe en TAL Le traitement des verbes dans le domaine du TAL s'appuie le plus souvent sur la caract\\u00e9risation de leur structure argumentale : la valence verbale (Eynde & Mertens, 2003) , les possibilit\\u00e9s combinatoires et les relations de d\\u00e9pendances (Marneffe et al., 2006) , les fonctions grammaticales et r\\u00f4les s\\u00e9mantiques des arguments (Gildea & Jurafsky, 2002) , la d\\u00e9sambigu\\u00efstation du sens des verbes (Ide & V\\u00e9ronis, 1998; Ye & Baldwin, 2006; Wagner et al., 2009; Brown et al., 2011) , l'acquisition de sch\\u00e9mas de sous-cat\\u00e9gorisation \\u00e0 partir de l'analyse automatique de gros corpus (Messiant et al., 2010) , etc. Dans cette section, nous nous focalisons sur deux types de travaux : l'\\u00e9tiquetage des r\\u00f4les s\\u00e9mantiques (section 4.1) et la d\\u00e9sambigu\\u00efsation du sens des verbes (section 4.2). Par la suite, nous faisons la description de quelques ressources d\\u00e9di\\u00e9es au verbe (section 4.3). \\u00c9tiquetage des r\\u00f4les s\\u00e9mantiques L'\\u00e9tiquetage des r\\u00f4les s\\u00e9mantiques (Gildea & Jurafsky, 2002; Palmer et al., 2005; Swier & Stevenson, 2004; Ye & Baldwin, 2006) , ou Semantic Role Labeling (SRL), est une t\\u00e2che du TAL qui consiste \\u00e0 identifier de fa\\u00e7on automatique les relations ou les r\\u00f4les s\\u00e9mantiques (agent, patient, recipient, etc.) que jouent les constituants d'une phrase dans un cadre s\\u00e9mantique donn\\u00e9. Cette t\\u00e2che est n\\u00e9cessaire pour la conception de diff\\u00e9rents types d'applications, et plus particuli\\u00e8rement celles qui touchent la compr\\u00e9hension et l'interpr\\u00e9tation de la langue. Il s'agit par exemple de syst\\u00e8mes de questionsr\\u00e9ponses (Miller et al., 1996) , d'extraction d'informations (Surdeanu et al., 2003) , de traduction automatique (Boas, 2002) , ou de r\\u00e9sum\\u00e9 automatique (Melli et al., 2005) . Les unit\\u00e9s pr\\u00e9dicatives (verbes, noms, adjectifs) occupent g\\u00e9n\\u00e9ralement le coeur des \\u00e9tudes qui concernent la SRL. En ce qui concerne le verbe, l'annotation consiste g\\u00e9n\\u00e9ralement \\u00e0 identifier dans la phrase les limites de ses arguments et \\u00e9ventuellement des circonstants, et ensuite de leur associer des r\\u00f4les s\\u00e9mantiques selon le contexte. La d\\u00e9marche la plus utilis\\u00e9e pour la r\\u00e9alisation d'une SRL comprend trois \\u00e9tapes principales : (1) l'identification des arguments du verbe, bas\\u00e9e le plus souvent sur des heuristiques (Xue & Palmer, 2004) qui permettent de r\\u00e9duire le nombre de candidats ; (2) le calcul des probabilit\\u00e9s pour chacune des \\u00e9tiquettes \\u00e0 repr\\u00e9senter les r\\u00f4les s\\u00e9mantiques possibles ; (3) l'attribution de scores \\u00e0 chaque \\u00e9tiquette, \\u00e9ventuellement combin\\u00e9e \\u00e0 d'autres facteurs de pr\\u00e9diction, pour assigner des \\u00e9tiquettes appropri\\u00e9es aux arguments des verbes. De nos jours, les mod\\u00e8les d'apprentissage statistiques sont tr\\u00e8s sollicit\\u00e9s pour l'annotation des textes en r\\u00f4les s\\u00e9mantiques. L'un des travaux de r\\u00e9f\\u00e9rence propose un syst\\u00e8me de SRL statistique, qui peut \\u00eatre utilis\\u00e9 aussi pour l'analyse syntaxique, l'\\u00e9tiquetage des parties du discours (Church, 1988) , et la d\\u00e9sambigu\\u00efsation du sens des mots (Lapata & Brew, 2004) . Ce syst\\u00e8me, con\\u00e7u pour les verbes, les noms et les adjectifs, atteint 82% de pr\\u00e9cision sur des phrases pr\\u00e9-annot\\u00e9es manuellement, tandis qu'il montre 65% de pr\\u00e9cision et 61% de rappel sur des phrases non annot\\u00e9s (Gildea & Jurafsky, 2002) . Il a d'ailleurs \\u00e9t\\u00e9 utilis\\u00e9e dans le cadre du projet FrameNet. D\\u00e9sambigu\\u00efsation du sens des verbes La d\\u00e9sambigu\\u00efsation du sens des verbes ou Verb Sense Disambiguation (VSD) est une sous-t\\u00e2che de la WSD (word sense disambiguation). Elle consiste \\u00e0 s\\u00e9lectionner automatiquement, parmi ses diff\\u00e9rents sens, le sens le plus appropri\\u00e9 d'un verbe polys\\u00e9mique, selon son contexte d'apparition. Par exemple, le verbe read (lire) a plusieurs sens. Pour faire la distinction entre les phrases telles que I read a book (je lis un livre) et I read you loud and clear (je te comprends parfaitement), il est n\\u00e9cessaire de d\\u00e9sambigu\\u00efser le contexte d'apparition du verbe, en suivant une des m\\u00e9thodes existantes. La d\\u00e9sambigu\\u00efsation du sens est une t\\u00e2che n\\u00e9cessaire pour la traduction automatique (Carpuat & Wu, 2007) ou l'extraction d'informations (Sch\\u00fctze & Pedersen, 1995; Sanderson, 1994) . Deux types d'approches sont utilis\\u00e9es habituellement pour la d\\u00e9sambigu\\u00efsation du sens des mots : approche \\u00e0 base de r\\u00e8gles et approche \\u00e0 base d'apprentissage. L'approche \\u00e0 base de r\\u00e8gles requiert des ressources comme les bases de donn\\u00e9es lexicales, les dictionnaires \\u00e9lectroniques, qui fournissent des descriptions lexicales, syntaxiques et s\\u00e9mantiques des mots. \\u00c0 partir de ces ressources, des r\\u00e8gles sont d\\u00e9finies pour d\\u00e9terminer le sens exact du mot parmi l'ensemble des sens possibles. En traduction automatique, un ensemble constitu\\u00e9 de 63 r\\u00e8gles est propos\\u00e9 comme source de connaissances (Specia et al., 2005) . L'approche la plus utilis\\u00e9e actuellement est DESCRIPTION DU VERBE DANS LES TRAVAUX DE LINGUISTIQUE, TERMINOLOGIE ET TAL bas\\u00e9e sur l'apprentissage automatique (Ye & Baldwin, 2006; Brown et al., 2011; Yarowsky, 1995) . L'apprentissage peut \\u00eatre supervis\\u00e9 (exigeant un ensemble d'exemples manuellement annot\\u00e9s) ou non supervis\\u00e9 (appliqu\\u00e9 sur des textes non annot\\u00e9s). En effet, certains chercheurs proposent une technique non supervis\\u00e9e de d\\u00e9sambigu\\u00efsation du sens des verbes, qui regroupent les verbes ayant les pr\\u00e9f\\u00e9rences s\\u00e9lectionnelles et de sous-cat\\u00e9gorisation similaires (Wagner et al., 2009) . Cette m\\u00e9thode montre 57.06% de pr\\u00e9cision. D'autres travaux identifient les pr\\u00e9f\\u00e9rences s\\u00e9mantiques (Lapata & Brew, 2004) et les marques de sous-cat\\u00e9gorisation (Lapata & Brew, 1999) des verbes apparaissant dans plusieurs classes de Levin. En ce qui concerne les m\\u00e9thodes supervis\\u00e9es, les premi\\u00e8res exp\\u00e9riences se focalisaient sur les bi-grammes et les fonctions linguistiques et contextuelles (Pedersen, 2000 (Pedersen, , 2001;; Hoa Trang & Palmer, 2002) . Par la suite, les chercheurs se sont int\\u00e9ress\\u00e9s \\u00e0 l'apport des bases de connaissances fournissant les informations telles que la cat\\u00e9gorie grammaticale des mots voisins, la forme morphologique, les collocations, la relation syntaxique verbe-objet, utiles pour lever certaines ambigu\\u00eft\\u00e9s (Yoong Keok & Hwee Tou, 2002) . De plus en plus, les chercheurs abordent les r\\u00f4les s\\u00e9mantiques des arguments des verbes comme des fonctions contribuant \\u00e0 l'am\\u00e9lioration des performances des syst\\u00e8mes lors de la d\\u00e9sambigu\\u00efsation du sens des verbes (Hoa Trang & Palmer, 2005; Ye & Baldwin, 2006) . Cette technique est d'ailleurs recommand\\u00e9e car les r\\u00f4les s\\u00e9mantiques associ\\u00e9s \\u00e0 un mot peuvent donner des indices pour la d\\u00e9duction de son sens, surtout lorsque ces r\\u00f4les sont associ\\u00e9s \\u00e0 des frames de sous-cat\\u00e9gorisation syntaxique (Gildea & Jurafsky, 2002) . Certains travaux suivent une approche supervis\\u00e9e bas\\u00e9e sur connaissances extraites des ressources lexicales externes (VerbNet, WordNet, etc.) (knowledge based WSD) (Brown et al., 2011) . Une autre approche, inspir\\u00e9e par les travaux en psycholinguistique, propose de nouveaux crit\\u00e8res de regroupement des sens d'un mot, en fonction de la diff\\u00e9rence faible ou importante qui existe entre ces mots (Brown, 2008) . Pour le fran\\u00e7ais, il existe une approche d'analyse s\\u00e9mantique des textes, bas\\u00e9e sur des r\\u00e9seaux lexicaux et les relations de d\\u00e9pendance entre les mots ambigus et les autres mots de la phrase (Mouton, 2010) . La r\\u00e9alisation de la WSD sur des textes sp\\u00e9cialis\\u00e9s est actuellement une t\\u00e2che relativement difficile selon les domaines, \\u00e0 cause de l'absence des ressources terminologiques n\\u00e9c\\u00e9ssaires ou de l'insuffisance des donn\\u00e9es disponibles. N\\u00e9anmoins, dans certains domaines comme l'informatique biom\\u00e9dicale, diff\\u00e9rentes \\u00e9tudes proposent des syst\\u00e8mes de WSD bas\\u00e9s sur des m\\u00e9thodes non supervis\\u00e9es (Liu et al., 2001) ou supervis\\u00e9es (Stevenson & Guo, 2010) , utilisant des terminologies existantes. Quelques ressources lexicales d\\u00e9di\\u00e9es au verbe Dans la suite de cette section, nous d\\u00e9crivons bri\\u00e8vement quelques ressources lexicales : FrameNet (section 4.3.1), Verb-Net (section 4.3.2), VerbOcean (section 4.3.3) et WordNet (section 4.3.4). Nous nous int\\u00e9ressons particuli\\u00e8rement \\u00e0 la mani\\u00e8re dont l'information sur le verbe est pr\\u00e9sent\\u00e9e. FrameNet FrameNet (Ruppenhofer et al., 2006) est une base de donn\\u00e9es lexicales 2 initialement con\\u00e7ue pour l'anglais. Elle contient plus de 10 000 sens des unit\\u00e9s lexicales d\\u00e9crits \\u00e0 travers plus de 1 000 cadres s\\u00e9mantiques li\\u00e9s hi\\u00e9rarchiquement les uns aux autres et illustr\\u00e9s par plus de 170 000 phrases. Le projet FrameNet propose une description des unit\\u00e9s lexicales pr\\u00e9dicatives (verbes, noms et adjectifs), bas\\u00e9e sur l'annotation en cadres s\\u00e9mantiques (Fillmore, 1982) des phrases dans lesquelles ces unit\\u00e9s apparaissent. His $20 TRANSACTION with Amazon.com for a new TV had been very smooth. Dans cette phrase, chaque couleur repr\\u00e9sente un \\u00e9l\\u00e9ment du cadre : bleu fonc\\u00e9=ACHETEUR, bleu ciel=ARGENT, rouge=VENDEUR, vert=BIEN. Ces frames mettent en \\u00e9vidence des informations s\\u00e9mantiques n\\u00e9cessaires pour capturer les sens de l'unit\\u00e9 lexicale cl\\u00e9. Ainsi, pour chacune de ses entr\\u00e9es, FrameNet est capable de fournir un cadre s\\u00e9mantique complet, une description du frame, ses \\u00e9ventuelles relations avec d'autres frames, une description des \\u00e9l\\u00e9ments du frame et une illustration des sch\\u00e9mas valenciels de l'entr\\u00e9e \\u00e0 l'aide d'exemples (Ruppenhofer et al., 2006) . 2. https ://framenet.icsi.berkeley.edu/fndrupal/about ORNELLA WANDJI VerbNet Contrairement \\u00e0 FrameNet, VerbNet 3 (Kipper et al., 2000; Kipper-Schuler, 2005) est totallement focalis\\u00e9 sur les verbes. Cette ressource lexicale propose une description des verbes bas\\u00e9e sur la classification de Levin (section 2.3.1). Elle consiste \\u00e0 regrouper les verbes en diff\\u00e9rentes classes, qui mettent en \\u00e9vidence leurs propri\\u00e9t\\u00e9s syntaxiques et s\\u00e9mantiques communes. Cette m\\u00e9thode de description permet de faire des g\\u00e9n\\u00e9ralisations sur le comportement des verbes. Par exemple, les verbes appartenant \\u00e0 la classe Hit 18.1 : bang, bash, hit, kick... sont des transitifs direct. Ils exigent un agent et un patient, et peuvent \\u00eatre modifi\\u00e9s par des pr\\u00e9dicats s\\u00e9mantiques exprimant la mani\\u00e8re, la cause, la direction, etc. VerbNet est donc un lexique hi\\u00e9rarchique de verbes anglais regroup\\u00e9s en classes, ind\\u00e9pendamment des domaines de sp\\u00e9cialit\\u00e9s auxquels ils peuvent appartenir. Chaque classe est d\\u00e9crite \\u00e0 travers : l'ensemble d'arguments possibles, pr\\u00e9sent\\u00e9s sous forme de r\\u00f4les th\\u00e9matiques ; les \\u00e9ventuelles restrictions de s\\u00e9lection d'arguments (comme anim\\u00e9, humain, organisation) ; les cadres, d\\u00e9crivant les possibles r\\u00e9alisations de surface de la structure argumentale (constructions transitives, intransitives, syntagmes pr\\u00e9positionnels, r\\u00e9sultatives) ; les alternances de diath\\u00e8se, c'est-\\u00e0-dire les variations des diff\\u00e9rents cadres. Selon le site officiel, apr\\u00e8s son extension (Korhonen & Briscoe, 2004) , VerbNet compte 274 classes de premier niveau, 23 r\\u00f4les th\\u00e9matiques, 94 pr\\u00e9dicats s\\u00e9mantiques, 55 restrictions syntaxiques, 5 257 sens des verbes et 3 769 lemmes. VerbOcean VerbOcean (Chklovski & Pantel, 2004) est une ressource lexicale qui propose un r\\u00e9seau s\\u00e9mantique de relations entre les verbes, et recense uniquement des paires de verbes s\\u00e9mantiquement proches. Elle contient 22 306 relations entre 3 477 verbes et identifie 5 types de relations : similitude (la similitude), strenght (la force), antonymy (l'antonymie), enablement (l'habilitation), et la relation temporelle happens-before (a lieu avant). L'approche appliqu\\u00e9e pour la conception de cet outil est bas\\u00e9e sur deux \\u00e9tapes : (1) la d\\u00e9tection des paires de verbes qui apparaissent en co-ooccurrence fr\\u00e9quente, gr\\u00e2ce \\u00e0 des requ\\u00eates effectu\\u00e9es sur le portail Google ; (2) pour chaque paire, le calcul du score de chaque relation possible, gr\\u00e2ce \\u00e0 35 sch\\u00e9mas lexico-syntaxiques. Par exemple, les verbes discover (d\\u00e9couvrir) et refine (affiner, am\\u00e9liorer) sont consid\\u00e9r\\u00e9s comme une paire illustrant la relation happens-before si la cha\\u00eene discovered and refined (instantiant le sch\\u00e9ma Xed and then Yed) est identifi\\u00e9e de fa\\u00e7on tr\\u00e8s fr\\u00e9quente sur Google. WordNet WordNet (Fellbaum, 1998) est une base de donn\\u00e9es lexicale qui propose une description des verbes, mais \\u00e9galement des noms et des adjectifs, sur la base de diff\\u00e9rentes relations s\\u00e9mantiques : la synonymie, l'antonymie, l'hyperonymie, l'hyponymie, la m\\u00e9ronymie, la troponymy et l'implication (Miller, 1995) . Contrairement \\u00e0 VerbOcean qui s'int\\u00e9resse uniquement aux paires de verbes s\\u00e9mantiquement proches, WordNet traite plusieurs cat\\u00e9gories d'unit\\u00e9s pr\\u00e9dicatives et ces unit\\u00e9s sont regroup\\u00e9es dans des synsets, 117 000 au total. Un synset est un groupe de mots (synonymes) s\\u00e9mantiquement homog\\u00e8nes. Il contient des pointeurs qui marquent ses relations conceptuelles avec d'autres synsets. En outre, un synset contient une br\\u00e8ve d\\u00e9finition et, dans la plupart des cas, une ou plusieurs courtes phrases illustrant l'utilisation des membres de ce synset. Les formes des mots ayant plusieurs significations sont repr\\u00e9sent\\u00e9es par autant de synsets distincts. Discussion et travaux futurs Comme indiqu\\u00e9 plus haut, le projet que nous entreprenons a pour objectif de proposer une m\\u00e9thode de simplification de textes m\\u00e9dicaux \\u00e9crits en fran\\u00e7ais, \\u00e0 partir d'une analyse syntaxico-s\\u00e9mantique des verbes en contexte. Nous avons vu dans la section 3 que les travaux sur les langues de sp\\u00e9cialit\\u00e9 sont le plus souvent focalis\\u00e9s sur les entit\\u00e9s nominales et, par cons\\u00e9quent, les travaux sur les verbes terminologiques sont peu nombreux. De m\\u00eame, dans la section 4, nous d\\u00e9montrons que peu de travaux en TAL appliquent la s\\u00e9mantique des cadres \\u00e0 des textes sp\\u00e9cialis\\u00e9s et qu'il existe encore des cadres th\\u00e9oriques dans lesquels le verbe et sa structure argumentale sont peu consid\\u00e9r\\u00e9s. En rupture avec ces constats, nous proposons d'exploiter l'\\u00e9tude de la structure argumentale des verbes pour la simplification des textes sp\\u00e9cialis\\u00e9s. Nous partons de l'hypoth\\u00e8se selon laquelle le verbe, en tant que pr\\u00e9dicat central dans la phrase, peut \\u00eatre le point de d\\u00e9part pour cerner la syntaxe et la s\\u00e9mantique des textes sp\\u00e9cialis\\u00e9s puisqu'il sert \\u00e0 articuler l'expertise et les connaissances port\\u00e9es 3. Simplification. Dans le cas des textes sp\\u00e9cialis\\u00e9s, r\\u00e9dig\\u00e9s pour un public de non experts, les emplois sp\\u00e9cialis\\u00e9s des verbes peuvent \\u00eatre consid\\u00e9r\\u00e9s comme des sources de difficult\\u00e9. Gr\\u00e2ce \\u00e0 l'\\u00e9tape pr\\u00e9c\\u00e9dente, de tels emplois sp\\u00e9cialis\\u00e9s peuvent \\u00eatre d\\u00e9tect\\u00e9s automatiquement. La simplification a pour objectif de rendre ces emplois de verbes plus abordables pour les utilisateurs non sp\\u00e9cialistes. A ce niveau, l'absence de ressources du type WordNet pour les langues de sp\\u00e9cialit\\u00e9 repr\\u00e9sente une difficult\\u00e9 cruciale que nous allons devoir affronter. Dans un premier temps, pour pallier \\u00e0 ce probl\\u00e8me, les phrases simplifi\\u00e9es seront con\\u00e7ues sur un mod\\u00e8le que proposent les d\\u00e9finitions des termes du DicoInfo (L'Homme, 2009). Il s'agira de fournir une d\\u00e9finition typique de la construction verbale ambig\\u00fce dans laquelle entre le verbe. Cette d\\u00e9finition sera enrichie par un ou plusieurs synonymes du verbe qui seront recherch\\u00e9s dans WordNet ou d'autres ressources qui proposent les synonymes des mots de la langue g\\u00e9n\\u00e9rale. Une \\u00e9tude comparative des corpus de textes sp\\u00e9cialis\\u00e9s, \\u00e9crits par des experts et ceux \\u00e9crits par des non-experts, effectu\\u00e9e au pr\\u00e9alable, sera utile lors de la simplification, pour l'identification, si possible, des constructions verbales synonymes. La simplification concernera \\u00e9galement les constituants syntaxiques de la phrase et \\u00e9ventuellement des temps verbaux (Brouwers et al., 2014) . En exploitant la m\\u00e9thode appliqu\\u00e9e dans FrameNet et gr\\u00e2ce aux observations en corpus, nous pouvons d\\u00e9tecter les arguments n\\u00e9cessaires (core) et non n\\u00e9cessaires (non core) et all\\u00e9ger les phrases en supprimant les \\u00e9l\\u00e9ments non n\\u00e9cessaires. De la m\\u00eame mani\\u00e8re, si les \\u00e9l\\u00e9ments n\\u00e9cessaires \\u00e0 la compr\\u00e9hension sont absents, nous pouvons les d\\u00e9duire et compl\\u00e9ter ainsi la structure argumentale de verbes, en esp\\u00e9rant que cela facilite la compr\\u00e9hension des phrases. Au terme de ces diff\\u00e9rentes \\u00e9tapes, nous pensons pouvoir am\\u00e9liorer la lisibilit\\u00e9 du texte et de rendre le sens des verbes plus accessibles aux utilisateurs non sp\\u00e9cialistes en m\\u00e9decine. Les r\\u00e9sultats de notre approche lexico-syntaxique seront \\u00e9valu\\u00e9s et compar\\u00e9s \\u00e0 ceux des m\\u00e9thodes de simplification focalis\\u00e9es uniquement sur les entit\\u00e9s nominales, c'est-\\u00e0-dire sur les arguments des verbes. Conclusion Tout au long de ce travail, nous avons explor\\u00e9 les principaux cadres th\\u00e9oriques et travaux qui s'int\\u00e9ressent particuli\\u00e8rement au pr\\u00e9dicat verbal dans trois domaines de recherche : terminologie, o\\u00f9 le verbe a tard\\u00e9 \\u00e0 s'imposer comme unit\\u00e9 pouvant exprimer des connaissances sp\\u00e9cialis\\u00e9es, face \\u00e0 la place dominante des entit\\u00e9s nominales ; linguistique, o\\u00f9 le verbe a toujours fait partie des cat\\u00e9gories grammaticales les plus \\u00e9tudi\\u00e9es ; TAL, o\\u00f9 de nos jours, de nombreuses ressources\",\n          \"Le travail pr\\u00e9sent\\u00e9 dans cet article se situe dans le cadre de l'identification de termes sp\\u00e9cialis\\u00e9s (unit\\u00e9s de mesure) \\u00e0 partir de donn\\u00e9es textuelles pour enrichir une Ressource Termino-Ontologique (RTO). La premi\\u00e8re \\u00e9tape de notre m\\u00e9thode consiste \\u00e0 pr\\u00e9dire la localisation des variants d'unit\\u00e9s de mesure dans les documents. Nous avons utilis\\u00e9 une m\\u00e9thode reposant sur l'apprentissage supervis\\u00e9. Cette m\\u00e9thode permet de r\\u00e9duire sensiblement l'espace de recherche des variants tout en restant dans un contexte optimal de recherche (r\\u00e9duction de 86% de l'espace de recherch\\u00e9 sur le corpus \\u00e9tudi\\u00e9). La deuxi\\u00e8me \\u00e9tape du processus, une fois l'espace de recherche r\\u00e9duit aux variants d'unit\\u00e9s, utilise une nouvelle mesure de similarit\\u00e9 permettant d'identifier automatiquement les variants d\\u00e9couverts par rapport \\u00e0 un terme d'unit\\u00e9 d\\u00e9j\\u00e0 r\\u00e9f\\u00e9renc\\u00e9 dans la RTO avec un taux de pr\\u00e9cision de 82% pour un seuil au dessus de 0.6 sur le corpus \\u00e9tudi\\u00e9. Introduction Le travail pr\\u00e9sent\\u00e9 dans cet article se situe dans le cadre de l'identification de termes sp\\u00e9cialis\\u00e9s \\u00e0 partir de donn\\u00e9es textuelles pour enrichir une Ressource Termino-Ontologique (RTO). Les travaux de (McCrae et al., 2011; Cimiano et al., 2011) proposent d'associer une partie terminologique et/ou linguistique aux ontologies afin d'\\u00e9tablir une distinction claire entre la manifestation linguistique (le terme) et la notion qu'elle d\\u00e9note (le concept). Nous nous int\\u00e9ressons \\u00e0 l'enrichissement d'une RTO permettant de mod\\u00e9liser des relations n-aires entre des donn\\u00e9es quantitatives exp\\u00e9rimentales (Touhami et al., 2011) , o\\u00f9 les arguments peuvent \\u00eatre des concepts symboliques ou des quantit\\u00e9s caract\\u00e9ris\\u00e9es par des unit\\u00e9s de mesure. En effet, l'extraction des donn\\u00e9es quantitatives est un enjeu majeur pour de nombreux domaines scientifiques dont l'objectif concerne la capitalisation et la p\\u00e9rennisation des connaissances du domaine. Cependant, la forte variation d'\\u00e9criture des unit\\u00e9s de mesure dans les documents engendre des probl\\u00e8mes d'identification des instances num\\u00e9riques dans les textes. Dans une d\\u00e9marche consensuelle, le Systeme International (SI) (Thompson & Taylor, 2008) organise, en posant plusieurs d\\u00e9finitions formelles, le syst\\u00e8me des quantit\\u00e9s et des unit\\u00e9s de mesure. Il d\\u00e9finit ainsi des unit\\u00e9s de base, i.e. unit\\u00e9s simples comme kilogram, et des unit\\u00e9s d\\u00e9riv\\u00e9es, i.e. unit\\u00e9s plus complexes comme kg.m \\u22121 . Ce standard pose les r\\u00e8gles d'\\u00e9criture de l'ensemble des unit\\u00e9s de mesure mais n'int\\u00e8gre pas la notion de variants d'unit\\u00e9s. Ces principes sont repris dans des travaux r\\u00e9cents (Rijgersberg et al., 2013) afin de mod\\u00e9liser formellement cette connaissance dans une ontologie d\\u00e9di\\u00e9e \\u00e0 la repr\\u00e9sentation des donn\\u00e9es quantitatives et des unit\\u00e9s de mesure. Les auteurs ont ainsi mod\\u00e9lis\\u00e9 OM (Ontology of Units of Measure and Related Concepts). Les travaux de (Van Assem et al., 2010) posent la probl\\u00e9matique d'identification des donn\\u00e9es quantitatives pr\\u00e9sentes dans les cellules des tableaux repr\\u00e9sent\\u00e9s dans les documents. La localisation des variants d'unit\\u00e9s n'est pas probl\\u00e9matique dans ces travaux car la m\\u00e9thode repose sur le format structur\\u00e9 des tableaux. Les travaux de (Grau et al., 2009) proposent des m\\u00e9thodes d'extraction des donn\\u00e9es exp\\u00e9rimentales dans le domaine biom\\u00e9dical. L'identification des unit\\u00e9s de mesure repose sur les unit\\u00e9s r\\u00e9f\\u00e9renc\\u00e9es dans le Systeme International (Thompson & Taylor, 2008) , la probl\\u00e9matique de l'identification des variants d'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e n'y est pas abord\\u00e9e. Ainsi, \\u00e0 notre connaissance, les m\\u00e9thodes de l'\\u00e9tat de l'art partageant l'objectif d'extraction de donn\\u00e9es quantitatives, ne permettent pas de r\\u00e9soudre la probl\\u00e9matique d'extraction et d'identification des variants d'unit\\u00e9s de mesure dispers\\u00e9s dans les documents scientifiques au format textuel non structur\\u00e9. Dans cet article, nous pr\\u00e9sentons notre proposition qui tente de r\\u00e9pondre \\u00e0 deux questions concernant l'identification des variants d'unit\\u00e9s de mesure dans les documents textuels non structur\\u00e9s : -La question concernant la localisation des variants dans le document. Sachant que nous travaillons sur l'int\\u00e9gralit\\u00e9 des documents, nous pr\\u00e9f\\u00e9rons l'apprentissage afin de pr\\u00e9dire la localisation des variants sans poser d'hypoth\\u00e8ses pr\\u00e9alables. -La question de l'identification du variant une fois qu'il est localis\\u00e9. A quel autre terme d'unit\\u00e9 de mesure r\\u00e9f\\u00e9renc\\u00e9e dans la RTO peut-on le rapprocher, en sachant que les termes d'unit\\u00e9s r\\u00e9pondent \\u00e0 leurs propres r\\u00e8gles syntaxiques ? Les m\\u00e9thodes existantes doivent \\u00eatre adapt\\u00e9es \\u00e0 ces nouvelles r\\u00e8gles. Les deux questions de recherche pr\\u00e9c\\u00e9dentes sont respectivement trait\\u00e9es en sections 2 et 3. Ces propositions sont alors exp\\u00e9riment\\u00e9es en section 4, avant la conclusion et les perspectives d\\u00e9crites en section 5. Dans notre approche, il est fondamental de pouvoir prendre en consid\\u00e9ration les particularit\\u00e9s d'\\u00e9criture des unit\\u00e9s de mesure, en notant que chaque bloc est ind\\u00e9pendant dans l'\\u00e9criture de l'unit\\u00e9. De ce fait, l'ordre des blocs n'est pas important \\u00e0 prendre en compte, en revanche, la comparaison des blocs entre eux nous semble plus pertinente et plus adapt\\u00e9e dans le calcul de la similarit\\u00e9. Il est alors int\\u00e9ressant de proposer une mesure qui calcule la similarit\\u00e9 en deux temps, qui s'appuie \\u00e0 la fois sur les unit\\u00e9s d\\u00e9j\\u00e0 r\\u00e9f\\u00e9renc\\u00e9es dans la RTO et sur les caract\\u00e8res sp\\u00e9cifiques (/, (, ), ., \\u00d7, \\u02c6...) utilis\\u00e9s comme s\\u00e9parateurs de blocs. Dans un premier temps, les candidats sont pr\\u00e9sectionn\\u00e9s selon la mesure de Jaccard. Le principe ci-dessous est alors mis en oeuvre : -Soit un couple compos\\u00e9 du variant candidat u i et d'une unit\\u00e9 r\\u00e9f\\u00e9rence dans la RTO u j . J(u i , u j ) (cf. formule (1)) calcule dans un premier temps le score de similarit\\u00e9 entre l'ensemble u i et l'ensemble u j par rapport aux blocs communs sans tenir compte de leur ordre. J(u i , u j ) = |u i \\u2229 u j | |u i \\u222a u j | (1) -On s\\u00e9lectionne le couple (u i , u j ) comme \\u00e9tant pertinent \\u00e0 \\u00eatre compar\\u00e9 si J(u i , u j ) > K , K \\u00e9tant le seuil minimal d\\u00e9fini pr\\u00e9alablement par l'utilisateur. Prenons l'exemple du couple compos\\u00e9 d'un variant candidat localis\\u00e9 et extrait \\u00e0 partir d'un document kg P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 et son r\\u00e9f\\u00e9rent dans la RTO lb.m.m \\u22122 .s \\u22121 .P a \\u22121 . Dans ce contexte, le calcul de la mesure de Jacccard donne le r\\u00e9sultat suivant : J(kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 ) = 4 6 = 0.7 Dans un deuxi\\u00e8me temps, apr\\u00e8s cette phase de pr\\u00e9s\\u00e9lection, les candidats sont s\\u00e9lectionn\\u00e9s selon une mesure \\u00e9tendue de Damereau-Levenshtein. La mesure de similarit\\u00e9 de Levenshtein (Levenshtein, 1966) calcule le co \\u00fbt minimal pour transformer une premi\\u00e8re cha \\u00eene de caract\\u00e8res en une deuxi\\u00e8me cha \\u00eene de caract\\u00e8res en consid\\u00e9rant les op\\u00e9rations de remplacement de caract\\u00e8res entre les deux cha \\u00eenes, d'ajout d'un caract\\u00e8re ou et de suppression d'un caract\\u00e8re. Le co \\u00fbt est ensuite normalis\\u00e9 pour obtenir une valeur de la distance entre les deux cha \\u00eenes entre 0 et 1. Cette mesure est \\u00e9tendue par Damerau (Damerau, 1964) qui inclut dans celle de Levenshtein la notion de transposition de caract\\u00e8res d'une cha \\u00eene \\u00e0 une autre, i.e. dans litre et liter, il y a transposition entre les caract\\u00e8res \\\"e\\\" et \\\"r\\\". La mesure adapt\\u00e9e \\u00e0 notre contexte (cf. formule (2)) ne consid\\u00e8re plus la comparaison des caract\\u00e8res mais des blocs de caract\\u00e8res, correspondant \\u00e0 des unit\\u00e9s simples. Le variant candidat et l'unit\\u00e9 de r\\u00e9f\\u00e9rence, composant le couple pr\\u00e9s\\u00e9lectionn\\u00e9 lors de la premi\\u00e8re phase, sont, dans cette seconde phase, compar\\u00e9s bloc \\u00e0 bloc pour d\\u00e9terminer leur similarit\\u00e9 finale. SM D b (u i , u j ) = max[0; min(|u i |, |u j |) \\u2212 D b (u i , u j ) min(|u i |, |u j |) ]; SM D b (u i , u j ) \\u2208 [0; 1] (2) -(u i , u j ) repr\\u00e9sente le couple s\\u00e9lectionn\\u00e9 \\u00e0 partir de la mesure de Jaccard ; -Chaque bloc de u i est compar\\u00e9 aux blocs de u j pour calculer la nouvelle distance D b ; u i est valid\\u00e9e comme un variant de l'unit\\u00e9 u j si SM D b > K, avec K un seuil de similarit\\u00e9 d\\u00e9fini pr\\u00e9alablement. En posant K = 0.5, le couple kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 , SM D b calcule la similarit\\u00e9 du couple en comparant chaque bloc dans les unit\\u00e9s : SM D b (kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 ) = max[0; 5 \\u2212 1 5 ] = 0.8 Un tel processus fond\\u00e9 sur ces deux phases cons\\u00e9cutives de pr\\u00e9s\\u00e9lection et de s\\u00e9lection finale permet la d\\u00e9couverte de nouveaux variants \\u00e0 int\\u00e9grer comme d\\u00e9taill\\u00e9 dans la section suivante. Exp\\u00e9rimentations Les exp\\u00e9rimentations ont \\u00e9t\\u00e9 men\\u00e9es \\u00e0 partir d'un corpus de 115 articles scientifiques en anglais issus du domaine des emballages alimentaires. Elles s'appuient \\u00e9galement sur une liste de 211 termes d\\u00e9notant les diff\\u00e9rents concepts d'unit\\u00e9s de mesure pour le domaine des emballages alimentaires. Ces diff\\u00e9rentes fen \\u00eatres correspondent \\u00e0 des sous-ensembles du corpus repr\\u00e9sentant 5000 phrases (e.g. f 0 ) \\u00e0 15000 phrases (e.g. f +2 ). Le corpus complet comportant plus de 35000 phrases. Le sac de mots repr\\u00e9sente un ensemble de 3000 \\u00e0 4800 descripteurs selon les diff\\u00e9rentes repr\\u00e9sentations. \\u00c9valuation de la m\\u00e9thode de localisation des unit\\u00e9s de mesure Notre objectif, au cours de cette premi\\u00e8re \\u00e9tape (cf. section 2), est de produire un mod\\u00e8le d'apprentissage appris \\u00e0 partir des donn\\u00e9es repr\\u00e9sent\\u00e9es sous forme de fen \\u00eatres textuelles, qui permette de r\\u00e9duire l'espace de recherche des variants d'unit\\u00e9s. Nous avons test\\u00e9 plusieurs fen \\u00eatres textuelles. Nous restituons en r\\u00e9sultats des exp\\u00e9rimentations uniquement ceux r\\u00e9v\\u00e9lant les fen \\u00eatres d'\\u00e9tude les plus pertinentes dans le tableau 1. Par souci de lisibilit\\u00e9, les fen \\u00eatres textuelles sont exprim\\u00e9es de la mani\\u00e8re suivante : f 0 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9. f +2 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9 ainsi que les deux phrases suivantes. f \\u22122 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9 ainsi que les deux phrases pr\\u00e9c\\u00e9dentes. Les tableaux 1 et 2 restituent les r\\u00e9sultats obtenus sur le corpus des emballages r\\u00e9alis\\u00e9 avec quatre algorithmes d'apprentissage (Naives Bayes, C4.5, DMNB (Discriminative Multinominal Naive Bayes), SMO (Sequential minimal optimization qui est une variante de SVM)) et une 10-validation crois\\u00e9e. Le tableaun1 restitue les r\\u00e9sultats, toutes mesures confondues. L'analyse des r\\u00e9sultats montrent que Naives Bayes produit une F-mesure allant de 0.85 \\u00e0 0.88, l'arbre de d\\u00e9cision \\u00e9tabli sur l'algorithme C4.5 (J48) produit de meilleurs r\\u00e9sultats autour de 0.93 \\u00e0 0.96. DMNB et SMO produisent les meilleurs r\\u00e9sultats, conform\\u00e9ment \\u00e0 ce qui est soulign\\u00e9 dans la litt\\u00e9rature du domaine (0.95 \\u00e0 0.99). Outre ces r\\u00e9sultats analytiques, nous remarquons qu'un plus large contexte, \\u00e0 partir des fen \\u00eatres f +2 et f \\u22122 , n'am\\u00e9liorent pas les r\\u00e9sultats d'apprentissage. Nous pouvons donc en d\\u00e9duire que la plus petite fen \\u00eatre textuelle, c'est-\\u00e0-dire celle o\\u00f9 au moins un terme d'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9 dans la RTO apparait, est le contexte le plus favorable \\u00e0 la d\\u00e9couverte de variants d'unit\\u00e9s. Cette conclusion permet de r\\u00e9duire sensiblement l'espace de recherche des variants, i.e. 5000 phrases \\u00e0 consid\\u00e9rer plut \\u00f4t que 35000 initialement d\\u00e9nombr\\u00e9es, tout en restant dans un contexte optimal de recherche (r\\u00e9duction de 86% de l'espace de recherche). Le tableau 2 synth\\u00e9tise les r\\u00e9sultats selon les diff\\u00e9rentes mesures de pond\\u00e9ration et la matrice bool\\u00e9enne pour la fen \\u00eatre optimale f 0 . Notre objectif \\u00e9tant d'\\u00e9valuer quel algorithme produit le mod\\u00e8le restituant des valeurs de F-mesure stables sur les diff\\u00e9rentes mesures de pond\\u00e9ration et la matrice bool\\u00e9enne. La F-mesure, ainsi que les valeurs de pr\\u00e9cision et de rappel restent stables et \\u00e9lev\\u00e9es avec le mod\\u00e8le DMNB, en restituant une valeur constante autour de 0.95. \\u00c9valuation de la m\\u00e9thode d'identification des unit\\u00e9s de mesure Dans la deuxi\\u00e8me \\u00e9tape de notre processus (cf. section 3), nous nous appuyons sur les r\\u00e9sultats obtenus pr\\u00e9c\\u00e9demment afin d'identifier les variants d'unit\\u00e9s. Notons que dans notre contexte, nos mesures doivent s\\u00e9lectionner les variants les plus pertinents \\u00e0 pr\\u00e9senter aux experts, ce qui revient \\u00e0 minimiser le bruit. Ainsi, nous privil\\u00e9gions la mesure de pr\\u00e9cision pour \\u00e9valuer nos propositions. Le tableau 3 restitue les r\\u00e9sultats obtenus avec la nouvelle mesure et permet de les comparer par seuil de similarit\\u00e9 : 1. La pr\\u00e9cision est globalement plus \\u00e9lev\\u00e9e avec le processus complet comparativement \\u00e0 l'application de la seule mesure de pr\\u00e9s\\u00e9lection (Jaccard), ceci confirme donc l'int\\u00e9r \\u00eat d'utiliser notre mesure SM D b . 2. Les seuils les plus int\\u00e9ressants \\u00e0 exploiter sont au-dessus de 0.6 avec un taux de pr\\u00e9cision sup\\u00e9rieur \\u00e0 82% apr\\u00e8s application des deux \\u00e9tapes successives ; l'essentiel des variants sont identifi\\u00e9s. 3. En dessous de 0.5, les r\\u00e9sultats se tassent largement. En choisissant de ne consid\\u00e9rer que les seuils au-del\\u00e0 de 0.5, nous cr\\u00e9ons forc\\u00e9ment du silence mais un silence \\\"contr \\u00f4l\\u00e9\\\". En effet, le processus d'extraction et d'identification des variants \\u00e9tant un processus it\\u00e9ratif, les nouvelles unit\\u00e9s int\\u00e9gr\\u00e9es dans la RTO favorisent la d\\u00e9couverte d'autres variants qui s'expriment dans cette plage de silence. Le tableau 3 montre la validation des couples variants et unit\\u00e9s de r\\u00e9f\\u00e9rence pertinents (avec K et K ayant les valeurs 0.5). Un m \\u00eame variant peut former un couple avec plusieurs unit\\u00e9s de r\\u00e9f\\u00e9rence dans la RTO. En effet, prenons l'exemple du variant amol / m s Pa, sa comparaison avec les unit\\u00e9s de r\\u00e9f\\u00e9rence amol/m/s/Pa, amol/(m.s.Pa), amol/(m s Pa)... est consid\\u00e9r\\u00e9e comme pertinente. Pour tous les couples pertinents valid\\u00e9s, nous n'int\\u00e9grons qu'une seule fois le variant amol / m s Pa. Consid\\u00e9rant cette remarque, sur les 267 couples cumul\\u00e9s dans le tableau 3, valid\\u00e9s par SM D b (260 couples s\\u00e9lectionn\\u00e9s), nous obtenons 121 variants d'unit\\u00e9s uniques \\u00e0 int\\u00e9grer dans notre RTO.  Nous avons montr\\u00e9 que la premi\\u00e8re \\u00e9tape de notre m\\u00e9thode, s'appuyant sur l'apprentissage supervis\\u00e9, permet de localiser automatiquement les variants d'unit\\u00e9 dans un contexte phrastique optimal de recherche. La m\\u00e9thode repose sur une nouvelle repr\\u00e9sentation des donn\\u00e9es, sous forme de fen \\u00eatres textuelles d'\\u00e9tude, que nous obtenons en \\u00e9tant guid\\u00e9 par la RTO. Par la suite, nous avons propos\\u00e9 une nouvelle mesure de similarit\\u00e9 adapt\\u00e9e aux sp\\u00e9cificit\\u00e9s des unit\\u00e9s de mesure. Le choix de la mesure de Damereau-Levenshtein est appropri\\u00e9e \\u00e0 notre contexte car elle prend en charge toutes les variations constat\\u00e9es pour les unit\\u00e9s de mesure. De plus, associ\\u00e9e \\u00e0 l'indice de Jaccard, la nouvelle mesure permet de rapprocher les couples variant-unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e de mani\\u00e8re plus pertinente en octroyant un premier score global de similarit\\u00e9 qui ne tient pas compte de l'ordre des blocs dans la construction de l'unit\\u00e9 complexe. Dans un second temps, la nouvelle mesure SM D b affine ce rapprochement en comparant chaque bloc du variant et de l'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e s\\u00e9lectionn\\u00e9e. Le processus d'enrichissement de la RTO \\u00e9tant un processus it\\u00e9ratif, une nouvelle phase d'extraction et d'identification permettrait alors de comparer d'autres variants avec ces nouvelles unit\\u00e9s int\\u00e9gr\\u00e9es dans la RTO, qui deviennent des r\\u00e9f\\u00e9rents.\",\n          \"Cet article propose une m\\u00e9thode de codage automatique de traits lexicaux s\\u00e9mantiques en fran\\u00e7ais. Cette approche exploite les relations fix\\u00e9es par l'instruction s\\u00e9mantique d'un op\\u00e9rateur de construction morphologique entre la base et le mot construit. En cela, la r\\u00e9flexion s'inspire des travaux de Marc Light (Light 1996) tout en exploitant le fonctionnement d'un syst\\u00e8me d'analyse morphologique existant : l'analyseur D\\u00e9riF. A ce jour, l'analyse de 12 types morphologiques conduit \\u00e0 l'\\u00e9tiquetage d'environ 10 % d'un lexique compos\\u00e9 de 99000 lemmes. L'article s'ach\\u00e8ve par la description de deux techniques utilis\\u00e9es pour valider les traits s\\u00e9mantiques. This paper presents an approach which aims at automatically tagging a French lexicon with semantic features. This approach makes use of correspondences which are fixed by the semantic instruction of morphological operators, between the base and the derived word. It is partly inspired by Light (1996) work, and makes use of an existing morphological parser : the DeriF system. So far, 12 morphological types have been analysed, enabling the semantic tagging of circa 10 % of a lexicon made up of 99000 lemmas. The paper ends up with the description of two techniques that have been applied to validate the semantic features. Keywords -Mots Cl\\u00e9s Morphologie d\\u00e9rivationnelle -affixation et conversion -acquisition de traits s\\u00e9mantiques Derivational Morphology -affixation and conversion -Semantic feature acquisition * Je remercie les relecteurs anonymes pour leurs commentaires et suggestions, qui m'ont aid\\u00e9e dans la r\\u00e9daction de la deuxi\\u00e8me version de cet article Introduction L'information s\\u00e9mantique est cruciale pour l'ensemble des applications en TALN. Cette information, cependant, n'est pas toujours accessible : le codage s\\u00e9mantique est une t\\u00e2che complexe, donc \\u00e0 co\\u00fbt \\u00e9lev\\u00e9. Nous allons voir comment la morphologie peut \\u00eatre utilis\\u00e9e pour amorcer le codage d'un lexique d'environ 99000 lemmes cat\\u00e9goris\\u00e9s au moyen de traits s\\u00e9mantiques. L'exp\\u00e9rience relat\\u00e9e se fonde sur les hypoth\\u00e8ses linguistiques \\u00e9nonc\\u00e9es \\u00e0 l'origine dans (Corbin, 1987) et tire parti du fonctionnement d'un analyseur de mots construits existant : le syst\\u00e8me D\\u00e9riF (Namer, 1999) . A la section 2, nous situons notre approche dans le cadre des travaux d'acquisition de s\\u00e9mantique lexicale. Ensuite, la section 3 illustre par des exemples le r\\u00f4le des contraintes s\\u00e9mantiques dans des op\\u00e9rations de construction lexicale par suffixation, pr\\u00e9fixation et conversion. Une partie de ces contraintes est mise en oeuvre, au d\\u00e9but de la section 4, dans le programme D\\u00e9riF pour annoter automatiquement certaines bases et d\\u00e9riv\\u00e9s au moyen des traits pertinents. La fin de la section 4 pr\\u00e9sente enfin une m\\u00e9thode de validation de ces traits, bas\\u00e9e sur deux approches : filtre automatique, et recherche sur Internet. Sens et ressources lexicales Disposer de ressources lexicales munies d'informations s\\u00e9mantiques \\u00e0 la fois facilement exploitables et publiquement disponibles est un atout pr\\u00e9cieux dans l'\\u00e9laboration de corpus annot\\u00e9s s\\u00e9mantiquement, t\\u00e2che primordiale dans de nombreuses applications TALN : l'anglais dispose du r\\u00e9seau WordNet, d\\u00e9velopp\\u00e9 \\u00e0 Princeton ( (Miller, 1995) , (Fellbaum, 1998) ). Le consortium EuroWordNet (Vossen, 1998) a r\\u00e9alis\\u00e9 des r\\u00e9seaux s\\u00e9mantiques en n\\u00e9erlandais, italien, espagnol, fran\\u00e7ais, allemand, tch\\u00e8que et estonien, en lien avec WordNet. La base lexicale du fran\\u00e7ais pr\\u00e9sente cependant des lacunes : seules les cat\\u00e9gories noms et verbes y sont repr\\u00e9sent\\u00e9es, et les seules relations disponibles sont la synonymie, l'hyperonymie et l'hyponymie. De nombreux travaux r\\u00e9cents portant sur l'\\u00e9tiquetage s\\u00e9mantique des corpus d\\u00e9crivent des m\\u00e9thodes d'acquisition de relations s\\u00e9mantiques compl\\u00e9mentaires, comme par exemple la structure argumentale et les contraintes de s\\u00e9lection des verbes, ce que r\\u00e9alise pour l'anglais (MacCarthy et al. 2001) . Parmi les travaux concernant le fran\\u00e7ais, Fabre et Jacquemin (2000) proposent le codage manuel d'un ensemble minimal de traits verbaux binaires de mani\\u00e8re \\u00e0 accro\\u00eetre la pr\\u00e9cision dans l'appariement de variantes teminologiques. Cet article propose une m\\u00e9thode de codage automatique de traits lexicaux s\\u00e9mantiques en fran\\u00e7ais qui se veut compl\\u00e9mentaire \\u00e0 la fois d'EuroWordNet, pour la nature des traits cod\\u00e9s, et de (Fabre & Jacquemin, 2000) , pour la m\\u00e9thode d'acquisition de ces informations : en effet, les traits s\\u00e9mantiques qui codent les entr\\u00e9es lexicales sont d\\u00e9duites de contraintes s\\u00e9mantiques associ\\u00e9es aux op\\u00e9rations de construction de mots, s'exer\\u00e7ant tant sur la base que sur le d\\u00e9riv\\u00e9. Cette m\\u00e9thode s'inspire des travaux de Light (1996) ; cependant, ce syst\\u00e8me d'acquisition s'appuie sur un programme d'analyse automatique du lexique construit en fran\\u00e7ais, effectuant \\u00e0 ce jour l'analyse compl\\u00e8te de 18500 mots construits appartenant \\u00e0 divers types morphologiques. En cela, notre approche s'\\u00e9loigne des pr\\u00e9occupations de M. Light, dont l'objectif est d'associer pour l'anglais \\u00e0 chaque op\\u00e9ration morphologique la repr\\u00e9sentation s\\u00e9mantique la plus probable de la base et du d\\u00e9riv\\u00e9. Son souci \\u00e9tant de pr\\u00e9server un \\u00e9quilibre entre pr\\u00e9cision et rappel, un taux d'erreur non nul est in\\u00e9vitable (il varie de 0% pour l'assignation de l'\\u00e9tiquette \\\"changement d'\\u00e9tat\\\" aux d\\u00e9riv\\u00e9s en -en, \\u00e0 80% pour l'assignation du trait \\\"\\u00e9tat final oppos\\u00e9 \\u00e0 la base\\\" aux d\\u00e9riv\\u00e9s en de-). A l'inverse, notre objectif est de privil\\u00e9gier les op\\u00e9rations morphologiques conduisant \\u00e0 un codage s\\u00e9mantique d'une pr\\u00e9cision proche de 100%. L'approche propos\\u00e9e ici est illustr\\u00e9e par l'\\u00e9tude de quelques op\\u00e9rations de construction de mots (suffixation, pr\\u00e9fixation ou conversion) s\\u00e9lectionn\\u00e9es pour leur r\\u00e9gularit\\u00e9 et, si possible, leur productivit\\u00e9, et donc les plus pertinentes du point de vue qualitatif (les op\\u00e9rateurs r\\u00e9guliers affichant un taux \\u00e9lev\\u00e9 de pr\\u00e9cision dans l'assignation des traits s\\u00e9mantiques \\u00e0 la base et au d\\u00e9riv\\u00e9) et, le cas \\u00e9ch\\u00e9ant, quantitatif (les op\\u00e9rateurs productifs garantissant un nombre \\u00e9lev\\u00e9 d'entr\\u00e9es cod\\u00e9es dans la base). Cadre th\\u00e9orique Le cadre th\\u00e9orique dans lequel se situent les analyses linguistiques propos\\u00e9es dans cette section est d\\u00e9crit essentiellement dans (Corbin, 1987) , o\\u00f9 la morphologie d\\u00e9rivationnelle est con\\u00e7ue comme le calcul conjoint de la structure et du sens des unit\\u00e9s lexicales. Cette perspective \\\" associative \\\" donne \\u00e0 analyser un mot comme construit s'il a une structure construite et un sens construit qui soit calculable \\u00e0 partir de celle-l\\u00e0. Op\\u00e9rateurs constructionnels et contraintes s\\u00e9mantiques Outre la composition, qui met en jeu deux unit\\u00e9s lexicales \\u00e0 sens r\\u00e9f\\u00e9rentiel, les op\\u00e9rations de construction de mots rel\\u00e8vent de deux types possibles de proc\\u00e9d\\u00e9s : l'affixation (application d'un suffixe ou un pr\\u00e9fixe \\u00e0 une base) associe sens et forme, alors que la conversion (appel\\u00e9e aussi \\\"d\\u00e9rivation impropre\\\", comme le rel\\u00e8vent, entre autre Arriv\\u00e9 et al. (1986) ) met en relation deux cat\\u00e9gories grammaticales sans l'entremise de mat\\u00e9riel lexical. Les op\\u00e9rations qui nous int\\u00e9ressent ici doivent avoir, comme propri\\u00e9t\\u00e9 commune, la facult\\u00e9 d'exercer des contraintes s\\u00e9lectionnelles sp\\u00e9cifiques et r\\u00e9guli\\u00e8res sur la base et/ou le d\\u00e9riv\\u00e9 qu'elles relient. De cette mani\\u00e8re, le calcul automatique des traits s\\u00e9mantiques sur les entr\\u00e9es de la base sera pr\\u00e9cis, le nombre d'exceptions limit\\u00e9, et l'amor\\u00e7age du codage s\\u00e9mantique qui en r\\u00e9sultera sera qualitativement pertinent. Un certain nombre d'op\\u00e9rations v\\u00e9rifient ces conditions, qu'il s'agisse d'affixation ou de conversion. Nous laissons volontairement de c\\u00f4t\\u00e9 l'\\u00e9tude th\\u00e9orique des noms d\\u00e9verbaux de proc\\u00e8s ; soit parce que le typage du d\\u00e9riv\\u00e9 en tant que nom abstrait est une propri\\u00e9t\\u00e9 connue (cf. -age (repassage) ou -ment (gonflement)) et que toute sp\\u00e9cification suppl\\u00e9mentaire est impossible en l'absence de traits s\\u00e9mantiques sur le verbe ; soit parce que le typage du d\\u00e9riv\\u00e9 est non d\\u00e9terministe, et d\\u00e9pend de l'aspect t\\u00e9lique du verbe de base (ainsi, les noms en -tion d\\u00e9signent \\u00e0 la fois un proc\\u00e8s, et soit une manifestation de ce proc\\u00e8s (destruction) soit l'entit\\u00e9 concr\\u00e8te r\\u00e9sultante (construction)) ; pour les m\\u00eames raisons, nous ne dirons rien des noms abstraits de propri\\u00e9t\\u00e9 construits par -it\\u00e9 1 . En revanche, nous examinons quelques op\\u00e9rateurs peut-\\u00eatre moins connus, mais dont l'instruction s\\u00e9mantique permet d'\\u00e9tiqueter de fa\\u00e7on fine des noms , des verbes, et, plus rarement, des adjectifs. Typage des noms Chacun \\u00e0 sa mani\\u00e8re, les suffixes -aille, -aie, et -oir(e) entra\\u00eenent un typage s\\u00e9mantique homog\\u00e8ne du nom en position de base et/ou du nom construit. Il existe deux suffixes -aille (Aliquot-Suengas, 1999), qui s\\u00e9lectionnent des bases concr\\u00e8tes, mais qui construisent des noms (\\u00e9galement concrets) ayant des caract\\u00e9ristiques diff\\u00e9rentes. On distingue en effet une majorit\\u00e9 de noms en -aille collectifs \\u00e0 valeur \\u00e9valuative (flicaille ou ferraille sont marqu\\u00e9s n\\u00e9gativement), qui d\\u00e9signent des agglom\\u00e9rats dont il est impossible de distinguer les morceaux, qu'ils soient comptables (flic) ou massifs (fer), et les noms en -aille \\u00e0 valeur \\u00e9nonciative argotique non p\\u00e9jorative, qui ne sont pas des collectifs (jupaille). Seuls les premiers sont pertinents dans un lexique de la langue g\\u00e9n\\u00e9rale. Comme -aille, le suffixeaie, (cf. Aliquot-Suengas, 1996) , est un constructeur de noms concrets collectifs qui s\\u00e9lectionne exclusivement des bases d\\u00e9signant des esp\\u00e8ces v\\u00e9g\\u00e9tales, et programme le nom qu'il construit \\u00e0 d\\u00e9crire un tout (ormaie, bananeraie), o\\u00f9 chaque entit\\u00e9 qui le constitue (orme, bananier) est ind\\u00e9pendante des autres, contrairement, par exemple, \\u00e0 ce que l'on a observ\\u00e9 avec -aille formateur d'\\u00e9valuatifs. Contrairement aux deux premiers, enfin, le suffixe -oir(e) s\\u00e9lectionne des bases verbales et construit des noms concrets d\\u00e9signant le lieu (fumoir) ou l'instrument (hachoir) participant au d\\u00e9roulement du proc\\u00e8s que d\\u00e9crit la base. Souvent, le nom peut d\\u00e9signer une entit\\u00e9 qui sert \\u00e0 la fois de lieu et d'instrument (abreuvoir). Typage crois\\u00e9 de verbes et adjectifs L'ensemble des op\\u00e9rations de construction de verbes d\\u00e9sadjectivaux (affixation et conversion) produisent par essence des pr\\u00e9dicats de changement d'\\u00e9tat, l'\\u00e9tat final \\u00e9tant d\\u00e9crit par l'adjectif en position de base, qui de ce fait exprime une propri\\u00e9t\\u00e9 que l'on peut qualifier d'extrins\\u00e8que 2 . Quand a-fabrique un verbe \\u00e0 partir d'une base adjectivale (aplatir, allonger), celui-ci est, causatif, transitif et le r\\u00e9f\\u00e9rent de son objet direct se retrouve dans l'\\u00e9tat d\\u00e9crit par l'adjectif \\u00e0 la fin du d\\u00e9roulement du proc\\u00e8s. A quelques exceptions pr\\u00e8s (brutaliser, b\\u00eatifier), il en est de m\\u00eame pour -is(er) et -ifi(er). Quant aux verbes d\\u00e9sadjectivaux construits par conversion, ils sont soit causatifs, transitifs (vider) soit r\\u00e9sultatifs intransitifs (bl\\u00eamir). Certains admettent les deux constructions : c'est notamment le cas des verbes \\u00e0 base adjectivale chromatique (rougir, brunir). Un autre cas de typage verbal homog\\u00e8ne s'observe par exemple avec les bases s\\u00e9lectionn\\u00e9es par le suffixe -able formateur d'adjectifs (cf. (Pl\\u00e9nat, 1988) , repris et mod\\u00e9lis\\u00e9 dans (Dal et al., 1999) ). En effet, quand ce suffixe construit un adjectif d\\u00e9verbal, celui-ci d\\u00e9signe une aptitude, une propri\\u00e9t\\u00e9 intrins\\u00e8que, et la base verbale est g\\u00e9n\\u00e9ralement transitive. Quand elle ne l'est pas, elle d\\u00e9signe un pr\\u00e9dicat intransitif de mouvement se construisant avec modifieur exprimant la surface sur laquelle s'effectue le mouvement : circulable, skiable, navigable, roulable, surfable. On remarquera (Dal et Namer, 2000) que la propri\\u00e9t\\u00e9 extrins\\u00e8que observ\\u00e9e sur les adjectifs s\\u00e9lectionn\\u00e9s par les op\\u00e9rateurs formateurs de verbes est incompatible avec la propri\\u00e9t\\u00e9 intrins\\u00e8que des adjectifs en -able, ce qui explique que ces derniers sont des bases tr\\u00e8s improbables pour des verbes d\\u00e9sadjectivaux de changement d'\\u00e9tat, comme l'illustrent les exemples agrammaticaux suivants : *lavabiliser, *imperm\\u00e9abl(ir|er), *aportabl(ir|er), *croyabilifier. Typage crois\\u00e9 de verbes et noms Le dernier exemple concerne le pr\\u00e9fixe \\u00e9-, illustrant le cas des verbes d\\u00e9nominaux construits 3 . Aurnague et Pl\\u00e9nat (1997) d\\u00e9crivent \\u00e9-comme un marqueur de dissociation entre une partie concr\\u00e8te le plus souvent inanim\\u00e9e, d\\u00e9sign\\u00e9e par ce \\u00e0 quoi r\\u00e9f\\u00e8re la base, et le tout (l'objet direct du verbe construit, qui par cons\\u00e9quent est un causatif transitif). Ils proposent une tripartition de ces verbes selon que la partie est (1) un constituant naturel du tout (effeuiller une plante, \\u00e9p\\u00e9piner un raisin), (2) produite \\u00e0 partir du tout \\u00e0 l'issue du proc\\u00e8s, et ne pr\\u00e9existant pas \\u00e0 celui-ci (effranger un tapis), (3) un parasite du tout (\\u00e9pouiller un chien, \\u00e9pousseter un meuble). Traits cod\\u00e9s Il est possible de d\\u00e9duire un ensemble de traits \\u00e0 partir des descriptions pr\\u00e9sent\\u00e9es en 3.1. Bien sur, le nombre de ces traits ainsi que leur structuration sera vraisemblablement remis en cause avec l'analyse de nouveaux types morphologiques. Nous avons alors pr\\u00e9f\\u00e9r\\u00e9 dans un premier temps adopter une typologie all\\u00e9g\\u00e9e, parfois h\\u00e9t\\u00e9rog\\u00e8ne, de mani\\u00e8re \\u00e0 simplifier les m\\u00e9thodes de traitement. C'est ainsi que le codage des adjectifs se limite \\u00e0 l'opposition extrins\\u00e8que/intrins\\u00e8que. En ce qui concerne le codage des noms et des verbes, il a \\u00e9t\\u00e9 proc\\u00e9d\\u00e9 \\u00e0 chaque fois \\u00e0 une structuration minimale sous forme de listes de trois champs. Tout champ non pertinent est cod\\u00e9 xxx. Ainsi, un nom est soit concret, soit abstrait, trait qui constitue le premier champ de la liste. Le second sp\\u00e9cifie les noms concrets : inanim\\u00e9, que l'on oppose \\u00e0 anim\\u00e9. Cette position reste sous-sp\\u00e9cifi\\u00e9e pour les noms abstraits. La troisi\\u00e8me position est occup\\u00e9e par la caract\\u00e9risation ult\\u00e9rieure du nom : collectif, lieu, esp\\u00e8ce-v\\u00e9g\\u00e9tale, instrument, pour les noms concrets, et propri\\u00e9t\\u00e9 ou proc\\u00e8s pour les abstraits. En ce qui concerne les verbes, on oppose d'abord causatif et r\\u00e9sultatif, puis transitif et intransitif. Le troisi\\u00e8me champ d\\u00e9crit la structure argumentale correspondante, \\u00e9tiquet\\u00e9e th\\u00e9matiquement. Le Tableau 1 \\u00e0 la section 4.1.2. illustre la structuration choisie. Mise en oeuvre : programme et r\\u00e9sultats Le calcul automatique de traits s\\u00e9mantiques sur les entr\\u00e9es d'un lexique de lemmes \\u00e9tiquet\\u00e9s, inspir\\u00e9 des hypoth\\u00e8ses th\\u00e9oriques expos\\u00e9es en 3, tire profit de l'existence d'un programme d'analyse automatique de mots construits, baptis\\u00e9 D\\u00e9riF (\\\"D\\u00e9rivation en Fran\\u00e7ais\\\"). Dans 3 Je ne parlerai par contre ni de suffixation (les bases des verbes d\\u00e9nominaux construits par -is(er) et -ifi (er) appartiennent \\u00e0 des types s\\u00e9mantiques vari\\u00e9s (vampiriser vs r\\u00e9volveriser, momifier vs baronifier) refl\\u00e9tant les variantes de l'instruction s\\u00e9mantique du suffixe) ni de conversion N->V. En effet, cette op\\u00e9ration s\\u00e9lectionne aussi bien des noms d'instrument ou lieu concrets (hache/hacher, coffre/coffrer) que abstraits (force/forcer) (Corbin, \\u00e0 para\\u00eetre). La base peut \\u00e9galement faire r\\u00e9f\\u00e9rence \\u00e0 un humain, jouant alors le r\\u00f4le d'agent typique (singe/singer), alors m\\u00eame que l'op\\u00e9ration de conversion inverse, produisant essentiellement des noms de (r\\u00e9sultat ou manifestation de) proc\\u00e8s (voler/vol, chuter/chute), peut conduire aussi \\u00e0 la construction de noms d'agents (garder/garde) (Corbin, 1987) . On le voit, le choix des crit\\u00e8res \\u00e0 prendre en compte pour d\\u00e9terminer les contraintes sous-jacentes \\u00e0 l'op\\u00e9ration de conversion N/V et \\u00e0 son orientation n\\u00e9cessite une r\\u00e9flexion qu'abordent entre autre Kerleroux (1996 Kerleroux ( , 1997) ) 1) continuit\\u00e9, N => [ [ continu A] it\\u00e9 N] (continuit\\u00e9/N, continu/A) :: \\\"facult\\u00e9 d'\\u00eatre continu\\\" 2) explicabilit\\u00e9, N => [ [ [ expliquer V] able A] it\\u00e9 N] (explicabilit\\u00e9/N, explicable/A, expliquer/V) :: \\\"facult\\u00e9 d'\\u00eatre explicable\\\" 3a) ragrafer, V => [ re [ [ agrafe N] (er) V ] V] (ragrafer/V, agrafer/V, agrafe/N) ::agrafer une nouvelle fois 3b) portable, N => [ [ [ porter V] able A] N ] (portable/N, portable/A, porter/V) :: \\\"entit\\u00e9 ayant pour propri\\u00e9t\\u00e9 principale d'\\u00eatre portable\\\" 3c) introuvable, A =>[in[[trouver V] able A]A] (introuvable/A, trouvable/A, trouver/V) :: \\\"non trouvable\\\" 3d) d\\u00e9sossable, A => [ [ d\\u00e9 [os N] V] able A] (d\\u00e9sossable Annotation de la base au moyen de traits s\\u00e9mantiques Les types morphologiques d\\u00e9crits en 3.1. auxquels s'ajoutent les noms d\\u00e9verbaux en -age etment, et les noms d\\u00e9sadjectivaux en -it\\u00e9 donnent lieu au codage automatique d'environ 9000 entr\\u00e9es, soit 9% du lexique de d\\u00e9part. Quand les op\\u00e9rations morphologiques d\\u00e9crites \\u00e0 la section 3 sont activ\\u00e9es lors de l'analyse d'un lemme L du corpus, la t\\u00e2che de codage s\\u00e9mantique est d\\u00e9clench\\u00e9e en parall\\u00e8le sur une copie du corpus, o\\u00f9 la description de L est enrichie au moyen du ou des traits d\\u00e9pendants de l'instruction s\\u00e9mantique de l'op\\u00e9rateur, ainsi qu'en t\\u00e9moigne l'\\u00e9chantillon cod\\u00e9 dans le Tableau 1, o\\u00f9 sont indiqu\\u00e9s (en colonne 5) les traits calcul\\u00e9s pour L (apparaissant dans la colonne 4) en tant que base ou d\\u00e9riv\\u00e9 (indiqu\\u00e9 par \\u00ab B \\u00bb ou \\u00ab D \\u00bb, colonne 3) de l'op\\u00e9ration morphologique qui s'est appliqu\\u00e9e (report\\u00e9e dans la colonne 2). Les traits cod\\u00e9s sont parfois sous-d\\u00e9termin\\u00e9s (ligne 2, 4), parfois erron\\u00e9s (ligne 3). Le fait que L accumule plusieurs s\\u00e9ries de traits est d\\u00fb soit au codage en soi (ligne 4), soit \\u00e0 l'implication de L dans plusieurs op\\u00e9rations morphologiques (ligne 7). Op. (D)\\u00e9riv\\u00e9 / (B)ase Lemme Etiquet\\u00e9 Traits Cod\\u00e9s 1 a- D aplatir/VERBE (causatif Validation des traits Comme l'illustre le Tableau 1, une partie des traits cod\\u00e9s automatiquement va n\\u00e9cessiter une v\\u00e9rification, et, le cas \\u00e9ch\\u00e9ant, une correction. En effet 1547 entr\\u00e9es sont munies d'un codage ambigu (c'est le cas des bases verbales des adjectifs en -able, des noms d\\u00e9verbaux en -oir, et des verbes d\\u00e9sadjectivaux obtenus par conversion) ; on s'attend \\u00e0 ce que la phase de validation permette de lever certaines ambigu\\u00eft\\u00e9s. A l'inverse, il est vraisemblable que certains codages soient erron\\u00e9s (ainsi, parmi les 425 verbes en -iser, les 58 verbes en -ifier et les 90 verbes en a-analys\\u00e9s par d\\u00e9faut comme \\u00e9tant causatifs transitifs, une partie d'entre eux poss\\u00e8de une construction exclusivement intransitive) ; la validation a ici pour objectif de guider vers le rep\\u00e9rage de ces entr\\u00e9es mal cod\\u00e9es, dont la correction in fine sera manuelle. D'autres traits erron\\u00e9s sont par contre ind\\u00e9tectables automatiquement : c'est le cas de quelques noms de type anim\\u00e9 d\\u00e9signant des parasites (chenille, pou, puce, taupe) en position de base des verbes en \\u00e9-et cod\\u00e9s par d\\u00e9faut -\\u00e0 tort -inanim\\u00e9 (cf. section 3.1.3). Ces v\\u00e9rifications vont \\u00eatre r\\u00e9alis\\u00e9es automatiquement au moyen de deux techniques. Validation par filtrage : C'est tout d'abord au moyen de filtres automatiques que sont rep\\u00e9r\\u00e9s les codages des lemmes qui sont le fait d'op\\u00e9rateurs diff\\u00e9rents ; ces filtres comparent les informations co-occurrentes, de mani\\u00e8re \\u00e0 relever les traits compl\\u00e9mentaires ou unifiables. A l'heure actuelle, la mise en oeuvre de cette m\\u00e9thode n'apporte pas de r\\u00e9sultats spectaculaires, en raison du nombre encore faible de types morphologiques trait\\u00e9s. Seuls 83 lemmes re\\u00e7oivent leur codage s\\u00e9mantique \\u00e0 partir de deux op\\u00e9rations morphologiques distinctes : c'est le cas des bases verbales construites dans les adjectifs en -able qui se retrouvent avec une double s\\u00e9rie de traits, dont le filtre v\\u00e9rifie la compatibilit\\u00e9. Par exemple, sur les verbes convertis (ex. 6), le filtre \\u00e9limine la derni\\u00e8re s\\u00e9rie de traits, incompatible avec le type de base verbale, et unifie les 1 \\u00e8re et 3 \\u00e8me s\\u00e9ries (la 1 \\u00e8re \\u00e9tant un cas particulier de la 3 \\u00e8me ). 6) pr\\u00e9ciser/VERBE (causatif, transitif, [cause, th\\u00e8me] ), (r\\u00e9sultatif, intransitif, [th\\u00e8me] ), (xxx, transitif, [agent, th\\u00e8me] ), (xxx, intransitif, [agent, (lieu(sur) )) L'autre type de codage valid\\u00e9 par filtre est celui de lemmes comme le nom tripaille, (cf. Tableau 1), qui poss\\u00e8de deux s\\u00e9ries de traits compl\\u00e9mentaires : cette entr\\u00e9e r\\u00e9unit \\u00e0 la fois les contraintes des bases nominales s\\u00e9lectionn\\u00e9es par \\u00e9-(cf. 3.1.3) et les propri\\u00e9t\\u00e9s des noms construits au moyen de -aille (cf. 3.1.1) . Le filtre remplace automatiquement dans l'une des listes de traits, la valeur vide (\\\"xxx\\\") par l'instance occupant la m\\u00eame place dans l'autre liste, aboutissant \\u00e0 la fin au codage : (concret, inanim\\u00e9, collectif). On peut pr\\u00e9dire qu'avec l'\\u00e9largissement de la couverture en termes d'op\\u00e9rations morphologiques, la juxtaposition des traits rep\\u00e9r\\u00e9e automatiquement au moyen de filtres pourra mettre en lumi\\u00e8re des ph\\u00e9nom\\u00e8nes int\\u00e9ressants : si les traits se contredisent, cela pourra signifier la pr\\u00e9sence d'exceptions \\u00e0 des r\\u00e8gles de formation de mots bien s\\u00fbr, mais aussi l'existence d'acceptions multiples pour une entr\\u00e9e lexicale ; ainsi, quand l'analyseur traitera les noms suffix\\u00e9s en -ure et en -ier on peut supposer que l'entr\\u00e9e nominale avocat recevra \\u00e0 la fois le trait (concret, inanim\\u00e9, xxx) quand il est analys\\u00e9 \\u00e0 partir de avocatier et (concret, humain, profession) quand il est reli\\u00e9 \\u00e0 avocature 4 . L'incompatibilit\\u00e9 entre inanim\\u00e9 et humain t\\u00e9moigne ici de l'existence de deux homonymes : le fruit et l'individu. A l'inverse, on peut aussi imaginer que les traits calcul\\u00e9s sur une entr\\u00e9e sp\\u00e9cifieront l'instruction s\\u00e9mantique d'un op\\u00e9rateur acc\\u00e9dant \\u00e0 cette base. C'est ce qu'on est en droit de s'attendre avec le verbe accessoiriser, actuellement analys\\u00e9 au moyen d'une disjonction de gloses (cf. note 3), que l'\\u00e9tiquetage instrumental du nom accessoire, permettra de r\\u00e9duire \\u00e0 : \\\"munir de accessoire\\\". Validation par Internet: La sp\\u00e9cification des informations s\\u00e9mantiques peut tirer profit des r\\u00e9sultats d'une autre approche de validation. Cette approche, d\\u00e9j\\u00e0 mise en oeuvre dans (Dal & Namer 2000) Conclusion Le travail pr\\u00e9sent\\u00e9 propose une m\\u00e9thode pour coder automatiquement des traits s\\u00e9mantiques lexicaux en se servant des contraintes impos\\u00e9es par les op\\u00e9rateurs de construction de mots. Seuls quelques traits s\\u00e9mantiques sont accessibles par cette m\\u00e9thode, et seule une partie du lexique est cod\\u00e9e \\u00e0 ce jour. Il est pr\\u00e9visible que l'\\u00e9volution de l'analyseur entra\\u00eene un enrichissement du codage pour les noms et les verbes. Mais m\\u00eame partiels, ces r\\u00e9sultats sont n\\u00e9anmoins utilisables d'ores et d\\u00e9j\\u00e0 en analyse morphologique pour expliciter des exceptions ou sp\\u00e9cifier des gloses. Ils sont \\u00e9galement exploitables pour s\\u00e9lectionner les bases candidates \\u00e0 la g\\u00e9n\\u00e9ration automatique de mots construits : ainsi, les traits codant les verbes en \\u00e9-ou en a-en font des bases potentielles pour des adjectifs en -able. Enfin, l'approche pr\\u00e9sent\\u00e9e peut servir, dans un syst\\u00e8me d'analyse automatique, \\u00e0 la caract\\u00e9risation s\\u00e9mantique des mots construits qui sont absents des dictionnaires de langue parce qu'ils appartiennent \\u00e0 un type morphologique trop productif, ou parce qu'il s'agit de n\\u00e9ologismes : la nature r\\u00e9guli\\u00e8re des n\\u00e9ologismes construits laisse en effet pr\\u00e9sager une bonne capacit\\u00e9 pr\\u00e9dictive du syst\\u00e8me d'assignation des traits. R\\u00e9f\\u00e9rences Arriv\\u00e9, M, Gadet F. et Galmiche M., 1986 , La Grammaire d'Aujourd'hui, Paris, Flammarion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publisher\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ATALA\",\n          \"Association pour le Traitement Automatique des Langues\",\n          \"ATALA/AFCP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"https://aclanthology.org/2006.jeptalnrecital-poster.10\",\n          \"https://aclanthology.org/2006.jeptalnrecital-recitalposter.2\",\n          \"https://aclanthology.org/F14-2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "data_netoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ab6I7lhGf0mN"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "hvUB9vwif0kA"
      },
      "outputs": [],
      "source": [
        "def texte_fr(text):\n",
        "  reg=re.match(r'(.+)(\\(.+\\))',text)\n",
        "  if not reg:\n",
        "    return text\n",
        "  if reg:\n",
        "    try:\n",
        "      uno=reg.group(1)\n",
        "      dos=reg.group(2)\n",
        "    except:\n",
        "      return text\n",
        "    try:\n",
        "      un=detect(uno) if uno else None\n",
        "      deux=detect(dos) if dos else None\n",
        "      if un==\"fr\":\n",
        "        return uno\n",
        "      elif deux==\"fr\":\n",
        "        return dos.strip(\"()\")\n",
        "      else:\n",
        "        return text\n",
        "    except LangDetectException as e:\n",
        "      lang=\"unknown\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "FpIrirPqf0is"
      },
      "outputs": [],
      "source": [
        "data_netoy[\"title_fr_clean\"]=data_netoy[\"title_fr\"].apply(texte_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "utXD7Tvaf-iv",
        "outputId": "63395ac1-7f45-46bd-8b8f-76802af30ce0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              acl_id  \\\n",
              "100      2004.jeptalnrecital-long.32   \n",
              "572      2018.jeptalnrecital-long.10   \n",
              "576     2018.jeptalnrecital-court.17   \n",
              "581      2017.jeptalnrecital-long.13   \n",
              "604       2015.jeptalnrecital-long.9   \n",
              "...                              ...   \n",
              "69517   2009.jeptalnrecital-court.21   \n",
              "69519  2015.jeptalnrecital-recital.5   \n",
              "69523  2015.jeptalnrecital-recital.6   \n",
              "69524   2009.jeptalnrecital-court.11   \n",
              "69528   2008.jeptalnrecital-court.16   \n",
              "\n",
              "                                                title_fr  \\\n",
              "100    Annoter en constituants pour évaluer des analy...   \n",
              "572    Intégration de contexte global par amorçage po...   \n",
              "576    Utilisation de Représentations Distribuées de ...   \n",
              "581    Apprendre des représentations jointes de mots ...   \n",
              "604    Méthode faiblement supervisée pour l'extractio...   \n",
              "...                                                  ...   \n",
              "69517  La complémentarité des approches manuelle et a...   \n",
              "69519  Alignement multimodal de ressources éducatives...   \n",
              "69523  État de l'art : analyse des conversations écri...   \n",
              "69524  Chaîne de traitement linguistique : du repérag...   \n",
              "69528  Vers l'identification et le traitement des act...   \n",
              "\n",
              "                                                abstract  \\\n",
              "100    Cet article présente l'annotation en constitua...   \n",
              "572    Les approches neuronales obtiennent depuis plu...   \n",
              "576    L'identification des entités nommées dans un t...   \n",
              "581    1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...   \n",
              "604    La détection d'opinion ciblée a pour but d'att...   \n",
              "...                                                  ...   \n",
              "69517  Les ressources lexicales sont essentielles pou...   \n",
              "69519  Cet article présente certaines questions de re...   \n",
              "69523  Le développement du Web 2.0 et le processus de...   \n",
              "69524  Cet article présente la chaîne de traitement l...   \n",
              "69528  Il peut être difficile d'attribuer une seule v...   \n",
              "\n",
              "                                               full_text publisher  \\\n",
              "100    Cet article présente l'annotation en constitua...     ATALA   \n",
              "572    Les approches neuronales obtiennent depuis plu...     ATALA   \n",
              "576    L'identification des entités nommées dans un t...     ATALA   \n",
              "581    1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...     ATALA   \n",
              "604    La détection d'opinion ciblée a pour but d'att...     ATALA   \n",
              "...                                                  ...       ...   \n",
              "69517  Les ressources lexicales sont essentielles pou...     ATALA   \n",
              "69519  Cet article présente certaines questions de re...     ATALA   \n",
              "69523  Le développement du Web 2.0 et le processus de...     ATALA   \n",
              "69524  Cet article présente la chaîne de traitement l...     ATALA   \n",
              "69528  Il peut être difficile d'attribuer une seule v...     ATALA   \n",
              "\n",
              "                                                     url  \\\n",
              "100    https://aclanthology.org/2004.jeptalnrecital-l...   \n",
              "572    https://aclanthology.org/2018.jeptalnrecital-l...   \n",
              "576    https://aclanthology.org/2018.jeptalnrecital-c...   \n",
              "581    https://aclanthology.org/2017.jeptalnrecital-l...   \n",
              "604    https://aclanthology.org/2015.jeptalnrecital-l...   \n",
              "...                                                  ...   \n",
              "69517  https://aclanthology.org/2009.jeptalnrecital-c...   \n",
              "69519  https://aclanthology.org/2015.jeptalnrecital-r...   \n",
              "69523  https://aclanthology.org/2015.jeptalnrecital-r...   \n",
              "69524  https://aclanthology.org/2009.jeptalnrecital-c...   \n",
              "69528  https://aclanthology.org/2008.jeptalnrecital-c...   \n",
              "\n",
              "                                          title_fr_clean  \n",
              "100    Annoter en constituants pour évaluer des analy...  \n",
              "572    Intégration de contexte global par amorçage po...  \n",
              "576    Utilisation de Représentations Distribuées de ...  \n",
              "581    Apprendre des représentations jointes de mots ...  \n",
              "604    Méthode faiblement supervisée pour l'extractio...  \n",
              "...                                                  ...  \n",
              "69517  La complémentarité des approches manuelle et a...  \n",
              "69519  Alignement multimodal de ressources éducatives...  \n",
              "69523  État de l'art : analyse des conversations écri...  \n",
              "69524  Chaîne de traitement linguistique : du repérag...  \n",
              "69528  Vers l'identification et le traitement des act...  \n",
              "\n",
              "[1501 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ea2ff3e-ddde-41b8-9b8b-956fa32ffdbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acl_id</th>\n",
              "      <th>title_fr</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "      <th>publisher</th>\n",
              "      <th>url</th>\n",
              "      <th>title_fr_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>2004.jeptalnrecital-long.32</td>\n",
              "      <td>Annoter en constituants pour évaluer des analy...</td>\n",
              "      <td>Cet article présente l'annotation en constitua...</td>\n",
              "      <td>Cet article présente l'annotation en constitua...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2004.jeptalnrecital-l...</td>\n",
              "      <td>Annoter en constituants pour évaluer des analy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>2018.jeptalnrecital-long.10</td>\n",
              "      <td>Intégration de contexte global par amorçage po...</td>\n",
              "      <td>Les approches neuronales obtiennent depuis plu...</td>\n",
              "      <td>Les approches neuronales obtiennent depuis plu...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2018.jeptalnrecital-l...</td>\n",
              "      <td>Intégration de contexte global par amorçage po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>2018.jeptalnrecital-court.17</td>\n",
              "      <td>Utilisation de Représentations Distribuées de ...</td>\n",
              "      <td>L'identification des entités nommées dans un t...</td>\n",
              "      <td>L'identification des entités nommées dans un t...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2018.jeptalnrecital-c...</td>\n",
              "      <td>Utilisation de Représentations Distribuées de ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>2017.jeptalnrecital-long.13</td>\n",
              "      <td>Apprendre des représentations jointes de mots ...</td>\n",
              "      <td>1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...</td>\n",
              "      <td>1 ,cea.fr 2 ,limsi.fr 3 } RÉSUMÉ La désambiguï...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2017.jeptalnrecital-l...</td>\n",
              "      <td>Apprendre des représentations jointes de mots ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>2015.jeptalnrecital-long.9</td>\n",
              "      <td>Méthode faiblement supervisée pour l'extractio...</td>\n",
              "      <td>La détection d'opinion ciblée a pour but d'att...</td>\n",
              "      <td>La détection d'opinion ciblée a pour but d'att...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-l...</td>\n",
              "      <td>Méthode faiblement supervisée pour l'extractio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69517</th>\n",
              "      <td>2009.jeptalnrecital-court.21</td>\n",
              "      <td>La complémentarité des approches manuelle et a...</td>\n",
              "      <td>Les ressources lexicales sont essentielles pou...</td>\n",
              "      <td>Les ressources lexicales sont essentielles pou...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2009.jeptalnrecital-c...</td>\n",
              "      <td>La complémentarité des approches manuelle et a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69519</th>\n",
              "      <td>2015.jeptalnrecital-recital.5</td>\n",
              "      <td>Alignement multimodal de ressources éducatives...</td>\n",
              "      <td>Cet article présente certaines questions de re...</td>\n",
              "      <td>Cet article présente certaines questions de re...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-r...</td>\n",
              "      <td>Alignement multimodal de ressources éducatives...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69523</th>\n",
              "      <td>2015.jeptalnrecital-recital.6</td>\n",
              "      <td>État de l'art : analyse des conversations écri...</td>\n",
              "      <td>Le développement du Web 2.0 et le processus de...</td>\n",
              "      <td>Le développement du Web 2.0 et le processus de...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2015.jeptalnrecital-r...</td>\n",
              "      <td>État de l'art : analyse des conversations écri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69524</th>\n",
              "      <td>2009.jeptalnrecital-court.11</td>\n",
              "      <td>Chaîne de traitement linguistique : du repérag...</td>\n",
              "      <td>Cet article présente la chaîne de traitement l...</td>\n",
              "      <td>Cet article présente la chaîne de traitement l...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2009.jeptalnrecital-c...</td>\n",
              "      <td>Chaîne de traitement linguistique : du repérag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69528</th>\n",
              "      <td>2008.jeptalnrecital-court.16</td>\n",
              "      <td>Vers l'identification et le traitement des act...</td>\n",
              "      <td>Il peut être difficile d'attribuer une seule v...</td>\n",
              "      <td>Il peut être difficile d'attribuer une seule v...</td>\n",
              "      <td>ATALA</td>\n",
              "      <td>https://aclanthology.org/2008.jeptalnrecital-c...</td>\n",
              "      <td>Vers l'identification et le traitement des act...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1501 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ea2ff3e-ddde-41b8-9b8b-956fa32ffdbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5ea2ff3e-ddde-41b8-9b8b-956fa32ffdbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5ea2ff3e-ddde-41b8-9b8b-956fa32ffdbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2b115421-5b14-4f94-a21e-7a3d9ef6582d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b115421-5b14-4f94-a21e-7a3d9ef6582d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2b115421-5b14-4f94-a21e-7a3d9ef6582d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3c13abc3-4ce4-4044-813f-4ed084a6c599\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_netoy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3c13abc3-4ce4-4044-813f-4ed084a6c599 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_netoy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_netoy",
              "summary": "{\n  \"name\": \"data_netoy\",\n  \"rows\": 1501,\n  \"fields\": [\n    {\n      \"column\": \"acl_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"2006.jeptalnrecital-poster.10\",\n          \"2006.jeptalnrecital-recitalposter.2\",\n          \"F14-2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_fr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"Reconnaissance de la m\\u00e9trique des po\\u00e8mes arabes par les r\\u00e9seaux de neurones artificiels\",\n          \"Une approche g\\u00e9ometrique pour la mod\\u00e9lisation des lexiques en langues sign\\u00e9es\",\n          \"Multilingual Summarization Experiments on English, Arabic and French (R\\u00e9sum\\u00e9 Automatique Multilingue Exp\\u00e9rimentations sur l'Anglais, l'Arabe et le Fran\\u00e7ais) [in French]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1458,\n        \"samples\": [\n          \"La portabilit\\u00e9 entre les langues des syst\\u00e8mes de reconnaissance d'entit\\u00e9s nomm\\u00e9es est co\\u00fbteuse en termes de temps et de connaissances linguistiques requises. L'adaptation des syst\\u00e8mes symboliques souffrent du co\\u00fbt de d\\u00e9veloppement de nouveaux lexiques et de la mise \\u00e0 jour des r\\u00e8gles contextuelles. D'un autre c\\u00f4t\\u00e9, l'adaptation des syst\\u00e8mes statistiques se heurtent au probl\\u00e8me du co\\u00fbt de pr\\u00e9paration d'un nouveau corpus d'apprentissage. Cet article \\u00e9tudie l'int\\u00e9r\\u00eat et le co\\u00fbt associ\\u00e9 pour porter un syst\\u00e8me existant de reconnaissance d'entit\\u00e9s nomm\\u00e9es pour du texte bien form\\u00e9 vers une autre langue. Nous pr\\u00e9sentons une m\\u00e9thode peu co\\u00fbteuse pour porter un syst\\u00e8me symbolique d\\u00e9di\\u00e9 au fran\\u00e7ais vers l'anglais. Pour ce faire, nous avons d'une part traduit automatiquement l'ensemble des lexiques de mots d\\u00e9clencheurs au moyen d'un dictionnaire bilingue. D'autre part, nous avons manuellement modifi\\u00e9 quelques r\\u00e8gles de mani\\u00e8re \\u00e0 respecter la syntaxe de la langue anglaise. Les r\\u00e9sultats exp\\u00e9rimentaux sont compar\\u00e9s \\u00e0 ceux obtenus avec un syst\\u00e8me de r\\u00e9f\\u00e9rence d\\u00e9velopp\\u00e9 pour l'anglais.\",\n          \"Ces derni\\u00e8res d\\u00e9cennies, notre regard sur les donn\\u00e9es lexicales informatis\\u00e9es a beaucoup \\u00e9volu\\u00e9. D'abord annexe lexicale d'une grammaire ou d'une application, les dictionnaires d'application sont devenues bases lexicales dans lesquelles s'agr\\u00e9geaient les donn\\u00e9es de diff\\u00e9rents modules. L'effort suivant s'est concentr\\u00e9 dans la normalisation du format, avec notamment un mouvement massif vers le tout XML. Le travail de normalisation des structures des lexiques a suivi ensuite. Mais, alors que les normes restent structurellement proches des dictionnaires originaux (vus comme une collection d'entr\\u00e9es organis\\u00e9es de mani\\u00e8re arborescentes), ont \\u00e9merg\\u00e9 des mod\\u00e8les de lexiques pens\\u00e9s comme des graphes. Parall\\u00e8lement, les travaux dans le domaine du Web S\\u00e9mantique nous ont donn\\u00e9 les moyens de repr\\u00e9senter, manipuler et surtout partager nos ressources lexicales. En adoptant une repr\\u00e9sentation en RDF (Resource Description Framework), ainsi que l'approche des donn\\u00e9es li\\u00e9es ouverte (Linked Open Data), nous avons enfin les moyens de lier, fusionner, parcourir l'ensemble des ressources lexicales comme s'il ne s'agissait que d'une seule ressource. Dans cette pr\\u00e9sentation, en m'appuyant sur les travaux r\\u00e9alis\\u00e9s dans le cadre des projets Papillon, LexALP et DBnary, j'essaierai de montrer en quoi, au del\\u00e0 de l'effet de mode actuel, l'utilisation du format des donn\\u00e9s li\\u00e9es ouvertes, est l'\\u00e9tape suivante naturelle dans notre \\u00e9tude du lexique.\",\n          \"Le contr\\u00f4le des hypoth\\u00e8ses concurrentes g\\u00e9n\\u00e9r\\u00e9es par les diff\\u00e9rents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgr\\u00e9 de nombreuses avanc\\u00e9es en terme de robustesse. Nous pr\\u00e9sentons dans cet article une m\\u00e9thodologie g\\u00e9n\\u00e9rique de contr\\u00f4le exploitant des techniques issues de l'aide multicrit\\u00e8re \\u00e0 la d\\u00e9cision. \\u00c0 partir de l'ensemble des crit\\u00e8res de comparaison disponibles et la formalisation des pr\\u00e9f\\u00e9rences d'un expert, l'approche propos\\u00e9e \\u00e9value la pertinence relative des diff\\u00e9rents objets linguistiques g\\u00e9n\\u00e9r\\u00e9s et conduit \\u00e0 la mise en place d'une action de contr\\u00f4le appropri\\u00e9e telle que le filtrage, le classement, le tri ou la propagation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1495,\n        \"samples\": [\n          \"Dans le cadre de notre projet de recherche, qui a pour but l'impl\\u00e9mentation d'un outil de simplification des emplois sp\\u00e9cialis\\u00e9s de verbes dans des corpus m\\u00e9dicaux \\u00e0 partir de l'analyse syntaxico-s\\u00e9mantique de ces verbes en contexte, nous proposons une analyse de quelques approches et travaux qui ont pour objet principal la description du verbe dans les trois domaines de recherche \\u00e0 l'interface desquels se situe notre projet : linguistique, TAL et terminologie. Nous d\\u00e9crivons plus particuli\\u00e8rement les travaux qui peuvent avoir une incidence sur notre \\u00e9tude. Cet \\u00e9tat de l'art nous permet de mieux conna\\u00eetre le cadre th\\u00e9orique dans lequel s'int\\u00e8gre notre projet de recherche et d'avoir les rep\\u00e8res et r\\u00e9f\\u00e9rences susceptibles de contribuer \\u00e0 sa r\\u00e9alisation. Introduction Contexte g\\u00e9n\\u00e9ral L'int\\u00e9r\\u00eat port\\u00e9 au verbe change selon que l'on se situe dans le domaine de la linguistique, de la terminologie ou celui du Traitement Automatique des Langues (TAL). En effet, selon leurs objectifs respectifs, chacune de ces disciplines octroie au verbe une place diff\\u00e9rente reconnaissable \\u00e0 travers l'importance qui lui est donn\\u00e9e dans les diverses \\u00e9tudes et les diff\\u00e9rents cadres th\\u00e9oriques propres au domaine concern\\u00e9. Quelles sont les \\u00e9clairages propos\\u00e9s par les diff\\u00e9rentes approches (linguistique, terminologie et TAL) qui prennent le verbe comme objet d'\\u00e9tude ? Comment-est ce que le verbe est abord\\u00e9 dans ces travaux ? Est-il trait\\u00e9 au m\\u00eame titre que les autres cat\\u00e9gories grammaticales en l'occurrence le nom ? En quoi est-ce que le verbe et sa structure argumentale peuvent-ils \\u00eatre utiles en vue de la simplification des textes sp\\u00e9cialis\\u00e9s ? Telles sont les questions auxquelles nous allons essayer de r\\u00e9pondre dans ce travail qui a pour objectif de dresser un \\u00e9tat de l'art de diff\\u00e9rents mod\\u00e8les de descriptions du verbe dans les trois disciplines concern\\u00e9es. Travail envisag\\u00e9 Le projet que nous entreprenons a pour objectif de proposer une m\\u00e9thode de simplification de textes m\\u00e9dicaux, \\u00e0 partir d'une analyse syntaxico-s\\u00e9mantique des verbes en contexte. Au terme de ce travail, nous souhaitons impl\\u00e9menter un outil de simplification des textes \\u00e9crits en fran\\u00e7ais (et \\u00e9ventuellement en anglais), sp\\u00e9cialis\\u00e9s en cardiologie (ou en d'autres domaines m\\u00e9dicaux). L'outil devra rep\\u00e9rer les emplois verbaux peu communs au discours des patients et devra ensuite proposer des emplois s\\u00e9mantiquement similaires, mais plus adapt\\u00e9s au niveau de sp\\u00e9cialisation de ces utilisateurs. La m\\u00e9thode propos\\u00e9e est bas\\u00e9e sur diff\\u00e9rentes hypoth\\u00e8ses. En effet, nous pensons que le pr\\u00e9dicat verbal peut \\u00eatre un excellent ORNELLA WANDJI point de d\\u00e9part pour cerner la s\\u00e9mantique des textes sp\\u00e9cialis\\u00e9s puisqu'il sert \\u00e0 exprimer l'expertise port\\u00e9e par les mots qui l'entourent dans la phrase (L'Homme & Bodson, 1997) . Par cons\\u00e9quent, nous consid\\u00e9rons la structure argumentale du verbe comme une importante source d'informations sur les propri\\u00e9t\\u00e9s s\\u00e9mantique et syntaxique du verbe. Ce projet de recherche s'inscrit dans le cadre de la simplification des textes sp\\u00e9cialis\\u00e9s. Il s'agit d'une t\\u00e2che du TAL qui consiste \\u00e0 cibler et \\u00e0 simplifier automatiquement les \\u00e9l\\u00e9ments, qui emp\\u00eachent la compr\\u00e9hension ais\\u00e9e d'un texte, afin de faciliter l'acc\\u00e8s au contenu de ce texte. Les travaux existants se focalisent sur la simplification syntaxique (Brouwers et al., 2014) , la simplification lexicale (Elhadad, 2006; Leroy et al., 2012) , la combinaison des fonctions lexicales, grammaticales, syntaxiques et discursives (Heilman et al., 2007 (Heilman et al., , 2008;; Pitler & Nenkova, 2008) , les caract\\u00e9ristiques de surface des textes (nombre de caract\\u00e8res et syllabes par mot), la capitalisation, la ponctuation et les ellipses (Tapas & Orr, 2009) , ou la mod\\u00e9lisation statistique de la langue (Thompson & Callan, 2004) . L'approche que nous proposons se situe \\u00e0 mi-chemin entre la simplification lexicale et la simplification syntaxique et vise \\u00e0 r\\u00e9duire les difficult\\u00e9s de compr\\u00e9hension des textes m\\u00e9dicaux fortement sp\\u00e9cialis\\u00e9s \\u00e0 travers la simplification des constructions verbales. \\u00c0 notre connaissance, il n'existe pas de travaux en simplification de textes autant orient\\u00e9s sur l'analyse des verbes et de leurs argumentales. Une \\u00e9tude comparative du fonctionnement des verbes dans des textes de corpus m\\u00e9dicaux r\\u00e9dig\\u00e9s par des experts et des non-experts en m\\u00e9d\\u00e9cine a permis d'observer que les verbes ont tendance \\u00e0 s'entourer d'arguments fortement sp\\u00e9cialis\\u00e9s dans les \\u00e9crits des experts, rendant parfois leur compr\\u00e9hension difficile pour les non-experts (Wandji Tchami et al., 2013) . Notre travail de recherche vient donner une suite \\u00e0 cette observation. L'objectif principal \\u00e9tant d'am\\u00e9liorer certains aspects de la m\\u00e9thode (l'annotation automatique des arguments, l'analyse des verbes) et de la d\\u00e9velopper davantage, en y int\\u00e9grant un travail de simplification. Le travail pr\\u00e9sent\\u00e9 ici a pour objectif de nous aider \\u00e0 mieux cerner le cadre th\\u00e9orique dans lequel s'inscrit le projet de recherche que nous envisageons de r\\u00e9aliser, en nous donnant une id\\u00e9e pr\\u00e9cise des travaux et outils existants, centr\\u00e9s sur le verbe et susceptibles de nous aider pour r\\u00e9alisation du travail envisag\\u00e9. Il est organis\\u00e9 autour de 4 grandes parties dont les trois premi\\u00e8res sont consacr\\u00e9es \\u00e0 l'exploration des travaux portant sur le pr\\u00e9dicat verbal, respectivement en linguistique (section 2), terminologie (section 3) et TAL (section 4). Dans la derni\\u00e8re partie (section 5), nous faisons une discussion de l'impact que les travaux de l'\\u00e9tat de l'art peuvent avoir sur la r\\u00e9alisation de notre projet de recherche et nous abordons les perspectives de travail que nous envisageons d'explorer. Les approches linguistiques d\\u00e9di\\u00e9es au verbe En linguistique, de nombreux cadres th\\u00e9oriques placent le verbe au coeur de leurs travaux. Nous nous attardons plus particuli\\u00e8rement sur les cadres th\\u00e9oriques qui s'int\\u00e9ressent au verbe en tant qu'\\u00e9l\\u00e9ment r\\u00e9gisseur (c'est-\\u00e0-dire un \\u00e9l\\u00e9ment dont la r\\u00e9alisation syntaxique et s\\u00e9mantique d\\u00e9pend grandement de la pr\\u00e9sence d'autres constituants qui lui sont subordonn\\u00e9s) et d\\u00e9crivent son rapport avec les autres constituants de la phrase. Il existe diverses approches de description du verbe, mais nous ne sommes pas en mesure de fournir une pr\\u00e9sentation exhaustive de toutes les approches th\\u00e9oriques existantes. Nous nous limitons \\u00e0 celles qui servent de bases \\u00e0 la r\\u00e9alisation de diff\\u00e9rentes t\\u00e2ches du TAL (sections 4.2 et 4.1), et \\u00e0 la conception des ressources (section 4.3). Le noeud verbal au coeur de la syntaxe structurale La syntaxe structurale (Tesni\\u00e8re, 1959) est la premi\\u00e8re th\\u00e9orie \\u00e0 avoir mis le verbe au centre de la phrase. En syntaxe structurale, l'ensemble des mots d'une phrase constitue une v\\u00e9ritable hi\\u00e9rarchie au sein de laquelle les constituants sont li\\u00e9s les uns aux autres par des liens de d\\u00e9pendance. La phrase, encore appel\\u00e9e stemma, est d\\u00e9crite comme \\u00e9tant un sch\\u00e9ma arborescent, ou un ensemble de noeuds. Le noeud quant \\u00e0 lui d\\u00e9signe un ensemble constitu\\u00e9 d'un r\\u00e9gissant et de tous ses subordonn\\u00e9s. Dans cette configuration, le noeud central correspond en g\\u00e9n\\u00e9ral au noeud verbal. Le verbe, \\u00e9tant au centre du noeud verbal, est par cons\\u00e9quent au coeur de la phrase. Il est pour ainsi dire le r\\u00e9gissant de toute la phrase. La notion de noeud verbal est d\\u00e9finie \\u00e0 travers une m\\u00e9taphore du drame : \\u00ab le noeuds verbale ... exprime un tout petit drame. Comme un drame, ... il comporte obligatoirement un proc\\u00e8s et plus souvent des acteurs et des circonstants \\u00bb. C'est dans cette optique que cette approche postule l'existence des actants ou participants au proc\\u00e8s verbal (Tesni\\u00e8re, 1959) La th\\u00e9orie des cadres s\\u00e9mantiques Encore appel\\u00e9e Frame semantics, la s\\u00e9mantique des cadres est une approche qui remonte aux ann\\u00e9es 1980. Elle est une extension de la grammaire des Cas (Fillmore, 1968) , qui \\u00e9voquait d\\u00e9j\\u00e0 l'existence des r\\u00f4les s\\u00e9mantiques (agent, lieu, etc.) dans la structure syntaxique profonde du verbe. La s\\u00e9mantique des cadres (Fillmore, 1982) vise \\u00e0 l'origine \\u00e0 faciliter la compr\\u00e9hension des textes. Son principal objectif est de d\\u00e9crire la syntaxe et la s\\u00e9mantique des unit\\u00e9s lexicales (noms, adjectifs, verbes). L'id\\u00e9e principale de Fillmore est que le sens d'un mot ne peut \\u00eatre interpr\\u00e9t\\u00e9 que si l'on a acc\\u00e8s aux informations (linguistiques, extralinguistiques ou encyclop\\u00e9diques) essentielles faisant r\\u00e9f\\u00e9rence \\u00e0 ce mot. Ces informations peuvent \\u00eatre accessibles gr\\u00e2ce \\u00e0 un frame ou cadre au sein duquel les unit\\u00e9s lexicales sont organis\\u00e9es. Le cadre est d\\u00e9fini comme un sc\\u00e9nario, un sch\\u00e9ma ou une structure conceptuelle qui sous-tend l'utilisation d'un item lexical ainsi que son interpr\\u00e9tation (Fontenelle, 2009) . Il d\\u00e9crit une situation particuli\\u00e8re ainsi que les participants Frame elements (FE) qui peuvent \\u00eatre obligatoires (core elements) ou facultatifs (non core elements). Un cadre est \\u00e9voqu\\u00e9 par une unit\\u00e9 lexicale (LU). Par exemple, le frame de la transaction commerciale (Fillmore, 1976) a plusieurs unit\\u00e9s \\u00e9vocatrices : acheter, vendre, payer, recup\\u00e9rer et plusieurs participants : obligatoires (VENDEUR, ARGENT, BIEN, ACHETEUR) et facultatifs (MOYEN), etc. Lorsque l'unit\\u00e9 \\u00e9vocatrice du cadre est un verbe, l'analyse est focalis\\u00e9e sur les arguments de ce dernier qui repr\\u00e9sentent les \\u00e9l\\u00e9ments du cadre. Les classifications de verbes La classification des verbes (anglais) selon Levin Beth Levin (Levin, 1993) propose une classification lexico-s\\u00e9mantique de verbes anglais \\u00e0 partir d'une analyse de leur fonctionnement (syntaxe, classe s\\u00e9mantique des arguments s\\u00e9lectionn\\u00e9s, etc.). Les verbes qui affichent un ensemble d'alternances (de diath\\u00e8ses ou frames) identiques ou similaires dans la r\\u00e9alisation de leurs structures argumentales sont suppos\\u00e9s partager certains \\u00e9l\\u00e9ments de sens et, de ce fait, sont regroup\\u00e9s dans une classe s\\u00e9mantiquement homog\\u00e8ne. L'alternance de diath\\u00e8ses (la relation entre deux r\\u00e9alisations de surface d'un m\\u00eame pr\\u00e9dicat), qui est le principal crit\\u00e8re d'identification des classes verbales dans cette approche, est appuy\\u00e9e par des propri\\u00e9t\\u00e9s suppl\\u00e9mentaires li\\u00e9es \\u00e0 la souscat\\u00e9gorisation, \\u00e0 la morphologie et aux verbes ayant un s\\u00e9mantisme complexe. \\u00c0 partir de ces crit\\u00e8res, la classification couvre 3 024 verbes, 4 186 sens, 240 classes de verbes construites autour de 79 alternances. Par exemple, la classe des pr\\u00e9dicats d\\u00e9notant une configuration spatiale contient les verbes suivants : balance, bend, bow, crouch, dangle, flop, fly, hang, hover, jut, kneel, lean, lie, loll, loom, lounge, nestle, open, perch, plop, project, protude, recline, rest, rise, roost, sag, sit, slope, slouch, slump, sprawl, squat, stand, stoop, straddle, swing, tilt, tower (Levin, 1993) . Une extension substantielle de cette classification int\\u00e8gre 57 nouvelles classes pour les verbes qui n'ont pas \\u00e9t\\u00e9 couverts initiallement (Korhonen & Briscoe, 2004) . Parmi les nouvelles classes, FORCE class regroupe les verbes tels que manipulate, pressure, force. Les classes d'objets de Gaston Gross Une classe d'objets est un \\u00ab ensemble de substantifs, s\\u00e9mantiquement homog\\u00e8nes, qui d\\u00e9termine une rupture d'interpr\\u00e9tation d'un pr\\u00e9dicat donn\\u00e9, en d\\u00e9limitant un emploi sp\\u00e9cifique \\u00bb (Gross, 2008). En d'autres termes, les classes d'objets d\\u00e9terminent l'interpr\\u00e9tation donn\\u00e9e d'un pr\\u00e9dicat parmi d'autres possibles. Elles sont induites par les pr\\u00e9dicats (verbes et adjectifs) et permettent d'identifier en contexte les mots avec lesquels ils entretiennent une relation conceptuelle telle que la synonymie, l'antonymie, etc. Ces entit\\u00e9s sont construites sur des bases syntaxiques et concernent particuli\\u00e8rement les compl\\u00e9ments qui apportent beaucoup plus d'informations que le sujet dans l'interpr\\u00e9tation d'un pr\\u00e9dicat (Gross, 2012) . Par exemple, la phrase vous suivez n'est pas assez pr\\u00e9cise, ce qui rend son interpr\\u00e9tation difficile. Par contre, si l'on y ajoute un compl\\u00e9ment, l'interpr\\u00e9tation sera plus ais\\u00e9e et la signification du verbe sera plus transparente. Ainsi, dans la phrase vous suivez ce chemin, l'objet chemin peut \\u00eatre remplac\\u00e9 par un autre substantif comme route, rue, voie, sentier et le verbe garde le m\\u00eame sens. Ces substantifs peuvent donc \\u00eatre consid\\u00e9r\\u00e9s comme appartenant \\u00e0 une m\\u00eame classe d'objets, celle de <voies>. Par contre, si on remplace chemin par le mot cours, on est face \\u00e0 un autre emploi du verbe car cours appartient \\u00e0 une autre classe d'objets, appel\\u00e9e <enseignements>. Elle contient les mots comme s\\u00e9minaire, stage, formation, cycle \\u00e9tudes, etc. Le principal int\\u00e9r\\u00eat des classes d'objets est de rendre compte des diff\\u00e9rents emplois des pr\\u00e9dicats, en d\\u00e9terminant leurs sch\\u00e9mas d'arguments et en rattachant \\u00e0 ceux-ci un ensemble de propri\\u00e9t\\u00e9s qui les caract\\u00e9risent (Gross, 2008) . ORNELLA WANDJI Lexique-Grammaire des verbes fran\\u00e7ais Le lexique-grammaire des verbes du fran\\u00e7ais (Gross, 1975) est un dictionnaire syntaxique \\u00e9lectronique t\\u00e9l\\u00e9chargeable 1 . Il est organis\\u00e9 en plusieurs tables, chacune regroupant les verbes du lexique qui ont un fonctionnement comparable : constructions types, distribution des actants, s\\u00e9mantique, etc. Chaque table comprend un ensemble de propri\\u00e9t\\u00e9s, et un codage qui pr\\u00e9cise si l'\\u00e9l\\u00e9ment a ou non cette propri\\u00e9t\\u00e9. Chaque entr\\u00e9e d'une table contient les informations suivantes : l'\\u00e9l\\u00e9ment vedette, une construction type dans laquelle il peut appara\\u00eetre, et des constructions associ\\u00e9es \\u00e0 cette construction type. Les diff\\u00e9rents emplois des verbes, \\u00e9num\\u00e9r\\u00e9s dans les tables, sont d\\u00e9crits gr\\u00e2ces \\u00e0 des propri\\u00e9t\\u00e9s structurelles, distributionnelles et s\\u00e9mantiques. Par exemple, les tables de constructions sans compl\\u00e9ments pr\\u00e9positionnels contiennent des constructions types, parmi lesquelles N 0 V et N 0 V N 1 . La construction N 0 V accueille les verbes tels que pleuvoir, b\\u00eatifier, bouillir, pisser et selon le verbe, elle peut accepter un N 0 humain (Luc b\\u00eatifie), non humain (l'eau bout), impersonnel (il pleut), et le verbe peut \\u00eatre modifi\\u00e9 par un adverbe (\\u00e7a ne pisse pas loin) (Leclere, 1990) . Le verbe dans les travaux en terminologie Les entit\\u00e9s nominales ont longtemps occup\\u00e9 la place centrale dans les travaux sur les langues de sp\\u00e9cialit\\u00e9 au d\\u00e9triment des autres parties du discours, plus particuli\\u00e8rement des verbes, mis \\u00e0 l'\\u00e9cart pour diverses raisons. En effet, les travaux en terminologie se focalisent la plupart du temps sur la description des concepts, ou des entit\\u00e9s nominales (particuli\\u00e8rement les noms) et la mise au jour des relations qu'elles partagent (genre-esp\\u00e8ce, partie-tout, etc.). L'un des motifs principaux \\u00e9nonc\\u00e9s justifiant l'exclusion du verbe est la place accord\\u00e9e aux objets et \\u00e0 leurs d\\u00e9nominations dans l'approche de W\\u00fcster (W\\u00fcster, 1985) . Cette situation trouve \\u00e9galement une explication dans le fait que les entit\\u00e9s nominales sont g\\u00e9n\\u00e9ralement utilis\\u00e9es pour le d\\u00e9veloppement des terminologies, ontologies, th\\u00e9saurus, glossaires, ou des vocabulaires. Ce constat s'explique \\u00e9galement par les besoins croissants des applications : l'indexation et l'extraction d'informations sont des t\\u00e2ches typiquement bas\\u00e9es sur les entit\\u00e9s nominales. Pour ces raisons, la plupart des approches th\\u00e9oriques et m\\u00e9thodologiques sont adapt\\u00e9es aux entit\\u00e9s nominales. N\\u00e9anmoins, quelques travaux s'inspirant de la s\\u00e9mantique lexicale s'int\\u00e9ressent aux verbes et \\u00e0 leur mode de fonctionnement dans les domaines sp\\u00e9cialis\\u00e9s. Ces travaux montrent que l'\\u00e9tude du verbe est quasi indispensable dans le cadre des activit\\u00e9s comme l'extraction d'informations (Tateisi et al., 2004) , la conception des dictionnaires terminographiques (Tellier, 2008) ou encore la traduction sp\\u00e9cialis\\u00e9e (Pimentel, 2011) . Les structures argumentales des verbes peuvent \\u00e9galement servir pour la d\\u00e9tection automatique des relations s\\u00e9mantiques (Massimiliano et al., 2008) . Nous parlerons de deux approches d'analyse du verbe terminologique : l'approche conceptuelle (section 3.1) et l'approche lexico-s\\u00e9mantique (section 3.2). L'approche conceptuelle Le principe de l'approche conceptuelle stipule que l'on ne s'int\\u00e9resse au verbe que s'il a l'aptitude de d\\u00e9signer un \\u00ab concept d'activit\\u00e9 \\u00bb, c'est-\\u00e0-dire une activit\\u00e9 (L'Homme, 2012). Telle est la condition qui d\\u00e9termine l'int\\u00e9gration des verbes dans des ressources terminologiques. Autrement dit, le verbe ne peut \\u00eatre consid\\u00e9r\\u00e9 comme terme que s'il est fortement assimilable \\u00e0 un nom sur le plan conceptuel. Rey d\\u00e9finit clairement le statut du verbe selon la perspective conceptuelle en ces termes : \\u00ab la terminologie ne s'int\\u00e9resse aux signes (mots et unit\\u00e9s plus grandes que le mot) qu'en tant qu'ils fonctionnent comme des noms d\\u00e9notant des objets et comme des \\u00ab indicateurs de notions \\u00bb (de concepts) et dans cette optique, les verbes sont des noms de processus, d'actions \\u00bb (Rey, 1979) . Cette conception justifie en partie la discrimination observ\\u00e9e entre les parties du discours trait\\u00e9es dans un dictionnaire sp\\u00e9cialis\\u00e9. En g\\u00e9n\\u00e9ral, on y compte tr\\u00e8s peu de verbes et d'adjectifs, mais beaucoup d'entit\\u00e9s nominales. Les r\\u00e9sultats d'une \\u00e9tude portant sur la pr\\u00e9sence des verbes dans les dictionnaires de sp\\u00e9cialit\\u00e9 \\u00e9valuent \\u00e0 2,44% (entre 0 et 4 verbes par dictionnaire) la moyenne d'apparition des verbes dans quatre dictionnaires terminologiques (L'Homme, 2003) . L'approche conceptuelle a d\\u00e9bouch\\u00e9 de nos jours sur une d\\u00e9marche conceptuelle, incarn\\u00e9e par les ontologies, qui permet de distinguer les concepts d'activit\\u00e9, exprim\\u00e9s par les noms ou par les verbes, dans les domaines de sp\\u00e9cialit\\u00e9. Ainsi, dans le domaine m\\u00e9dical par exemple, les verbes tels que traiter, observer et activer peuvent devenir terminologiques puisqu'ils permettent de rendre compte des notions comme traitement de la maladie, observation du patient et activation des cellules (L'Homme, 2012). L'approche lexico-s\\u00e9mantique La s\\u00e9mantique lexicale est le cadre th\\u00e9orique qui a montr\\u00e9 l'importance de la structure argumentale du verbe et du r\\u00e9seau lexical auquel le verbe appartient. Dans ce cadre, la caract\\u00e9risation de la nature sp\\u00e9cialis\\u00e9e du verbe est bas\\u00e9e sur la description de sa structure argumentale ou son appartenance \\u00e0 un ou plusieurs r\\u00e9seaux lexicaux, (morpho-)s\\u00e9mantiques ou paradigmatiques. Ces t\\u00e2ches reposent sur l'observation et l'analyse des diff\\u00e9rentes occurrences du verbe en corpus. La structure argumentale L'analyse de la structure argumentale du verbe peut avoir pour but de d\\u00e9montrer sa nature terminologique. En effet, la nature pr\\u00e9dicative du verbe fait qu'il a besoin des \\u00e9l\\u00e9ments qu'il r\\u00e9git pour la r\\u00e9alisation de son sens. Une \\u00e9tude propose de prendre en consid\\u00e9ration la nature des arguments du verbe qui d\\u00e9termine son degr\\u00e9 de sp\\u00e9cialisation (L'Homme, 1998). Ce raisonnement illustre l'hypoth\\u00e8se selon laquelle le verbe n'est pas sp\\u00e9cialis\\u00e9 par lui m\\u00eame, mais gr\\u00e2ce \\u00e0 la prise en compte de sa structure argumentale (L'Homme, 2012). C'est ce crit\\u00e8re qui permet d'admettre installer comme verbe sp\\u00e9cialis\\u00e9 dans l'exemple suivant : L'utilisateur installe la nouvelle version du traitement de texte sur son PC. Dans cette phrase, les termes (utilisateur, version, PC) qui repr\\u00e9sentent les t\\u00eates des arguments du verbe appartiennent au domaine de l'informatique. Par cons\\u00e9quent, installer peut \\u00eatre consid\\u00e9r\\u00e9 comme verbe terminologique dans ce domaine. L'analyse des arguments des verbes constitue \\u00e9galement un crit\\u00e8re de poids chez (Tellier, 2008) qui y trouve un moyen de s\\u00e9lection des verbes, \\u00e0 partir d'un corpus sp\\u00e9cialis\\u00e9 relevant du domaine de l'infectiologie, repr\\u00e9sentant de bons candidats termes \\u00e0 ajouter dans un dictionnaire sp\\u00e9cialis\\u00e9. Ce crit\\u00e8re est \\u00e9galement utilis\\u00e9 dans d'autres travaux (Lerat, 2002; Pimentel, 2011) . Cependant, la caract\\u00e9risation des arguments du pr\\u00e9dicat verbal n'a pas pour unique but l'identification des verbes terminologiques. D'autres objectifs peuvent \\u00eatre poursuivis : l'extraction d'informations dans les corpus sp\\u00e9cialis\\u00e9s du domaine de la biologie mol\\u00e9culaire (Tateisi et al., 2004) , l'\\u00e9laboration d'un dictionnaire juridique portugaisanglais (Pimentel, 2011) , l'analyse contrastive des corpus m\\u00e9dicaux de niveaux de sp\\u00e9cialisation diff\\u00e9rents (expert vs profane) (Wandji Tchami et al., 2013) . Le r\\u00e9seau lexical Outre la nature des actants, d'autres param\\u00e8tres peuvent \\u00eatre pris en compte par les chercheurs lors du rep\\u00e9rage des verbes terminologiques. L'un de ces param\\u00e8tres, qui revient tr\\u00e8s souvent, est le lien qu'un verbe peut avoir avec un nom. Ainsi, si le nom est terminologique, et si le verbe est s\\u00e9mantiquement et le plus souvent morphologiquement apparent\\u00e9 \\u00e0 celui-ci, alors, il est fort possible que le verbe soit sp\\u00e9cialis\\u00e9 lui aussi (L'Homme, 2012). Ce crit\\u00e8re s'observe avec les couples tels que d\\u00e9veloppementd\\u00e9velopper, t\\u00e9l\\u00e9chargementt\\u00e9l\\u00e9charger, rechauffementrechauffer, le verbe et le nom correspondant d\\u00e9signent tous les deux une activit\\u00e9. Cependant, il existe des cas o\\u00f9 le sens du verbe et celui du nom sont distincts malgr\\u00e9 le lien morphologique qui existe entre eux. C'est le cas du couple programmeprogrammer, o\\u00f9 le nom programme d\\u00e9signe le r\\u00e9sultat de l'activit\\u00e9 que d\\u00e9note le verbe programmer. Comme nous pouvons le constater, dans l'approche lexico-s\\u00e9mantique, les noms peuvent servir de point de d\\u00e9part \\u00e0 partir duquel les verbes sp\\u00e9cialis\\u00e9s sont identifi\\u00e9s en fonction des liens qu'ils partagent avec eux. C'est d'ailleurs cette m\\u00e9thode qui permet de retenir les verbes \\u00e9voluer, excr\\u00e9ter, infecter et s\\u00e9cr\\u00e9ter comme termes du domaine de l'infectiologie, de part leur parent\\u00e9 aux noms \\u00e9volution, excr\\u00e9tion, infection et s\\u00e9cr\\u00e9tion (Tellier, 2008) . Comme ces noms sont fortement sp\\u00e9cialis\\u00e9s dans ce domaine, les verbes correspondant h\\u00e9ritent de cette caract\\u00e9ristique. Toutefois, il est possible de d\\u00e9placer le point de d\\u00e9part de l'analyse vers le verbe. Cette technique peut permettre de d\\u00e9couvrir d'autres unit\\u00e9s reli\\u00e9es au verbe et d'\\u00e9largir ainsi le r\\u00e9seau lexical construit autour de ce dernier (L'Homme, 2012). Cette d\\u00e9marche a \\u00e9t\\u00e9 appliqu\\u00e9e lors de la conception du DicoInfo (Dictionnaire fondamental de l'informatique et de l'Internet), une base de donn\\u00e9es lexicales contenant des termes (les verbes y compris) fondamentaux du domaine de l'informatique et de l'internet (L'Homme, 2009). L'approche utilis\\u00e9e s'inspire grandement des principes th\\u00e9oriques et m\\u00e9thodologiques de la Lexicologie explicative et combinatoire (Mel'cuk et al., 1995) et permet de fournir pour chaque entr\\u00e9e diff\\u00e9rents types d'informations : la r\\u00e9alisation linguistique des actants, les liens lexicaux, les synonymes, les contextes d'apparition du terme, etc. Pour le verbe programmer par exemple, DicoInfo propose divers types d'unit\\u00e9s lexicales appartenant au r\\u00e9seau lexical notamment, programmation (action de programmer), programme (r\\u00e9sultat de l'action de programmer), informaticien (agent de l'action de programmer), langage (instrument utilis\\u00e9 pour programmer), logiciel (r\\u00e9sultat l'action de programmer), \\u00e9crire (synonyme de programmer), d\\u00e9velopper (synonyme de programmer), etc. Cet exemple permet d'observer que les mots rep\\u00e9r\\u00e9s sont li\\u00e9s au verbe par diff\\u00e9rentes relations exprim\\u00e9es de fa\\u00e7on implicite \\u00e0 travers de courtes gloses explicatives. Le verbe en TAL Le traitement des verbes dans le domaine du TAL s'appuie le plus souvent sur la caract\\u00e9risation de leur structure argumentale : la valence verbale (Eynde & Mertens, 2003) , les possibilit\\u00e9s combinatoires et les relations de d\\u00e9pendances (Marneffe et al., 2006) , les fonctions grammaticales et r\\u00f4les s\\u00e9mantiques des arguments (Gildea & Jurafsky, 2002) , la d\\u00e9sambigu\\u00efstation du sens des verbes (Ide & V\\u00e9ronis, 1998; Ye & Baldwin, 2006; Wagner et al., 2009; Brown et al., 2011) , l'acquisition de sch\\u00e9mas de sous-cat\\u00e9gorisation \\u00e0 partir de l'analyse automatique de gros corpus (Messiant et al., 2010) , etc. Dans cette section, nous nous focalisons sur deux types de travaux : l'\\u00e9tiquetage des r\\u00f4les s\\u00e9mantiques (section 4.1) et la d\\u00e9sambigu\\u00efsation du sens des verbes (section 4.2). Par la suite, nous faisons la description de quelques ressources d\\u00e9di\\u00e9es au verbe (section 4.3). \\u00c9tiquetage des r\\u00f4les s\\u00e9mantiques L'\\u00e9tiquetage des r\\u00f4les s\\u00e9mantiques (Gildea & Jurafsky, 2002; Palmer et al., 2005; Swier & Stevenson, 2004; Ye & Baldwin, 2006) , ou Semantic Role Labeling (SRL), est une t\\u00e2che du TAL qui consiste \\u00e0 identifier de fa\\u00e7on automatique les relations ou les r\\u00f4les s\\u00e9mantiques (agent, patient, recipient, etc.) que jouent les constituants d'une phrase dans un cadre s\\u00e9mantique donn\\u00e9. Cette t\\u00e2che est n\\u00e9cessaire pour la conception de diff\\u00e9rents types d'applications, et plus particuli\\u00e8rement celles qui touchent la compr\\u00e9hension et l'interpr\\u00e9tation de la langue. Il s'agit par exemple de syst\\u00e8mes de questionsr\\u00e9ponses (Miller et al., 1996) , d'extraction d'informations (Surdeanu et al., 2003) , de traduction automatique (Boas, 2002) , ou de r\\u00e9sum\\u00e9 automatique (Melli et al., 2005) . Les unit\\u00e9s pr\\u00e9dicatives (verbes, noms, adjectifs) occupent g\\u00e9n\\u00e9ralement le coeur des \\u00e9tudes qui concernent la SRL. En ce qui concerne le verbe, l'annotation consiste g\\u00e9n\\u00e9ralement \\u00e0 identifier dans la phrase les limites de ses arguments et \\u00e9ventuellement des circonstants, et ensuite de leur associer des r\\u00f4les s\\u00e9mantiques selon le contexte. La d\\u00e9marche la plus utilis\\u00e9e pour la r\\u00e9alisation d'une SRL comprend trois \\u00e9tapes principales : (1) l'identification des arguments du verbe, bas\\u00e9e le plus souvent sur des heuristiques (Xue & Palmer, 2004) qui permettent de r\\u00e9duire le nombre de candidats ; (2) le calcul des probabilit\\u00e9s pour chacune des \\u00e9tiquettes \\u00e0 repr\\u00e9senter les r\\u00f4les s\\u00e9mantiques possibles ; (3) l'attribution de scores \\u00e0 chaque \\u00e9tiquette, \\u00e9ventuellement combin\\u00e9e \\u00e0 d'autres facteurs de pr\\u00e9diction, pour assigner des \\u00e9tiquettes appropri\\u00e9es aux arguments des verbes. De nos jours, les mod\\u00e8les d'apprentissage statistiques sont tr\\u00e8s sollicit\\u00e9s pour l'annotation des textes en r\\u00f4les s\\u00e9mantiques. L'un des travaux de r\\u00e9f\\u00e9rence propose un syst\\u00e8me de SRL statistique, qui peut \\u00eatre utilis\\u00e9 aussi pour l'analyse syntaxique, l'\\u00e9tiquetage des parties du discours (Church, 1988) , et la d\\u00e9sambigu\\u00efsation du sens des mots (Lapata & Brew, 2004) . Ce syst\\u00e8me, con\\u00e7u pour les verbes, les noms et les adjectifs, atteint 82% de pr\\u00e9cision sur des phrases pr\\u00e9-annot\\u00e9es manuellement, tandis qu'il montre 65% de pr\\u00e9cision et 61% de rappel sur des phrases non annot\\u00e9s (Gildea & Jurafsky, 2002) . Il a d'ailleurs \\u00e9t\\u00e9 utilis\\u00e9e dans le cadre du projet FrameNet. D\\u00e9sambigu\\u00efsation du sens des verbes La d\\u00e9sambigu\\u00efsation du sens des verbes ou Verb Sense Disambiguation (VSD) est une sous-t\\u00e2che de la WSD (word sense disambiguation). Elle consiste \\u00e0 s\\u00e9lectionner automatiquement, parmi ses diff\\u00e9rents sens, le sens le plus appropri\\u00e9 d'un verbe polys\\u00e9mique, selon son contexte d'apparition. Par exemple, le verbe read (lire) a plusieurs sens. Pour faire la distinction entre les phrases telles que I read a book (je lis un livre) et I read you loud and clear (je te comprends parfaitement), il est n\\u00e9cessaire de d\\u00e9sambigu\\u00efser le contexte d'apparition du verbe, en suivant une des m\\u00e9thodes existantes. La d\\u00e9sambigu\\u00efsation du sens est une t\\u00e2che n\\u00e9cessaire pour la traduction automatique (Carpuat & Wu, 2007) ou l'extraction d'informations (Sch\\u00fctze & Pedersen, 1995; Sanderson, 1994) . Deux types d'approches sont utilis\\u00e9es habituellement pour la d\\u00e9sambigu\\u00efsation du sens des mots : approche \\u00e0 base de r\\u00e8gles et approche \\u00e0 base d'apprentissage. L'approche \\u00e0 base de r\\u00e8gles requiert des ressources comme les bases de donn\\u00e9es lexicales, les dictionnaires \\u00e9lectroniques, qui fournissent des descriptions lexicales, syntaxiques et s\\u00e9mantiques des mots. \\u00c0 partir de ces ressources, des r\\u00e8gles sont d\\u00e9finies pour d\\u00e9terminer le sens exact du mot parmi l'ensemble des sens possibles. En traduction automatique, un ensemble constitu\\u00e9 de 63 r\\u00e8gles est propos\\u00e9 comme source de connaissances (Specia et al., 2005) . L'approche la plus utilis\\u00e9e actuellement est DESCRIPTION DU VERBE DANS LES TRAVAUX DE LINGUISTIQUE, TERMINOLOGIE ET TAL bas\\u00e9e sur l'apprentissage automatique (Ye & Baldwin, 2006; Brown et al., 2011; Yarowsky, 1995) . L'apprentissage peut \\u00eatre supervis\\u00e9 (exigeant un ensemble d'exemples manuellement annot\\u00e9s) ou non supervis\\u00e9 (appliqu\\u00e9 sur des textes non annot\\u00e9s). En effet, certains chercheurs proposent une technique non supervis\\u00e9e de d\\u00e9sambigu\\u00efsation du sens des verbes, qui regroupent les verbes ayant les pr\\u00e9f\\u00e9rences s\\u00e9lectionnelles et de sous-cat\\u00e9gorisation similaires (Wagner et al., 2009) . Cette m\\u00e9thode montre 57.06% de pr\\u00e9cision. D'autres travaux identifient les pr\\u00e9f\\u00e9rences s\\u00e9mantiques (Lapata & Brew, 2004) et les marques de sous-cat\\u00e9gorisation (Lapata & Brew, 1999) des verbes apparaissant dans plusieurs classes de Levin. En ce qui concerne les m\\u00e9thodes supervis\\u00e9es, les premi\\u00e8res exp\\u00e9riences se focalisaient sur les bi-grammes et les fonctions linguistiques et contextuelles (Pedersen, 2000 (Pedersen, , 2001;; Hoa Trang & Palmer, 2002) . Par la suite, les chercheurs se sont int\\u00e9ress\\u00e9s \\u00e0 l'apport des bases de connaissances fournissant les informations telles que la cat\\u00e9gorie grammaticale des mots voisins, la forme morphologique, les collocations, la relation syntaxique verbe-objet, utiles pour lever certaines ambigu\\u00eft\\u00e9s (Yoong Keok & Hwee Tou, 2002) . De plus en plus, les chercheurs abordent les r\\u00f4les s\\u00e9mantiques des arguments des verbes comme des fonctions contribuant \\u00e0 l'am\\u00e9lioration des performances des syst\\u00e8mes lors de la d\\u00e9sambigu\\u00efsation du sens des verbes (Hoa Trang & Palmer, 2005; Ye & Baldwin, 2006) . Cette technique est d'ailleurs recommand\\u00e9e car les r\\u00f4les s\\u00e9mantiques associ\\u00e9s \\u00e0 un mot peuvent donner des indices pour la d\\u00e9duction de son sens, surtout lorsque ces r\\u00f4les sont associ\\u00e9s \\u00e0 des frames de sous-cat\\u00e9gorisation syntaxique (Gildea & Jurafsky, 2002) . Certains travaux suivent une approche supervis\\u00e9e bas\\u00e9e sur connaissances extraites des ressources lexicales externes (VerbNet, WordNet, etc.) (knowledge based WSD) (Brown et al., 2011) . Une autre approche, inspir\\u00e9e par les travaux en psycholinguistique, propose de nouveaux crit\\u00e8res de regroupement des sens d'un mot, en fonction de la diff\\u00e9rence faible ou importante qui existe entre ces mots (Brown, 2008) . Pour le fran\\u00e7ais, il existe une approche d'analyse s\\u00e9mantique des textes, bas\\u00e9e sur des r\\u00e9seaux lexicaux et les relations de d\\u00e9pendance entre les mots ambigus et les autres mots de la phrase (Mouton, 2010) . La r\\u00e9alisation de la WSD sur des textes sp\\u00e9cialis\\u00e9s est actuellement une t\\u00e2che relativement difficile selon les domaines, \\u00e0 cause de l'absence des ressources terminologiques n\\u00e9c\\u00e9ssaires ou de l'insuffisance des donn\\u00e9es disponibles. N\\u00e9anmoins, dans certains domaines comme l'informatique biom\\u00e9dicale, diff\\u00e9rentes \\u00e9tudes proposent des syst\\u00e8mes de WSD bas\\u00e9s sur des m\\u00e9thodes non supervis\\u00e9es (Liu et al., 2001) ou supervis\\u00e9es (Stevenson & Guo, 2010) , utilisant des terminologies existantes. Quelques ressources lexicales d\\u00e9di\\u00e9es au verbe Dans la suite de cette section, nous d\\u00e9crivons bri\\u00e8vement quelques ressources lexicales : FrameNet (section 4.3.1), Verb-Net (section 4.3.2), VerbOcean (section 4.3.3) et WordNet (section 4.3.4). Nous nous int\\u00e9ressons particuli\\u00e8rement \\u00e0 la mani\\u00e8re dont l'information sur le verbe est pr\\u00e9sent\\u00e9e. FrameNet FrameNet (Ruppenhofer et al., 2006) est une base de donn\\u00e9es lexicales 2 initialement con\\u00e7ue pour l'anglais. Elle contient plus de 10 000 sens des unit\\u00e9s lexicales d\\u00e9crits \\u00e0 travers plus de 1 000 cadres s\\u00e9mantiques li\\u00e9s hi\\u00e9rarchiquement les uns aux autres et illustr\\u00e9s par plus de 170 000 phrases. Le projet FrameNet propose une description des unit\\u00e9s lexicales pr\\u00e9dicatives (verbes, noms et adjectifs), bas\\u00e9e sur l'annotation en cadres s\\u00e9mantiques (Fillmore, 1982) des phrases dans lesquelles ces unit\\u00e9s apparaissent. His $20 TRANSACTION with Amazon.com for a new TV had been very smooth. Dans cette phrase, chaque couleur repr\\u00e9sente un \\u00e9l\\u00e9ment du cadre : bleu fonc\\u00e9=ACHETEUR, bleu ciel=ARGENT, rouge=VENDEUR, vert=BIEN. Ces frames mettent en \\u00e9vidence des informations s\\u00e9mantiques n\\u00e9cessaires pour capturer les sens de l'unit\\u00e9 lexicale cl\\u00e9. Ainsi, pour chacune de ses entr\\u00e9es, FrameNet est capable de fournir un cadre s\\u00e9mantique complet, une description du frame, ses \\u00e9ventuelles relations avec d'autres frames, une description des \\u00e9l\\u00e9ments du frame et une illustration des sch\\u00e9mas valenciels de l'entr\\u00e9e \\u00e0 l'aide d'exemples (Ruppenhofer et al., 2006) . 2. https ://framenet.icsi.berkeley.edu/fndrupal/about ORNELLA WANDJI VerbNet Contrairement \\u00e0 FrameNet, VerbNet 3 (Kipper et al., 2000; Kipper-Schuler, 2005) est totallement focalis\\u00e9 sur les verbes. Cette ressource lexicale propose une description des verbes bas\\u00e9e sur la classification de Levin (section 2.3.1). Elle consiste \\u00e0 regrouper les verbes en diff\\u00e9rentes classes, qui mettent en \\u00e9vidence leurs propri\\u00e9t\\u00e9s syntaxiques et s\\u00e9mantiques communes. Cette m\\u00e9thode de description permet de faire des g\\u00e9n\\u00e9ralisations sur le comportement des verbes. Par exemple, les verbes appartenant \\u00e0 la classe Hit 18.1 : bang, bash, hit, kick... sont des transitifs direct. Ils exigent un agent et un patient, et peuvent \\u00eatre modifi\\u00e9s par des pr\\u00e9dicats s\\u00e9mantiques exprimant la mani\\u00e8re, la cause, la direction, etc. VerbNet est donc un lexique hi\\u00e9rarchique de verbes anglais regroup\\u00e9s en classes, ind\\u00e9pendamment des domaines de sp\\u00e9cialit\\u00e9s auxquels ils peuvent appartenir. Chaque classe est d\\u00e9crite \\u00e0 travers : l'ensemble d'arguments possibles, pr\\u00e9sent\\u00e9s sous forme de r\\u00f4les th\\u00e9matiques ; les \\u00e9ventuelles restrictions de s\\u00e9lection d'arguments (comme anim\\u00e9, humain, organisation) ; les cadres, d\\u00e9crivant les possibles r\\u00e9alisations de surface de la structure argumentale (constructions transitives, intransitives, syntagmes pr\\u00e9positionnels, r\\u00e9sultatives) ; les alternances de diath\\u00e8se, c'est-\\u00e0-dire les variations des diff\\u00e9rents cadres. Selon le site officiel, apr\\u00e8s son extension (Korhonen & Briscoe, 2004) , VerbNet compte 274 classes de premier niveau, 23 r\\u00f4les th\\u00e9matiques, 94 pr\\u00e9dicats s\\u00e9mantiques, 55 restrictions syntaxiques, 5 257 sens des verbes et 3 769 lemmes. VerbOcean VerbOcean (Chklovski & Pantel, 2004) est une ressource lexicale qui propose un r\\u00e9seau s\\u00e9mantique de relations entre les verbes, et recense uniquement des paires de verbes s\\u00e9mantiquement proches. Elle contient 22 306 relations entre 3 477 verbes et identifie 5 types de relations : similitude (la similitude), strenght (la force), antonymy (l'antonymie), enablement (l'habilitation), et la relation temporelle happens-before (a lieu avant). L'approche appliqu\\u00e9e pour la conception de cet outil est bas\\u00e9e sur deux \\u00e9tapes : (1) la d\\u00e9tection des paires de verbes qui apparaissent en co-ooccurrence fr\\u00e9quente, gr\\u00e2ce \\u00e0 des requ\\u00eates effectu\\u00e9es sur le portail Google ; (2) pour chaque paire, le calcul du score de chaque relation possible, gr\\u00e2ce \\u00e0 35 sch\\u00e9mas lexico-syntaxiques. Par exemple, les verbes discover (d\\u00e9couvrir) et refine (affiner, am\\u00e9liorer) sont consid\\u00e9r\\u00e9s comme une paire illustrant la relation happens-before si la cha\\u00eene discovered and refined (instantiant le sch\\u00e9ma Xed and then Yed) est identifi\\u00e9e de fa\\u00e7on tr\\u00e8s fr\\u00e9quente sur Google. WordNet WordNet (Fellbaum, 1998) est une base de donn\\u00e9es lexicale qui propose une description des verbes, mais \\u00e9galement des noms et des adjectifs, sur la base de diff\\u00e9rentes relations s\\u00e9mantiques : la synonymie, l'antonymie, l'hyperonymie, l'hyponymie, la m\\u00e9ronymie, la troponymy et l'implication (Miller, 1995) . Contrairement \\u00e0 VerbOcean qui s'int\\u00e9resse uniquement aux paires de verbes s\\u00e9mantiquement proches, WordNet traite plusieurs cat\\u00e9gories d'unit\\u00e9s pr\\u00e9dicatives et ces unit\\u00e9s sont regroup\\u00e9es dans des synsets, 117 000 au total. Un synset est un groupe de mots (synonymes) s\\u00e9mantiquement homog\\u00e8nes. Il contient des pointeurs qui marquent ses relations conceptuelles avec d'autres synsets. En outre, un synset contient une br\\u00e8ve d\\u00e9finition et, dans la plupart des cas, une ou plusieurs courtes phrases illustrant l'utilisation des membres de ce synset. Les formes des mots ayant plusieurs significations sont repr\\u00e9sent\\u00e9es par autant de synsets distincts. Discussion et travaux futurs Comme indiqu\\u00e9 plus haut, le projet que nous entreprenons a pour objectif de proposer une m\\u00e9thode de simplification de textes m\\u00e9dicaux \\u00e9crits en fran\\u00e7ais, \\u00e0 partir d'une analyse syntaxico-s\\u00e9mantique des verbes en contexte. Nous avons vu dans la section 3 que les travaux sur les langues de sp\\u00e9cialit\\u00e9 sont le plus souvent focalis\\u00e9s sur les entit\\u00e9s nominales et, par cons\\u00e9quent, les travaux sur les verbes terminologiques sont peu nombreux. De m\\u00eame, dans la section 4, nous d\\u00e9montrons que peu de travaux en TAL appliquent la s\\u00e9mantique des cadres \\u00e0 des textes sp\\u00e9cialis\\u00e9s et qu'il existe encore des cadres th\\u00e9oriques dans lesquels le verbe et sa structure argumentale sont peu consid\\u00e9r\\u00e9s. En rupture avec ces constats, nous proposons d'exploiter l'\\u00e9tude de la structure argumentale des verbes pour la simplification des textes sp\\u00e9cialis\\u00e9s. Nous partons de l'hypoth\\u00e8se selon laquelle le verbe, en tant que pr\\u00e9dicat central dans la phrase, peut \\u00eatre le point de d\\u00e9part pour cerner la syntaxe et la s\\u00e9mantique des textes sp\\u00e9cialis\\u00e9s puisqu'il sert \\u00e0 articuler l'expertise et les connaissances port\\u00e9es 3. Simplification. Dans le cas des textes sp\\u00e9cialis\\u00e9s, r\\u00e9dig\\u00e9s pour un public de non experts, les emplois sp\\u00e9cialis\\u00e9s des verbes peuvent \\u00eatre consid\\u00e9r\\u00e9s comme des sources de difficult\\u00e9. Gr\\u00e2ce \\u00e0 l'\\u00e9tape pr\\u00e9c\\u00e9dente, de tels emplois sp\\u00e9cialis\\u00e9s peuvent \\u00eatre d\\u00e9tect\\u00e9s automatiquement. La simplification a pour objectif de rendre ces emplois de verbes plus abordables pour les utilisateurs non sp\\u00e9cialistes. A ce niveau, l'absence de ressources du type WordNet pour les langues de sp\\u00e9cialit\\u00e9 repr\\u00e9sente une difficult\\u00e9 cruciale que nous allons devoir affronter. Dans un premier temps, pour pallier \\u00e0 ce probl\\u00e8me, les phrases simplifi\\u00e9es seront con\\u00e7ues sur un mod\\u00e8le que proposent les d\\u00e9finitions des termes du DicoInfo (L'Homme, 2009). Il s'agira de fournir une d\\u00e9finition typique de la construction verbale ambig\\u00fce dans laquelle entre le verbe. Cette d\\u00e9finition sera enrichie par un ou plusieurs synonymes du verbe qui seront recherch\\u00e9s dans WordNet ou d'autres ressources qui proposent les synonymes des mots de la langue g\\u00e9n\\u00e9rale. Une \\u00e9tude comparative des corpus de textes sp\\u00e9cialis\\u00e9s, \\u00e9crits par des experts et ceux \\u00e9crits par des non-experts, effectu\\u00e9e au pr\\u00e9alable, sera utile lors de la simplification, pour l'identification, si possible, des constructions verbales synonymes. La simplification concernera \\u00e9galement les constituants syntaxiques de la phrase et \\u00e9ventuellement des temps verbaux (Brouwers et al., 2014) . En exploitant la m\\u00e9thode appliqu\\u00e9e dans FrameNet et gr\\u00e2ce aux observations en corpus, nous pouvons d\\u00e9tecter les arguments n\\u00e9cessaires (core) et non n\\u00e9cessaires (non core) et all\\u00e9ger les phrases en supprimant les \\u00e9l\\u00e9ments non n\\u00e9cessaires. De la m\\u00eame mani\\u00e8re, si les \\u00e9l\\u00e9ments n\\u00e9cessaires \\u00e0 la compr\\u00e9hension sont absents, nous pouvons les d\\u00e9duire et compl\\u00e9ter ainsi la structure argumentale de verbes, en esp\\u00e9rant que cela facilite la compr\\u00e9hension des phrases. Au terme de ces diff\\u00e9rentes \\u00e9tapes, nous pensons pouvoir am\\u00e9liorer la lisibilit\\u00e9 du texte et de rendre le sens des verbes plus accessibles aux utilisateurs non sp\\u00e9cialistes en m\\u00e9decine. Les r\\u00e9sultats de notre approche lexico-syntaxique seront \\u00e9valu\\u00e9s et compar\\u00e9s \\u00e0 ceux des m\\u00e9thodes de simplification focalis\\u00e9es uniquement sur les entit\\u00e9s nominales, c'est-\\u00e0-dire sur les arguments des verbes. Conclusion Tout au long de ce travail, nous avons explor\\u00e9 les principaux cadres th\\u00e9oriques et travaux qui s'int\\u00e9ressent particuli\\u00e8rement au pr\\u00e9dicat verbal dans trois domaines de recherche : terminologie, o\\u00f9 le verbe a tard\\u00e9 \\u00e0 s'imposer comme unit\\u00e9 pouvant exprimer des connaissances sp\\u00e9cialis\\u00e9es, face \\u00e0 la place dominante des entit\\u00e9s nominales ; linguistique, o\\u00f9 le verbe a toujours fait partie des cat\\u00e9gories grammaticales les plus \\u00e9tudi\\u00e9es ; TAL, o\\u00f9 de nos jours, de nombreuses ressources\",\n          \"Le travail pr\\u00e9sent\\u00e9 dans cet article se situe dans le cadre de l'identification de termes sp\\u00e9cialis\\u00e9s (unit\\u00e9s de mesure) \\u00e0 partir de donn\\u00e9es textuelles pour enrichir une Ressource Termino-Ontologique (RTO). La premi\\u00e8re \\u00e9tape de notre m\\u00e9thode consiste \\u00e0 pr\\u00e9dire la localisation des variants d'unit\\u00e9s de mesure dans les documents. Nous avons utilis\\u00e9 une m\\u00e9thode reposant sur l'apprentissage supervis\\u00e9. Cette m\\u00e9thode permet de r\\u00e9duire sensiblement l'espace de recherche des variants tout en restant dans un contexte optimal de recherche (r\\u00e9duction de 86% de l'espace de recherch\\u00e9 sur le corpus \\u00e9tudi\\u00e9). La deuxi\\u00e8me \\u00e9tape du processus, une fois l'espace de recherche r\\u00e9duit aux variants d'unit\\u00e9s, utilise une nouvelle mesure de similarit\\u00e9 permettant d'identifier automatiquement les variants d\\u00e9couverts par rapport \\u00e0 un terme d'unit\\u00e9 d\\u00e9j\\u00e0 r\\u00e9f\\u00e9renc\\u00e9 dans la RTO avec un taux de pr\\u00e9cision de 82% pour un seuil au dessus de 0.6 sur le corpus \\u00e9tudi\\u00e9. Introduction Le travail pr\\u00e9sent\\u00e9 dans cet article se situe dans le cadre de l'identification de termes sp\\u00e9cialis\\u00e9s \\u00e0 partir de donn\\u00e9es textuelles pour enrichir une Ressource Termino-Ontologique (RTO). Les travaux de (McCrae et al., 2011; Cimiano et al., 2011) proposent d'associer une partie terminologique et/ou linguistique aux ontologies afin d'\\u00e9tablir une distinction claire entre la manifestation linguistique (le terme) et la notion qu'elle d\\u00e9note (le concept). Nous nous int\\u00e9ressons \\u00e0 l'enrichissement d'une RTO permettant de mod\\u00e9liser des relations n-aires entre des donn\\u00e9es quantitatives exp\\u00e9rimentales (Touhami et al., 2011) , o\\u00f9 les arguments peuvent \\u00eatre des concepts symboliques ou des quantit\\u00e9s caract\\u00e9ris\\u00e9es par des unit\\u00e9s de mesure. En effet, l'extraction des donn\\u00e9es quantitatives est un enjeu majeur pour de nombreux domaines scientifiques dont l'objectif concerne la capitalisation et la p\\u00e9rennisation des connaissances du domaine. Cependant, la forte variation d'\\u00e9criture des unit\\u00e9s de mesure dans les documents engendre des probl\\u00e8mes d'identification des instances num\\u00e9riques dans les textes. Dans une d\\u00e9marche consensuelle, le Systeme International (SI) (Thompson & Taylor, 2008) organise, en posant plusieurs d\\u00e9finitions formelles, le syst\\u00e8me des quantit\\u00e9s et des unit\\u00e9s de mesure. Il d\\u00e9finit ainsi des unit\\u00e9s de base, i.e. unit\\u00e9s simples comme kilogram, et des unit\\u00e9s d\\u00e9riv\\u00e9es, i.e. unit\\u00e9s plus complexes comme kg.m \\u22121 . Ce standard pose les r\\u00e8gles d'\\u00e9criture de l'ensemble des unit\\u00e9s de mesure mais n'int\\u00e8gre pas la notion de variants d'unit\\u00e9s. Ces principes sont repris dans des travaux r\\u00e9cents (Rijgersberg et al., 2013) afin de mod\\u00e9liser formellement cette connaissance dans une ontologie d\\u00e9di\\u00e9e \\u00e0 la repr\\u00e9sentation des donn\\u00e9es quantitatives et des unit\\u00e9s de mesure. Les auteurs ont ainsi mod\\u00e9lis\\u00e9 OM (Ontology of Units of Measure and Related Concepts). Les travaux de (Van Assem et al., 2010) posent la probl\\u00e9matique d'identification des donn\\u00e9es quantitatives pr\\u00e9sentes dans les cellules des tableaux repr\\u00e9sent\\u00e9s dans les documents. La localisation des variants d'unit\\u00e9s n'est pas probl\\u00e9matique dans ces travaux car la m\\u00e9thode repose sur le format structur\\u00e9 des tableaux. Les travaux de (Grau et al., 2009) proposent des m\\u00e9thodes d'extraction des donn\\u00e9es exp\\u00e9rimentales dans le domaine biom\\u00e9dical. L'identification des unit\\u00e9s de mesure repose sur les unit\\u00e9s r\\u00e9f\\u00e9renc\\u00e9es dans le Systeme International (Thompson & Taylor, 2008) , la probl\\u00e9matique de l'identification des variants d'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e n'y est pas abord\\u00e9e. Ainsi, \\u00e0 notre connaissance, les m\\u00e9thodes de l'\\u00e9tat de l'art partageant l'objectif d'extraction de donn\\u00e9es quantitatives, ne permettent pas de r\\u00e9soudre la probl\\u00e9matique d'extraction et d'identification des variants d'unit\\u00e9s de mesure dispers\\u00e9s dans les documents scientifiques au format textuel non structur\\u00e9. Dans cet article, nous pr\\u00e9sentons notre proposition qui tente de r\\u00e9pondre \\u00e0 deux questions concernant l'identification des variants d'unit\\u00e9s de mesure dans les documents textuels non structur\\u00e9s : -La question concernant la localisation des variants dans le document. Sachant que nous travaillons sur l'int\\u00e9gralit\\u00e9 des documents, nous pr\\u00e9f\\u00e9rons l'apprentissage afin de pr\\u00e9dire la localisation des variants sans poser d'hypoth\\u00e8ses pr\\u00e9alables. -La question de l'identification du variant une fois qu'il est localis\\u00e9. A quel autre terme d'unit\\u00e9 de mesure r\\u00e9f\\u00e9renc\\u00e9e dans la RTO peut-on le rapprocher, en sachant que les termes d'unit\\u00e9s r\\u00e9pondent \\u00e0 leurs propres r\\u00e8gles syntaxiques ? Les m\\u00e9thodes existantes doivent \\u00eatre adapt\\u00e9es \\u00e0 ces nouvelles r\\u00e8gles. Les deux questions de recherche pr\\u00e9c\\u00e9dentes sont respectivement trait\\u00e9es en sections 2 et 3. Ces propositions sont alors exp\\u00e9riment\\u00e9es en section 4, avant la conclusion et les perspectives d\\u00e9crites en section 5. Dans notre approche, il est fondamental de pouvoir prendre en consid\\u00e9ration les particularit\\u00e9s d'\\u00e9criture des unit\\u00e9s de mesure, en notant que chaque bloc est ind\\u00e9pendant dans l'\\u00e9criture de l'unit\\u00e9. De ce fait, l'ordre des blocs n'est pas important \\u00e0 prendre en compte, en revanche, la comparaison des blocs entre eux nous semble plus pertinente et plus adapt\\u00e9e dans le calcul de la similarit\\u00e9. Il est alors int\\u00e9ressant de proposer une mesure qui calcule la similarit\\u00e9 en deux temps, qui s'appuie \\u00e0 la fois sur les unit\\u00e9s d\\u00e9j\\u00e0 r\\u00e9f\\u00e9renc\\u00e9es dans la RTO et sur les caract\\u00e8res sp\\u00e9cifiques (/, (, ), ., \\u00d7, \\u02c6...) utilis\\u00e9s comme s\\u00e9parateurs de blocs. Dans un premier temps, les candidats sont pr\\u00e9sectionn\\u00e9s selon la mesure de Jaccard. Le principe ci-dessous est alors mis en oeuvre : -Soit un couple compos\\u00e9 du variant candidat u i et d'une unit\\u00e9 r\\u00e9f\\u00e9rence dans la RTO u j . J(u i , u j ) (cf. formule (1)) calcule dans un premier temps le score de similarit\\u00e9 entre l'ensemble u i et l'ensemble u j par rapport aux blocs communs sans tenir compte de leur ordre. J(u i , u j ) = |u i \\u2229 u j | |u i \\u222a u j | (1) -On s\\u00e9lectionne le couple (u i , u j ) comme \\u00e9tant pertinent \\u00e0 \\u00eatre compar\\u00e9 si J(u i , u j ) > K , K \\u00e9tant le seuil minimal d\\u00e9fini pr\\u00e9alablement par l'utilisateur. Prenons l'exemple du couple compos\\u00e9 d'un variant candidat localis\\u00e9 et extrait \\u00e0 partir d'un document kg P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 et son r\\u00e9f\\u00e9rent dans la RTO lb.m.m \\u22122 .s \\u22121 .P a \\u22121 . Dans ce contexte, le calcul de la mesure de Jacccard donne le r\\u00e9sultat suivant : J(kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 ) = 4 6 = 0.7 Dans un deuxi\\u00e8me temps, apr\\u00e8s cette phase de pr\\u00e9s\\u00e9lection, les candidats sont s\\u00e9lectionn\\u00e9s selon une mesure \\u00e9tendue de Damereau-Levenshtein. La mesure de similarit\\u00e9 de Levenshtein (Levenshtein, 1966) calcule le co \\u00fbt minimal pour transformer une premi\\u00e8re cha \\u00eene de caract\\u00e8res en une deuxi\\u00e8me cha \\u00eene de caract\\u00e8res en consid\\u00e9rant les op\\u00e9rations de remplacement de caract\\u00e8res entre les deux cha \\u00eenes, d'ajout d'un caract\\u00e8re ou et de suppression d'un caract\\u00e8re. Le co \\u00fbt est ensuite normalis\\u00e9 pour obtenir une valeur de la distance entre les deux cha \\u00eenes entre 0 et 1. Cette mesure est \\u00e9tendue par Damerau (Damerau, 1964) qui inclut dans celle de Levenshtein la notion de transposition de caract\\u00e8res d'une cha \\u00eene \\u00e0 une autre, i.e. dans litre et liter, il y a transposition entre les caract\\u00e8res \\\"e\\\" et \\\"r\\\". La mesure adapt\\u00e9e \\u00e0 notre contexte (cf. formule (2)) ne consid\\u00e8re plus la comparaison des caract\\u00e8res mais des blocs de caract\\u00e8res, correspondant \\u00e0 des unit\\u00e9s simples. Le variant candidat et l'unit\\u00e9 de r\\u00e9f\\u00e9rence, composant le couple pr\\u00e9s\\u00e9lectionn\\u00e9 lors de la premi\\u00e8re phase, sont, dans cette seconde phase, compar\\u00e9s bloc \\u00e0 bloc pour d\\u00e9terminer leur similarit\\u00e9 finale. SM D b (u i , u j ) = max[0; min(|u i |, |u j |) \\u2212 D b (u i , u j ) min(|u i |, |u j |) ]; SM D b (u i , u j ) \\u2208 [0; 1] (2) -(u i , u j ) repr\\u00e9sente le couple s\\u00e9lectionn\\u00e9 \\u00e0 partir de la mesure de Jaccard ; -Chaque bloc de u i est compar\\u00e9 aux blocs de u j pour calculer la nouvelle distance D b ; u i est valid\\u00e9e comme un variant de l'unit\\u00e9 u j si SM D b > K, avec K un seuil de similarit\\u00e9 d\\u00e9fini pr\\u00e9alablement. En posant K = 0.5, le couple kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 , SM D b calcule la similarit\\u00e9 du couple en comparant chaque bloc dans les unit\\u00e9s : SM D b (kg m P a \\u02c6\\u22121 s \\u02c6\\u22121 m \\u02c6\\u22122 , lb.m.m \\u22122 .s \\u22121 .P a \\u22121 ) = max[0; 5 \\u2212 1 5 ] = 0.8 Un tel processus fond\\u00e9 sur ces deux phases cons\\u00e9cutives de pr\\u00e9s\\u00e9lection et de s\\u00e9lection finale permet la d\\u00e9couverte de nouveaux variants \\u00e0 int\\u00e9grer comme d\\u00e9taill\\u00e9 dans la section suivante. Exp\\u00e9rimentations Les exp\\u00e9rimentations ont \\u00e9t\\u00e9 men\\u00e9es \\u00e0 partir d'un corpus de 115 articles scientifiques en anglais issus du domaine des emballages alimentaires. Elles s'appuient \\u00e9galement sur une liste de 211 termes d\\u00e9notant les diff\\u00e9rents concepts d'unit\\u00e9s de mesure pour le domaine des emballages alimentaires. Ces diff\\u00e9rentes fen \\u00eatres correspondent \\u00e0 des sous-ensembles du corpus repr\\u00e9sentant 5000 phrases (e.g. f 0 ) \\u00e0 15000 phrases (e.g. f +2 ). Le corpus complet comportant plus de 35000 phrases. Le sac de mots repr\\u00e9sente un ensemble de 3000 \\u00e0 4800 descripteurs selon les diff\\u00e9rentes repr\\u00e9sentations. \\u00c9valuation de la m\\u00e9thode de localisation des unit\\u00e9s de mesure Notre objectif, au cours de cette premi\\u00e8re \\u00e9tape (cf. section 2), est de produire un mod\\u00e8le d'apprentissage appris \\u00e0 partir des donn\\u00e9es repr\\u00e9sent\\u00e9es sous forme de fen \\u00eatres textuelles, qui permette de r\\u00e9duire l'espace de recherche des variants d'unit\\u00e9s. Nous avons test\\u00e9 plusieurs fen \\u00eatres textuelles. Nous restituons en r\\u00e9sultats des exp\\u00e9rimentations uniquement ceux r\\u00e9v\\u00e9lant les fen \\u00eatres d'\\u00e9tude les plus pertinentes dans le tableau 1. Par souci de lisibilit\\u00e9, les fen \\u00eatres textuelles sont exprim\\u00e9es de la mani\\u00e8re suivante : f 0 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9. f +2 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9 ainsi que les deux phrases suivantes. f \\u22122 : repr\\u00e9sente la fen \\u00eatre comportant la phrase o\\u00f9 au moins un terme d'unit\\u00e9 d\\u00e9notant un concept de la RTO est identifi\\u00e9 ainsi que les deux phrases pr\\u00e9c\\u00e9dentes. Les tableaux 1 et 2 restituent les r\\u00e9sultats obtenus sur le corpus des emballages r\\u00e9alis\\u00e9 avec quatre algorithmes d'apprentissage (Naives Bayes, C4.5, DMNB (Discriminative Multinominal Naive Bayes), SMO (Sequential minimal optimization qui est une variante de SVM)) et une 10-validation crois\\u00e9e. Le tableaun1 restitue les r\\u00e9sultats, toutes mesures confondues. L'analyse des r\\u00e9sultats montrent que Naives Bayes produit une F-mesure allant de 0.85 \\u00e0 0.88, l'arbre de d\\u00e9cision \\u00e9tabli sur l'algorithme C4.5 (J48) produit de meilleurs r\\u00e9sultats autour de 0.93 \\u00e0 0.96. DMNB et SMO produisent les meilleurs r\\u00e9sultats, conform\\u00e9ment \\u00e0 ce qui est soulign\\u00e9 dans la litt\\u00e9rature du domaine (0.95 \\u00e0 0.99). Outre ces r\\u00e9sultats analytiques, nous remarquons qu'un plus large contexte, \\u00e0 partir des fen \\u00eatres f +2 et f \\u22122 , n'am\\u00e9liorent pas les r\\u00e9sultats d'apprentissage. Nous pouvons donc en d\\u00e9duire que la plus petite fen \\u00eatre textuelle, c'est-\\u00e0-dire celle o\\u00f9 au moins un terme d'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9 dans la RTO apparait, est le contexte le plus favorable \\u00e0 la d\\u00e9couverte de variants d'unit\\u00e9s. Cette conclusion permet de r\\u00e9duire sensiblement l'espace de recherche des variants, i.e. 5000 phrases \\u00e0 consid\\u00e9rer plut \\u00f4t que 35000 initialement d\\u00e9nombr\\u00e9es, tout en restant dans un contexte optimal de recherche (r\\u00e9duction de 86% de l'espace de recherche). Le tableau 2 synth\\u00e9tise les r\\u00e9sultats selon les diff\\u00e9rentes mesures de pond\\u00e9ration et la matrice bool\\u00e9enne pour la fen \\u00eatre optimale f 0 . Notre objectif \\u00e9tant d'\\u00e9valuer quel algorithme produit le mod\\u00e8le restituant des valeurs de F-mesure stables sur les diff\\u00e9rentes mesures de pond\\u00e9ration et la matrice bool\\u00e9enne. La F-mesure, ainsi que les valeurs de pr\\u00e9cision et de rappel restent stables et \\u00e9lev\\u00e9es avec le mod\\u00e8le DMNB, en restituant une valeur constante autour de 0.95. \\u00c9valuation de la m\\u00e9thode d'identification des unit\\u00e9s de mesure Dans la deuxi\\u00e8me \\u00e9tape de notre processus (cf. section 3), nous nous appuyons sur les r\\u00e9sultats obtenus pr\\u00e9c\\u00e9demment afin d'identifier les variants d'unit\\u00e9s. Notons que dans notre contexte, nos mesures doivent s\\u00e9lectionner les variants les plus pertinents \\u00e0 pr\\u00e9senter aux experts, ce qui revient \\u00e0 minimiser le bruit. Ainsi, nous privil\\u00e9gions la mesure de pr\\u00e9cision pour \\u00e9valuer nos propositions. Le tableau 3 restitue les r\\u00e9sultats obtenus avec la nouvelle mesure et permet de les comparer par seuil de similarit\\u00e9 : 1. La pr\\u00e9cision est globalement plus \\u00e9lev\\u00e9e avec le processus complet comparativement \\u00e0 l'application de la seule mesure de pr\\u00e9s\\u00e9lection (Jaccard), ceci confirme donc l'int\\u00e9r \\u00eat d'utiliser notre mesure SM D b . 2. Les seuils les plus int\\u00e9ressants \\u00e0 exploiter sont au-dessus de 0.6 avec un taux de pr\\u00e9cision sup\\u00e9rieur \\u00e0 82% apr\\u00e8s application des deux \\u00e9tapes successives ; l'essentiel des variants sont identifi\\u00e9s. 3. En dessous de 0.5, les r\\u00e9sultats se tassent largement. En choisissant de ne consid\\u00e9rer que les seuils au-del\\u00e0 de 0.5, nous cr\\u00e9ons forc\\u00e9ment du silence mais un silence \\\"contr \\u00f4l\\u00e9\\\". En effet, le processus d'extraction et d'identification des variants \\u00e9tant un processus it\\u00e9ratif, les nouvelles unit\\u00e9s int\\u00e9gr\\u00e9es dans la RTO favorisent la d\\u00e9couverte d'autres variants qui s'expriment dans cette plage de silence. Le tableau 3 montre la validation des couples variants et unit\\u00e9s de r\\u00e9f\\u00e9rence pertinents (avec K et K ayant les valeurs 0.5). Un m \\u00eame variant peut former un couple avec plusieurs unit\\u00e9s de r\\u00e9f\\u00e9rence dans la RTO. En effet, prenons l'exemple du variant amol / m s Pa, sa comparaison avec les unit\\u00e9s de r\\u00e9f\\u00e9rence amol/m/s/Pa, amol/(m.s.Pa), amol/(m s Pa)... est consid\\u00e9r\\u00e9e comme pertinente. Pour tous les couples pertinents valid\\u00e9s, nous n'int\\u00e9grons qu'une seule fois le variant amol / m s Pa. Consid\\u00e9rant cette remarque, sur les 267 couples cumul\\u00e9s dans le tableau 3, valid\\u00e9s par SM D b (260 couples s\\u00e9lectionn\\u00e9s), nous obtenons 121 variants d'unit\\u00e9s uniques \\u00e0 int\\u00e9grer dans notre RTO.  Nous avons montr\\u00e9 que la premi\\u00e8re \\u00e9tape de notre m\\u00e9thode, s'appuyant sur l'apprentissage supervis\\u00e9, permet de localiser automatiquement les variants d'unit\\u00e9 dans un contexte phrastique optimal de recherche. La m\\u00e9thode repose sur une nouvelle repr\\u00e9sentation des donn\\u00e9es, sous forme de fen \\u00eatres textuelles d'\\u00e9tude, que nous obtenons en \\u00e9tant guid\\u00e9 par la RTO. Par la suite, nous avons propos\\u00e9 une nouvelle mesure de similarit\\u00e9 adapt\\u00e9e aux sp\\u00e9cificit\\u00e9s des unit\\u00e9s de mesure. Le choix de la mesure de Damereau-Levenshtein est appropri\\u00e9e \\u00e0 notre contexte car elle prend en charge toutes les variations constat\\u00e9es pour les unit\\u00e9s de mesure. De plus, associ\\u00e9e \\u00e0 l'indice de Jaccard, la nouvelle mesure permet de rapprocher les couples variant-unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e de mani\\u00e8re plus pertinente en octroyant un premier score global de similarit\\u00e9 qui ne tient pas compte de l'ordre des blocs dans la construction de l'unit\\u00e9 complexe. Dans un second temps, la nouvelle mesure SM D b affine ce rapprochement en comparant chaque bloc du variant et de l'unit\\u00e9 r\\u00e9f\\u00e9renc\\u00e9e s\\u00e9lectionn\\u00e9e. Le processus d'enrichissement de la RTO \\u00e9tant un processus it\\u00e9ratif, une nouvelle phase d'extraction et d'identification permettrait alors de comparer d'autres variants avec ces nouvelles unit\\u00e9s int\\u00e9gr\\u00e9es dans la RTO, qui deviennent des r\\u00e9f\\u00e9rents.\",\n          \"Cet article propose une m\\u00e9thode de codage automatique de traits lexicaux s\\u00e9mantiques en fran\\u00e7ais. Cette approche exploite les relations fix\\u00e9es par l'instruction s\\u00e9mantique d'un op\\u00e9rateur de construction morphologique entre la base et le mot construit. En cela, la r\\u00e9flexion s'inspire des travaux de Marc Light (Light 1996) tout en exploitant le fonctionnement d'un syst\\u00e8me d'analyse morphologique existant : l'analyseur D\\u00e9riF. A ce jour, l'analyse de 12 types morphologiques conduit \\u00e0 l'\\u00e9tiquetage d'environ 10 % d'un lexique compos\\u00e9 de 99000 lemmes. L'article s'ach\\u00e8ve par la description de deux techniques utilis\\u00e9es pour valider les traits s\\u00e9mantiques. This paper presents an approach which aims at automatically tagging a French lexicon with semantic features. This approach makes use of correspondences which are fixed by the semantic instruction of morphological operators, between the base and the derived word. It is partly inspired by Light (1996) work, and makes use of an existing morphological parser : the DeriF system. So far, 12 morphological types have been analysed, enabling the semantic tagging of circa 10 % of a lexicon made up of 99000 lemmas. The paper ends up with the description of two techniques that have been applied to validate the semantic features. Keywords -Mots Cl\\u00e9s Morphologie d\\u00e9rivationnelle -affixation et conversion -acquisition de traits s\\u00e9mantiques Derivational Morphology -affixation and conversion -Semantic feature acquisition * Je remercie les relecteurs anonymes pour leurs commentaires et suggestions, qui m'ont aid\\u00e9e dans la r\\u00e9daction de la deuxi\\u00e8me version de cet article Introduction L'information s\\u00e9mantique est cruciale pour l'ensemble des applications en TALN. Cette information, cependant, n'est pas toujours accessible : le codage s\\u00e9mantique est une t\\u00e2che complexe, donc \\u00e0 co\\u00fbt \\u00e9lev\\u00e9. Nous allons voir comment la morphologie peut \\u00eatre utilis\\u00e9e pour amorcer le codage d'un lexique d'environ 99000 lemmes cat\\u00e9goris\\u00e9s au moyen de traits s\\u00e9mantiques. L'exp\\u00e9rience relat\\u00e9e se fonde sur les hypoth\\u00e8ses linguistiques \\u00e9nonc\\u00e9es \\u00e0 l'origine dans (Corbin, 1987) et tire parti du fonctionnement d'un analyseur de mots construits existant : le syst\\u00e8me D\\u00e9riF (Namer, 1999) . A la section 2, nous situons notre approche dans le cadre des travaux d'acquisition de s\\u00e9mantique lexicale. Ensuite, la section 3 illustre par des exemples le r\\u00f4le des contraintes s\\u00e9mantiques dans des op\\u00e9rations de construction lexicale par suffixation, pr\\u00e9fixation et conversion. Une partie de ces contraintes est mise en oeuvre, au d\\u00e9but de la section 4, dans le programme D\\u00e9riF pour annoter automatiquement certaines bases et d\\u00e9riv\\u00e9s au moyen des traits pertinents. La fin de la section 4 pr\\u00e9sente enfin une m\\u00e9thode de validation de ces traits, bas\\u00e9e sur deux approches : filtre automatique, et recherche sur Internet. Sens et ressources lexicales Disposer de ressources lexicales munies d'informations s\\u00e9mantiques \\u00e0 la fois facilement exploitables et publiquement disponibles est un atout pr\\u00e9cieux dans l'\\u00e9laboration de corpus annot\\u00e9s s\\u00e9mantiquement, t\\u00e2che primordiale dans de nombreuses applications TALN : l'anglais dispose du r\\u00e9seau WordNet, d\\u00e9velopp\\u00e9 \\u00e0 Princeton ( (Miller, 1995) , (Fellbaum, 1998) ). Le consortium EuroWordNet (Vossen, 1998) a r\\u00e9alis\\u00e9 des r\\u00e9seaux s\\u00e9mantiques en n\\u00e9erlandais, italien, espagnol, fran\\u00e7ais, allemand, tch\\u00e8que et estonien, en lien avec WordNet. La base lexicale du fran\\u00e7ais pr\\u00e9sente cependant des lacunes : seules les cat\\u00e9gories noms et verbes y sont repr\\u00e9sent\\u00e9es, et les seules relations disponibles sont la synonymie, l'hyperonymie et l'hyponymie. De nombreux travaux r\\u00e9cents portant sur l'\\u00e9tiquetage s\\u00e9mantique des corpus d\\u00e9crivent des m\\u00e9thodes d'acquisition de relations s\\u00e9mantiques compl\\u00e9mentaires, comme par exemple la structure argumentale et les contraintes de s\\u00e9lection des verbes, ce que r\\u00e9alise pour l'anglais (MacCarthy et al. 2001) . Parmi les travaux concernant le fran\\u00e7ais, Fabre et Jacquemin (2000) proposent le codage manuel d'un ensemble minimal de traits verbaux binaires de mani\\u00e8re \\u00e0 accro\\u00eetre la pr\\u00e9cision dans l'appariement de variantes teminologiques. Cet article propose une m\\u00e9thode de codage automatique de traits lexicaux s\\u00e9mantiques en fran\\u00e7ais qui se veut compl\\u00e9mentaire \\u00e0 la fois d'EuroWordNet, pour la nature des traits cod\\u00e9s, et de (Fabre & Jacquemin, 2000) , pour la m\\u00e9thode d'acquisition de ces informations : en effet, les traits s\\u00e9mantiques qui codent les entr\\u00e9es lexicales sont d\\u00e9duites de contraintes s\\u00e9mantiques associ\\u00e9es aux op\\u00e9rations de construction de mots, s'exer\\u00e7ant tant sur la base que sur le d\\u00e9riv\\u00e9. Cette m\\u00e9thode s'inspire des travaux de Light (1996) ; cependant, ce syst\\u00e8me d'acquisition s'appuie sur un programme d'analyse automatique du lexique construit en fran\\u00e7ais, effectuant \\u00e0 ce jour l'analyse compl\\u00e8te de 18500 mots construits appartenant \\u00e0 divers types morphologiques. En cela, notre approche s'\\u00e9loigne des pr\\u00e9occupations de M. Light, dont l'objectif est d'associer pour l'anglais \\u00e0 chaque op\\u00e9ration morphologique la repr\\u00e9sentation s\\u00e9mantique la plus probable de la base et du d\\u00e9riv\\u00e9. Son souci \\u00e9tant de pr\\u00e9server un \\u00e9quilibre entre pr\\u00e9cision et rappel, un taux d'erreur non nul est in\\u00e9vitable (il varie de 0% pour l'assignation de l'\\u00e9tiquette \\\"changement d'\\u00e9tat\\\" aux d\\u00e9riv\\u00e9s en -en, \\u00e0 80% pour l'assignation du trait \\\"\\u00e9tat final oppos\\u00e9 \\u00e0 la base\\\" aux d\\u00e9riv\\u00e9s en de-). A l'inverse, notre objectif est de privil\\u00e9gier les op\\u00e9rations morphologiques conduisant \\u00e0 un codage s\\u00e9mantique d'une pr\\u00e9cision proche de 100%. L'approche propos\\u00e9e ici est illustr\\u00e9e par l'\\u00e9tude de quelques op\\u00e9rations de construction de mots (suffixation, pr\\u00e9fixation ou conversion) s\\u00e9lectionn\\u00e9es pour leur r\\u00e9gularit\\u00e9 et, si possible, leur productivit\\u00e9, et donc les plus pertinentes du point de vue qualitatif (les op\\u00e9rateurs r\\u00e9guliers affichant un taux \\u00e9lev\\u00e9 de pr\\u00e9cision dans l'assignation des traits s\\u00e9mantiques \\u00e0 la base et au d\\u00e9riv\\u00e9) et, le cas \\u00e9ch\\u00e9ant, quantitatif (les op\\u00e9rateurs productifs garantissant un nombre \\u00e9lev\\u00e9 d'entr\\u00e9es cod\\u00e9es dans la base). Cadre th\\u00e9orique Le cadre th\\u00e9orique dans lequel se situent les analyses linguistiques propos\\u00e9es dans cette section est d\\u00e9crit essentiellement dans (Corbin, 1987) , o\\u00f9 la morphologie d\\u00e9rivationnelle est con\\u00e7ue comme le calcul conjoint de la structure et du sens des unit\\u00e9s lexicales. Cette perspective \\\" associative \\\" donne \\u00e0 analyser un mot comme construit s'il a une structure construite et un sens construit qui soit calculable \\u00e0 partir de celle-l\\u00e0. Op\\u00e9rateurs constructionnels et contraintes s\\u00e9mantiques Outre la composition, qui met en jeu deux unit\\u00e9s lexicales \\u00e0 sens r\\u00e9f\\u00e9rentiel, les op\\u00e9rations de construction de mots rel\\u00e8vent de deux types possibles de proc\\u00e9d\\u00e9s : l'affixation (application d'un suffixe ou un pr\\u00e9fixe \\u00e0 une base) associe sens et forme, alors que la conversion (appel\\u00e9e aussi \\\"d\\u00e9rivation impropre\\\", comme le rel\\u00e8vent, entre autre Arriv\\u00e9 et al. (1986) ) met en relation deux cat\\u00e9gories grammaticales sans l'entremise de mat\\u00e9riel lexical. Les op\\u00e9rations qui nous int\\u00e9ressent ici doivent avoir, comme propri\\u00e9t\\u00e9 commune, la facult\\u00e9 d'exercer des contraintes s\\u00e9lectionnelles sp\\u00e9cifiques et r\\u00e9guli\\u00e8res sur la base et/ou le d\\u00e9riv\\u00e9 qu'elles relient. De cette mani\\u00e8re, le calcul automatique des traits s\\u00e9mantiques sur les entr\\u00e9es de la base sera pr\\u00e9cis, le nombre d'exceptions limit\\u00e9, et l'amor\\u00e7age du codage s\\u00e9mantique qui en r\\u00e9sultera sera qualitativement pertinent. Un certain nombre d'op\\u00e9rations v\\u00e9rifient ces conditions, qu'il s'agisse d'affixation ou de conversion. Nous laissons volontairement de c\\u00f4t\\u00e9 l'\\u00e9tude th\\u00e9orique des noms d\\u00e9verbaux de proc\\u00e8s ; soit parce que le typage du d\\u00e9riv\\u00e9 en tant que nom abstrait est une propri\\u00e9t\\u00e9 connue (cf. -age (repassage) ou -ment (gonflement)) et que toute sp\\u00e9cification suppl\\u00e9mentaire est impossible en l'absence de traits s\\u00e9mantiques sur le verbe ; soit parce que le typage du d\\u00e9riv\\u00e9 est non d\\u00e9terministe, et d\\u00e9pend de l'aspect t\\u00e9lique du verbe de base (ainsi, les noms en -tion d\\u00e9signent \\u00e0 la fois un proc\\u00e8s, et soit une manifestation de ce proc\\u00e8s (destruction) soit l'entit\\u00e9 concr\\u00e8te r\\u00e9sultante (construction)) ; pour les m\\u00eames raisons, nous ne dirons rien des noms abstraits de propri\\u00e9t\\u00e9 construits par -it\\u00e9 1 . En revanche, nous examinons quelques op\\u00e9rateurs peut-\\u00eatre moins connus, mais dont l'instruction s\\u00e9mantique permet d'\\u00e9tiqueter de fa\\u00e7on fine des noms , des verbes, et, plus rarement, des adjectifs. Typage des noms Chacun \\u00e0 sa mani\\u00e8re, les suffixes -aille, -aie, et -oir(e) entra\\u00eenent un typage s\\u00e9mantique homog\\u00e8ne du nom en position de base et/ou du nom construit. Il existe deux suffixes -aille (Aliquot-Suengas, 1999), qui s\\u00e9lectionnent des bases concr\\u00e8tes, mais qui construisent des noms (\\u00e9galement concrets) ayant des caract\\u00e9ristiques diff\\u00e9rentes. On distingue en effet une majorit\\u00e9 de noms en -aille collectifs \\u00e0 valeur \\u00e9valuative (flicaille ou ferraille sont marqu\\u00e9s n\\u00e9gativement), qui d\\u00e9signent des agglom\\u00e9rats dont il est impossible de distinguer les morceaux, qu'ils soient comptables (flic) ou massifs (fer), et les noms en -aille \\u00e0 valeur \\u00e9nonciative argotique non p\\u00e9jorative, qui ne sont pas des collectifs (jupaille). Seuls les premiers sont pertinents dans un lexique de la langue g\\u00e9n\\u00e9rale. Comme -aille, le suffixeaie, (cf. Aliquot-Suengas, 1996) , est un constructeur de noms concrets collectifs qui s\\u00e9lectionne exclusivement des bases d\\u00e9signant des esp\\u00e8ces v\\u00e9g\\u00e9tales, et programme le nom qu'il construit \\u00e0 d\\u00e9crire un tout (ormaie, bananeraie), o\\u00f9 chaque entit\\u00e9 qui le constitue (orme, bananier) est ind\\u00e9pendante des autres, contrairement, par exemple, \\u00e0 ce que l'on a observ\\u00e9 avec -aille formateur d'\\u00e9valuatifs. Contrairement aux deux premiers, enfin, le suffixe -oir(e) s\\u00e9lectionne des bases verbales et construit des noms concrets d\\u00e9signant le lieu (fumoir) ou l'instrument (hachoir) participant au d\\u00e9roulement du proc\\u00e8s que d\\u00e9crit la base. Souvent, le nom peut d\\u00e9signer une entit\\u00e9 qui sert \\u00e0 la fois de lieu et d'instrument (abreuvoir). Typage crois\\u00e9 de verbes et adjectifs L'ensemble des op\\u00e9rations de construction de verbes d\\u00e9sadjectivaux (affixation et conversion) produisent par essence des pr\\u00e9dicats de changement d'\\u00e9tat, l'\\u00e9tat final \\u00e9tant d\\u00e9crit par l'adjectif en position de base, qui de ce fait exprime une propri\\u00e9t\\u00e9 que l'on peut qualifier d'extrins\\u00e8que 2 . Quand a-fabrique un verbe \\u00e0 partir d'une base adjectivale (aplatir, allonger), celui-ci est, causatif, transitif et le r\\u00e9f\\u00e9rent de son objet direct se retrouve dans l'\\u00e9tat d\\u00e9crit par l'adjectif \\u00e0 la fin du d\\u00e9roulement du proc\\u00e8s. A quelques exceptions pr\\u00e8s (brutaliser, b\\u00eatifier), il en est de m\\u00eame pour -is(er) et -ifi(er). Quant aux verbes d\\u00e9sadjectivaux construits par conversion, ils sont soit causatifs, transitifs (vider) soit r\\u00e9sultatifs intransitifs (bl\\u00eamir). Certains admettent les deux constructions : c'est notamment le cas des verbes \\u00e0 base adjectivale chromatique (rougir, brunir). Un autre cas de typage verbal homog\\u00e8ne s'observe par exemple avec les bases s\\u00e9lectionn\\u00e9es par le suffixe -able formateur d'adjectifs (cf. (Pl\\u00e9nat, 1988) , repris et mod\\u00e9lis\\u00e9 dans (Dal et al., 1999) ). En effet, quand ce suffixe construit un adjectif d\\u00e9verbal, celui-ci d\\u00e9signe une aptitude, une propri\\u00e9t\\u00e9 intrins\\u00e8que, et la base verbale est g\\u00e9n\\u00e9ralement transitive. Quand elle ne l'est pas, elle d\\u00e9signe un pr\\u00e9dicat intransitif de mouvement se construisant avec modifieur exprimant la surface sur laquelle s'effectue le mouvement : circulable, skiable, navigable, roulable, surfable. On remarquera (Dal et Namer, 2000) que la propri\\u00e9t\\u00e9 extrins\\u00e8que observ\\u00e9e sur les adjectifs s\\u00e9lectionn\\u00e9s par les op\\u00e9rateurs formateurs de verbes est incompatible avec la propri\\u00e9t\\u00e9 intrins\\u00e8que des adjectifs en -able, ce qui explique que ces derniers sont des bases tr\\u00e8s improbables pour des verbes d\\u00e9sadjectivaux de changement d'\\u00e9tat, comme l'illustrent les exemples agrammaticaux suivants : *lavabiliser, *imperm\\u00e9abl(ir|er), *aportabl(ir|er), *croyabilifier. Typage crois\\u00e9 de verbes et noms Le dernier exemple concerne le pr\\u00e9fixe \\u00e9-, illustrant le cas des verbes d\\u00e9nominaux construits 3 . Aurnague et Pl\\u00e9nat (1997) d\\u00e9crivent \\u00e9-comme un marqueur de dissociation entre une partie concr\\u00e8te le plus souvent inanim\\u00e9e, d\\u00e9sign\\u00e9e par ce \\u00e0 quoi r\\u00e9f\\u00e8re la base, et le tout (l'objet direct du verbe construit, qui par cons\\u00e9quent est un causatif transitif). Ils proposent une tripartition de ces verbes selon que la partie est (1) un constituant naturel du tout (effeuiller une plante, \\u00e9p\\u00e9piner un raisin), (2) produite \\u00e0 partir du tout \\u00e0 l'issue du proc\\u00e8s, et ne pr\\u00e9existant pas \\u00e0 celui-ci (effranger un tapis), (3) un parasite du tout (\\u00e9pouiller un chien, \\u00e9pousseter un meuble). Traits cod\\u00e9s Il est possible de d\\u00e9duire un ensemble de traits \\u00e0 partir des descriptions pr\\u00e9sent\\u00e9es en 3.1. Bien sur, le nombre de ces traits ainsi que leur structuration sera vraisemblablement remis en cause avec l'analyse de nouveaux types morphologiques. Nous avons alors pr\\u00e9f\\u00e9r\\u00e9 dans un premier temps adopter une typologie all\\u00e9g\\u00e9e, parfois h\\u00e9t\\u00e9rog\\u00e8ne, de mani\\u00e8re \\u00e0 simplifier les m\\u00e9thodes de traitement. C'est ainsi que le codage des adjectifs se limite \\u00e0 l'opposition extrins\\u00e8que/intrins\\u00e8que. En ce qui concerne le codage des noms et des verbes, il a \\u00e9t\\u00e9 proc\\u00e9d\\u00e9 \\u00e0 chaque fois \\u00e0 une structuration minimale sous forme de listes de trois champs. Tout champ non pertinent est cod\\u00e9 xxx. Ainsi, un nom est soit concret, soit abstrait, trait qui constitue le premier champ de la liste. Le second sp\\u00e9cifie les noms concrets : inanim\\u00e9, que l'on oppose \\u00e0 anim\\u00e9. Cette position reste sous-sp\\u00e9cifi\\u00e9e pour les noms abstraits. La troisi\\u00e8me position est occup\\u00e9e par la caract\\u00e9risation ult\\u00e9rieure du nom : collectif, lieu, esp\\u00e8ce-v\\u00e9g\\u00e9tale, instrument, pour les noms concrets, et propri\\u00e9t\\u00e9 ou proc\\u00e8s pour les abstraits. En ce qui concerne les verbes, on oppose d'abord causatif et r\\u00e9sultatif, puis transitif et intransitif. Le troisi\\u00e8me champ d\\u00e9crit la structure argumentale correspondante, \\u00e9tiquet\\u00e9e th\\u00e9matiquement. Le Tableau 1 \\u00e0 la section 4.1.2. illustre la structuration choisie. Mise en oeuvre : programme et r\\u00e9sultats Le calcul automatique de traits s\\u00e9mantiques sur les entr\\u00e9es d'un lexique de lemmes \\u00e9tiquet\\u00e9s, inspir\\u00e9 des hypoth\\u00e8ses th\\u00e9oriques expos\\u00e9es en 3, tire profit de l'existence d'un programme d'analyse automatique de mots construits, baptis\\u00e9 D\\u00e9riF (\\\"D\\u00e9rivation en Fran\\u00e7ais\\\"). Dans 3 Je ne parlerai par contre ni de suffixation (les bases des verbes d\\u00e9nominaux construits par -is(er) et -ifi (er) appartiennent \\u00e0 des types s\\u00e9mantiques vari\\u00e9s (vampiriser vs r\\u00e9volveriser, momifier vs baronifier) refl\\u00e9tant les variantes de l'instruction s\\u00e9mantique du suffixe) ni de conversion N->V. En effet, cette op\\u00e9ration s\\u00e9lectionne aussi bien des noms d'instrument ou lieu concrets (hache/hacher, coffre/coffrer) que abstraits (force/forcer) (Corbin, \\u00e0 para\\u00eetre). La base peut \\u00e9galement faire r\\u00e9f\\u00e9rence \\u00e0 un humain, jouant alors le r\\u00f4le d'agent typique (singe/singer), alors m\\u00eame que l'op\\u00e9ration de conversion inverse, produisant essentiellement des noms de (r\\u00e9sultat ou manifestation de) proc\\u00e8s (voler/vol, chuter/chute), peut conduire aussi \\u00e0 la construction de noms d'agents (garder/garde) (Corbin, 1987) . On le voit, le choix des crit\\u00e8res \\u00e0 prendre en compte pour d\\u00e9terminer les contraintes sous-jacentes \\u00e0 l'op\\u00e9ration de conversion N/V et \\u00e0 son orientation n\\u00e9cessite une r\\u00e9flexion qu'abordent entre autre Kerleroux (1996 Kerleroux ( , 1997) ) 1) continuit\\u00e9, N => [ [ continu A] it\\u00e9 N] (continuit\\u00e9/N, continu/A) :: \\\"facult\\u00e9 d'\\u00eatre continu\\\" 2) explicabilit\\u00e9, N => [ [ [ expliquer V] able A] it\\u00e9 N] (explicabilit\\u00e9/N, explicable/A, expliquer/V) :: \\\"facult\\u00e9 d'\\u00eatre explicable\\\" 3a) ragrafer, V => [ re [ [ agrafe N] (er) V ] V] (ragrafer/V, agrafer/V, agrafe/N) ::agrafer une nouvelle fois 3b) portable, N => [ [ [ porter V] able A] N ] (portable/N, portable/A, porter/V) :: \\\"entit\\u00e9 ayant pour propri\\u00e9t\\u00e9 principale d'\\u00eatre portable\\\" 3c) introuvable, A =>[in[[trouver V] able A]A] (introuvable/A, trouvable/A, trouver/V) :: \\\"non trouvable\\\" 3d) d\\u00e9sossable, A => [ [ d\\u00e9 [os N] V] able A] (d\\u00e9sossable Annotation de la base au moyen de traits s\\u00e9mantiques Les types morphologiques d\\u00e9crits en 3.1. auxquels s'ajoutent les noms d\\u00e9verbaux en -age etment, et les noms d\\u00e9sadjectivaux en -it\\u00e9 donnent lieu au codage automatique d'environ 9000 entr\\u00e9es, soit 9% du lexique de d\\u00e9part. Quand les op\\u00e9rations morphologiques d\\u00e9crites \\u00e0 la section 3 sont activ\\u00e9es lors de l'analyse d'un lemme L du corpus, la t\\u00e2che de codage s\\u00e9mantique est d\\u00e9clench\\u00e9e en parall\\u00e8le sur une copie du corpus, o\\u00f9 la description de L est enrichie au moyen du ou des traits d\\u00e9pendants de l'instruction s\\u00e9mantique de l'op\\u00e9rateur, ainsi qu'en t\\u00e9moigne l'\\u00e9chantillon cod\\u00e9 dans le Tableau 1, o\\u00f9 sont indiqu\\u00e9s (en colonne 5) les traits calcul\\u00e9s pour L (apparaissant dans la colonne 4) en tant que base ou d\\u00e9riv\\u00e9 (indiqu\\u00e9 par \\u00ab B \\u00bb ou \\u00ab D \\u00bb, colonne 3) de l'op\\u00e9ration morphologique qui s'est appliqu\\u00e9e (report\\u00e9e dans la colonne 2). Les traits cod\\u00e9s sont parfois sous-d\\u00e9termin\\u00e9s (ligne 2, 4), parfois erron\\u00e9s (ligne 3). Le fait que L accumule plusieurs s\\u00e9ries de traits est d\\u00fb soit au codage en soi (ligne 4), soit \\u00e0 l'implication de L dans plusieurs op\\u00e9rations morphologiques (ligne 7). Op. (D)\\u00e9riv\\u00e9 / (B)ase Lemme Etiquet\\u00e9 Traits Cod\\u00e9s 1 a- D aplatir/VERBE (causatif Validation des traits Comme l'illustre le Tableau 1, une partie des traits cod\\u00e9s automatiquement va n\\u00e9cessiter une v\\u00e9rification, et, le cas \\u00e9ch\\u00e9ant, une correction. En effet 1547 entr\\u00e9es sont munies d'un codage ambigu (c'est le cas des bases verbales des adjectifs en -able, des noms d\\u00e9verbaux en -oir, et des verbes d\\u00e9sadjectivaux obtenus par conversion) ; on s'attend \\u00e0 ce que la phase de validation permette de lever certaines ambigu\\u00eft\\u00e9s. A l'inverse, il est vraisemblable que certains codages soient erron\\u00e9s (ainsi, parmi les 425 verbes en -iser, les 58 verbes en -ifier et les 90 verbes en a-analys\\u00e9s par d\\u00e9faut comme \\u00e9tant causatifs transitifs, une partie d'entre eux poss\\u00e8de une construction exclusivement intransitive) ; la validation a ici pour objectif de guider vers le rep\\u00e9rage de ces entr\\u00e9es mal cod\\u00e9es, dont la correction in fine sera manuelle. D'autres traits erron\\u00e9s sont par contre ind\\u00e9tectables automatiquement : c'est le cas de quelques noms de type anim\\u00e9 d\\u00e9signant des parasites (chenille, pou, puce, taupe) en position de base des verbes en \\u00e9-et cod\\u00e9s par d\\u00e9faut -\\u00e0 tort -inanim\\u00e9 (cf. section 3.1.3). Ces v\\u00e9rifications vont \\u00eatre r\\u00e9alis\\u00e9es automatiquement au moyen de deux techniques. Validation par filtrage : C'est tout d'abord au moyen de filtres automatiques que sont rep\\u00e9r\\u00e9s les codages des lemmes qui sont le fait d'op\\u00e9rateurs diff\\u00e9rents ; ces filtres comparent les informations co-occurrentes, de mani\\u00e8re \\u00e0 relever les traits compl\\u00e9mentaires ou unifiables. A l'heure actuelle, la mise en oeuvre de cette m\\u00e9thode n'apporte pas de r\\u00e9sultats spectaculaires, en raison du nombre encore faible de types morphologiques trait\\u00e9s. Seuls 83 lemmes re\\u00e7oivent leur codage s\\u00e9mantique \\u00e0 partir de deux op\\u00e9rations morphologiques distinctes : c'est le cas des bases verbales construites dans les adjectifs en -able qui se retrouvent avec une double s\\u00e9rie de traits, dont le filtre v\\u00e9rifie la compatibilit\\u00e9. Par exemple, sur les verbes convertis (ex. 6), le filtre \\u00e9limine la derni\\u00e8re s\\u00e9rie de traits, incompatible avec le type de base verbale, et unifie les 1 \\u00e8re et 3 \\u00e8me s\\u00e9ries (la 1 \\u00e8re \\u00e9tant un cas particulier de la 3 \\u00e8me ). 6) pr\\u00e9ciser/VERBE (causatif, transitif, [cause, th\\u00e8me] ), (r\\u00e9sultatif, intransitif, [th\\u00e8me] ), (xxx, transitif, [agent, th\\u00e8me] ), (xxx, intransitif, [agent, (lieu(sur) )) L'autre type de codage valid\\u00e9 par filtre est celui de lemmes comme le nom tripaille, (cf. Tableau 1), qui poss\\u00e8de deux s\\u00e9ries de traits compl\\u00e9mentaires : cette entr\\u00e9e r\\u00e9unit \\u00e0 la fois les contraintes des bases nominales s\\u00e9lectionn\\u00e9es par \\u00e9-(cf. 3.1.3) et les propri\\u00e9t\\u00e9s des noms construits au moyen de -aille (cf. 3.1.1) . Le filtre remplace automatiquement dans l'une des listes de traits, la valeur vide (\\\"xxx\\\") par l'instance occupant la m\\u00eame place dans l'autre liste, aboutissant \\u00e0 la fin au codage : (concret, inanim\\u00e9, collectif). On peut pr\\u00e9dire qu'avec l'\\u00e9largissement de la couverture en termes d'op\\u00e9rations morphologiques, la juxtaposition des traits rep\\u00e9r\\u00e9e automatiquement au moyen de filtres pourra mettre en lumi\\u00e8re des ph\\u00e9nom\\u00e8nes int\\u00e9ressants : si les traits se contredisent, cela pourra signifier la pr\\u00e9sence d'exceptions \\u00e0 des r\\u00e8gles de formation de mots bien s\\u00fbr, mais aussi l'existence d'acceptions multiples pour une entr\\u00e9e lexicale ; ainsi, quand l'analyseur traitera les noms suffix\\u00e9s en -ure et en -ier on peut supposer que l'entr\\u00e9e nominale avocat recevra \\u00e0 la fois le trait (concret, inanim\\u00e9, xxx) quand il est analys\\u00e9 \\u00e0 partir de avocatier et (concret, humain, profession) quand il est reli\\u00e9 \\u00e0 avocature 4 . L'incompatibilit\\u00e9 entre inanim\\u00e9 et humain t\\u00e9moigne ici de l'existence de deux homonymes : le fruit et l'individu. A l'inverse, on peut aussi imaginer que les traits calcul\\u00e9s sur une entr\\u00e9e sp\\u00e9cifieront l'instruction s\\u00e9mantique d'un op\\u00e9rateur acc\\u00e9dant \\u00e0 cette base. C'est ce qu'on est en droit de s'attendre avec le verbe accessoiriser, actuellement analys\\u00e9 au moyen d'une disjonction de gloses (cf. note 3), que l'\\u00e9tiquetage instrumental du nom accessoire, permettra de r\\u00e9duire \\u00e0 : \\\"munir de accessoire\\\". Validation par Internet: La sp\\u00e9cification des informations s\\u00e9mantiques peut tirer profit des r\\u00e9sultats d'une autre approche de validation. Cette approche, d\\u00e9j\\u00e0 mise en oeuvre dans (Dal & Namer 2000) Conclusion Le travail pr\\u00e9sent\\u00e9 propose une m\\u00e9thode pour coder automatiquement des traits s\\u00e9mantiques lexicaux en se servant des contraintes impos\\u00e9es par les op\\u00e9rateurs de construction de mots. Seuls quelques traits s\\u00e9mantiques sont accessibles par cette m\\u00e9thode, et seule une partie du lexique est cod\\u00e9e \\u00e0 ce jour. Il est pr\\u00e9visible que l'\\u00e9volution de l'analyseur entra\\u00eene un enrichissement du codage pour les noms et les verbes. Mais m\\u00eame partiels, ces r\\u00e9sultats sont n\\u00e9anmoins utilisables d'ores et d\\u00e9j\\u00e0 en analyse morphologique pour expliciter des exceptions ou sp\\u00e9cifier des gloses. Ils sont \\u00e9galement exploitables pour s\\u00e9lectionner les bases candidates \\u00e0 la g\\u00e9n\\u00e9ration automatique de mots construits : ainsi, les traits codant les verbes en \\u00e9-ou en a-en font des bases potentielles pour des adjectifs en -able. Enfin, l'approche pr\\u00e9sent\\u00e9e peut servir, dans un syst\\u00e8me d'analyse automatique, \\u00e0 la caract\\u00e9risation s\\u00e9mantique des mots construits qui sont absents des dictionnaires de langue parce qu'ils appartiennent \\u00e0 un type morphologique trop productif, ou parce qu'il s'agit de n\\u00e9ologismes : la nature r\\u00e9guli\\u00e8re des n\\u00e9ologismes construits laisse en effet pr\\u00e9sager une bonne capacit\\u00e9 pr\\u00e9dictive du syst\\u00e8me d'assignation des traits. R\\u00e9f\\u00e9rences Arriv\\u00e9, M, Gadet F. et Galmiche M., 1986 , La Grammaire d'Aujourd'hui, Paris, Flammarion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publisher\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ATALA\",\n          \"Association pour le Traitement Automatique des Langues\",\n          \"ATALA/AFCP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1501,\n        \"samples\": [\n          \"https://aclanthology.org/2006.jeptalnrecital-poster.10\",\n          \"https://aclanthology.org/2006.jeptalnrecital-recitalposter.2\",\n          \"https://aclanthology.org/F14-2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_fr_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          \"Le corpus Text+Berg Une ressource parall\\u00e8le alpin fran\\u00e7ais-allemand \",\n          \"Libellex : une plateforme multiservices pour la gestion des contenus multilingues \",\n          \"R\\u00e9sum\\u00e9 Automatique Multilingue Exp\\u00e9rimentations sur l'Anglais, l'Arabe et le Fran\\u00e7ais\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "data_netoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiTJ6d-Pf-fh",
        "outputId": "7915e572-f8ed-4e58-c7e8-ad995a5406a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank-bm25) (2.0.2)\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install rank-bm25\n",
        "!spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "E5mBA7Osf-cu"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import spacy\n",
        "from spacy.lang.fr import stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "9Gcj1nISf-ao"
      },
      "outputs": [],
      "source": [
        "model=spacy.load(\"fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Ptkz1OG1f-Y4"
      },
      "outputs": [],
      "source": [
        "title_fr=data_netoy[\"title_fr_clean\"].dropna().to_list()\n",
        "full_text=data_netoy[\"full_text\"].dropna().to_list()\n",
        "doc_ids=data_netoy.index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmA-r8ufgNfj",
        "outputId": "be38132c-8910-47ed-ee54-799c7ab39fa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1501"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "len(data_netoy.title_fr_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "uUq0cem-gNcX"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  doc=model(text)\n",
        "  tokens=[token.text for token in doc if not token.is_stop]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ZoUyJIzAgeTb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "PjVaOlPBkLFQ"
      },
      "outputs": [],
      "source": [
        "title_fr_tokens=[tokenize(text) for text in title_fr]\n",
        "full_text_tokens=[tokenize(full) for full in full_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "9lsHRglHj0pm"
      },
      "outputs": [],
      "source": [
        "bm25_title=BM25Okapi(title_fr_tokens)\n",
        "bm25_full=BM25Okapi(full_text_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRUyKBs_nY83",
        "outputId": "cfb5f3ef-806d-42f2-8b48-afa46e5ab9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "9n8rGBTgnb3Y"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "dEC2lPqKnd50"
      },
      "outputs": [],
      "source": [
        "model_dense=SentenceTransformer(\"antoinelouis/biencoder-mMiniLMv2-L6-mmarcoFR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN2PkEEmE7CE"
      },
      "outputs": [],
      "source": [
        "QUERIES = [\"reconnaissance d'entités nommées\",\"modélisation statistique en traduction automatique\",\n",
        "    \"enrichissement des systèmes de recherche d'information\",\n",
        "    \"biais de genre en traduction automatique\",\n",
        "    \"le tal et l'enseignement\"]\n",
        "bm25_final=[]\n",
        "sims_run=[]\n",
        "sims_final=[]\n",
        "rrf_final=[]\n",
        "k=60\n",
        "query_results: dict[str, list[dict[str | float]]] = {}\n",
        "\n",
        "\n",
        "for q in QUERIES:\n",
        "    query_tokens = tokenize(q)\n",
        "    bm_scores = bm25_full.get_scores(query_tokens)\n",
        "    bm25_sorted = sorted(enumerate(bm_scores), key=lambda x: x[1], reverse=True)\n",
        "    bm25_final.append(bm25_sorted)\n",
        "\n",
        "    q_embedding = model_dense.encode_query([q], normalize_embeddings=True)\n",
        "    p_embedding = model_dense.encode_document(full_text, normalize_embeddings=True)\n",
        "\n",
        "    dense_similarity = q_embedding @ p_embedding.T\n",
        "\n",
        "    sims_2 = [(i, float(score)) for i, score in enumerate(dense_similarity[0])]\n",
        "    sims_2_sorted = sorted(sims_2, key=lambda x: x[1], reverse=True)\n",
        "    sims_run.append(sims_2_sorted)\n",
        "\n",
        "\n",
        "    sims_final.append(sims_2_sorted)\n",
        "\n",
        "    # --- RRF ---\n",
        "    rrf_scores = defaultdict(float)\n",
        "    for rank, (idx, score) in enumerate(bm25_sorted):\n",
        "        rrf_scores[idx] += 1 / (k + rank + 1)\n",
        "\n",
        "    for rank, (idx, score) in enumerate(sims_2_sorted):\n",
        "        rrf_scores[idx] += 1 / (k + rank + 1)\n",
        "\n",
        "    sorted_rrf = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    rrf_final.append(sorted_rrf)\n",
        "\n",
        "    # --- Stockage des résultats ---\n",
        "    results = []\n",
        "    for _, (idx, rrf_score) in enumerate(sorted_rrf):\n",
        "        bm25_score = bm_scores[idx]\n",
        "        dense_score = float(dense_similarity[0][idx])\n",
        "        json_entry = {\n",
        "            \"document\": full_text[idx],\n",
        "            \"rrf_score\": rrf_score,\n",
        "            \"bm25_score\": bm25_score,\n",
        "            \"dense_score\": dense_score,\n",
        "        }\n",
        "        results.append(json_entry)\n",
        "\n",
        "    query_results[q] = results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPemz--FfOO2"
      },
      "outputs": [],
      "source": [
        "with open('precomputed_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(query_results, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQgUD3VCHMUr"
      },
      "source": [
        "Création des run files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehfvn4n8gcsg"
      },
      "outputs": [],
      "source": [
        "#création du runfile bm25\n",
        "for bm in bm25_final:\n",
        "  query_ids = [f\"{i+1:03d}\" for i in range(len(QUERIES))]\n",
        "  run_name = \"bm25\"\n",
        "  with open(f\"{run_name}_run.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    for qid, query in zip(query_ids, QUERIES):\n",
        "      ranked_docs = bm # récup. résultats pour requête, retourne list[tuple]: [(doc_idx, score)]\n",
        "      for rank, (idx, score) in enumerate(ranked_docs,start=1):\n",
        "        doc_id = doc_ids[idx]\n",
        "\n",
        "        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JMn6hXGgcqP"
      },
      "outputs": [],
      "source": [
        "#création du runfile modèle dense\n",
        "for dense in sims_run:\n",
        "  query_ids = [f\"{i+1:03d}\" for i in range(len(QUERIES))]\n",
        "  run_name = \"dense\"\n",
        "\n",
        "  with open(f\"{run_name}_run.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "      for qid, dense in zip(query_ids, sims_run):\n",
        "          ranked_docs = dense  # list of tuples: (doc_id, score)\n",
        "          for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
        "              f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyiEluj-gcoW"
      },
      "outputs": [],
      "source": [
        "#création du runfile modèle hybrid\n",
        "for q in rrf_final:\n",
        "  query_ids = [f\"{i+1:03d}\" for i in range(len(QUERIES))]\n",
        "  run_name = \"hybrid\"\n",
        "  with open(f\"{run_name}_run.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    for qid, query in zip(query_ids, QUERIES):\n",
        "      ranked_docs = q # récup. résultats pour requête, retourne list[tuple]: [(doc_idx, score)]\n",
        "      for rank, (idx, score) in enumerate(ranked_docs,start=1):\n",
        "        doc_id = doc_ids[idx]\n",
        "\n",
        "        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sgu3EuA2ttx"
      },
      "outputs": [],
      "source": [
        "!pip install ir_measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bOfLV1H2thW"
      },
      "outputs": [],
      "source": [
        "from ir_measures import read_trec_qrels, read_trec_run\n",
        "from ir_measures import P, R, MRR\n",
        "\n",
        "# Charger les fichiers\n",
        "qrels = read_trec_qrels(\"ri.qrel\")\n",
        "run = read_trec_run(\"bm25_run.txt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results_bm=ir_measures.calc_aggregate([P@5,R@5,MRR],qrels,run)\n",
        "results_dict = {str(m): v for m, v in results_bm.items()}\n",
        "\n",
        "with open(\"bm_eval.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_dict, f)\n",
        "\n",
        "\n",
        "print(results_bm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYaX8kXG6EwX"
      },
      "outputs": [],
      "source": [
        "# Charger les fichiers\n",
        "qrels = read_trec_qrels(\"ri.qrel\")\n",
        "run = read_trec_run(\"dense_run.txt\")\n",
        "\n",
        "\n",
        "results_dense=ir_measures.calc_aggregate([P@5,R@5,MRR],qrels,run)\n",
        "results_dict_dense = {str(m): v for m, v in results_dense.items()}\n",
        "\n",
        "with open(\"dense_eval.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_dict_dense, f)\n",
        "\n",
        "print(results_dense)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jypQoPhk6I_z"
      },
      "outputs": [],
      "source": [
        "# Charger les fichiers\n",
        "qrels = read_trec_qrels(\"ri.qrel\")\n",
        "run = read_trec_run(\"hybrid_run.txt\")\n",
        "\n",
        "results_hybrid=ir_measures.calc_aggregate([P@5,R@5,MRR],qrels,run)\n",
        "results_dict_hybrid = {str(m): v for m, v in results_dense.items()}\n",
        "\n",
        "with open(\"hybrid_eval.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_dict_hybrid, f)\n",
        "\n",
        "print(results_hybrid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt0ByNm2ME0I"
      },
      "source": [
        "###Application Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrBjeIlYksNV"
      },
      "source": [
        "###Pour BM25\n",
        "Calcul des scores pour le modèle BM25 à ajouter sur l'application streamlit. Les résultats sont sous forme de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw6yL318b5W_"
      },
      "outputs": [],
      "source": [
        "dfs_bm = []\n",
        "for i in range(5):\n",
        "\n",
        "    df_bm = pd.DataFrame(columns=[\"id\", \"score\", \"title\", \"abstract\", \"url\"])\n",
        "\n",
        "    ids = []\n",
        "    scores = []\n",
        "    titles = []\n",
        "    abstracts = []\n",
        "    urls = []\n",
        "\n",
        "    iterer = 10\n",
        "\n",
        "    for (idx, score) in bm25_final[i]:\n",
        "        ids.append(idx)\n",
        "        scores.append(score)\n",
        "        titles.append(data_netoy.iloc[idx][\"title_fr_clean\"])\n",
        "        abstracts.append(data_netoy.iloc[idx][\"abstract\"])\n",
        "        urls.append(data_netoy.iloc[idx][\"url\"])\n",
        "\n",
        "    for j, (doc_id, score, title, abstract, url) in enumerate(\n",
        "        zip(ids[:iterer], scores, titles, abstracts, urls)\n",
        "    ):\n",
        "        df_bm.loc[j] = [doc_id, score, title, abstract, url]\n",
        "\n",
        "    # sauvegarde\n",
        "    df_bm.to_csv(f\"df_bm_{i}.csv\", index=False)\n",
        "\n",
        "    # stockage\n",
        "    dfs_bm.append(df_bm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hz9v7d_kwpL"
      },
      "source": [
        "###Pour modèle dense\n",
        "Calcul des scores pour le modèle dense à ajouter sur l'application streamlit. Les résultats sont sous forme de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUNm9PekwA2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs_dense = []\n",
        "\n",
        "iterer = 10  # nombre de résultats à afficher\n",
        "\n",
        "for i in range(len(sims_run)):\n",
        "    df_dense = pd.DataFrame(columns=[\"id\", \"score\", \"title\", \"abstract\", \"url\"])\n",
        "\n",
        "    # sims_run[i] contient désormais des tuples (doc_id, score)\n",
        "    for j, (doc_id, score) in enumerate(sims_run[i][:iterer]):\n",
        "        df_dense.loc[j] = [\n",
        "            doc_id,\n",
        "            score,\n",
        "            data_netoy.iloc[doc_id][\"title_fr_clean\"],\n",
        "            data_netoy.iloc[doc_id][\"abstract\"],\n",
        "            data_netoy.iloc[doc_id][\"url\"]\n",
        "        ]\n",
        "\n",
        "    # Sauvegarde facultative\n",
        "    df_dense.to_csv(f\"df_dense_{i}.csv\", index=False)\n",
        "\n",
        "    # Stockage dans la liste\n",
        "    dfs_dense.append(df_dense)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP9QdyqkuQH_"
      },
      "source": [
        "###Modèle hybride\n",
        "Calcul des scores pour le modèle hybride à ajouter sur l'application streamlit. Les résultats sont sous forme de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVYBUWSxuSkI"
      },
      "outputs": [],
      "source": [
        "dfs_hybrid = []\n",
        "for i in range(5):\n",
        "\n",
        "    df_hybrid = pd.DataFrame(columns=[\"id\", \"score\", \"title\", \"abstract\", \"url\"])\n",
        "\n",
        "    ids = []\n",
        "    scores = []\n",
        "    titles = []\n",
        "    abstracts = []\n",
        "    urls = []\n",
        "\n",
        "    iterer = 10\n",
        "\n",
        "    for (idx, score) in rrf_final[i]:\n",
        "        ids.append(idx)\n",
        "        scores.append(score)\n",
        "        titles.append(data_netoy.iloc[idx][\"title_fr_clean\"])\n",
        "        abstracts.append(data_netoy.iloc[idx][\"abstract\"])\n",
        "        urls.append(data_netoy.iloc[idx][\"url\"])\n",
        "\n",
        "    for j, (doc_id, score, title, abstract, url) in enumerate(\n",
        "        zip(ids[:iterer], scores, titles, abstracts, urls)\n",
        "    ):\n",
        "        df_hybrid.loc[j] = [doc_id, score, title, abstract, url]\n",
        "\n",
        "    # sauvegarde\n",
        "    df_hybrid.to_csv(f\"df_bm_{i}.csv\", index=False)\n",
        "\n",
        "    # stockage\n",
        "    dfs_hybrid.append(df_hybrid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ChBeapcJP6C"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_263ApYRht6m"
      },
      "outputs": [],
      "source": [
        "with open(\"dfs_bm.pkl\", \"wb\") as f:\n",
        "    pickle.dump(dfs_bm, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbofqllClt-D"
      },
      "outputs": [],
      "source": [
        "with open(\"dfs_dense.pkl\", \"wb\") as f:\n",
        "    pickle.dump(dfs_dense, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_rR03K6uMRB"
      },
      "outputs": [],
      "source": [
        "with open(\"dfs_hybrid.pkl\", \"wb\") as f:\n",
        "    pickle.dump(dfs_hybrid, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a1cF1UbTfGC"
      },
      "outputs": [],
      "source": [
        "data_netoy.to_csv(\"data_netoy.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}